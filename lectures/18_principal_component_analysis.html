

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Dimensionality Reduction: Principal Component Analysis (PCA) &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/18_principal_component_analysis';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Density Estimation" href="17_density_estimation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamentals of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">Introduction to the Labs and Work Environment Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_python_crash_course.html">Python Crash Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_python_data_science_crash_course.html">Python for Data Science Crash Course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro_data_analysis.html">Data Analysis Key Concepts, Loading and Inspecting the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="02_describing_and_visualizing_the_data.html">Describing and Visualizing the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_probability_for_data_analysis.html">Probability for Data Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_association_between_variables.html">Association between variables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05_data_distributions.html">Data Distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_statistical_inference.html">Statistical Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="07_storytelling_with_data.html">Storytelling with Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_predictive_modeling.html">Introduction to Predictive Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_linear_regression.html">Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 11</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_beyond_linear_regression.html">Beyond Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 12</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_classification_knn.html">Classification Task, Evaluation Measures, and K-Nearest Neighbor</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 13</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_logistic_regression.html">Logistic Regression - Statistical View</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 14</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_multiclass_logistic_regression.html">Multiclass Logistic Regression and Predictive View</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 15</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="14_map_naive_bayes.html">Generative Classifiers and Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 16</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="15_data_as_nd_points.html">Data as N-Dimensional Points</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_clustering.html">Clustering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 17</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17_density_estimation.html">Density Estimation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 18</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Dimensionality Reduction: Principal Component Analysis (PCA)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes2526/blob/master/lecturenotes/lectures/18_principal_component_analysis.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526/issues/new?title=Issue%20on%20page%20%2Flectures/18_principal_component_analysis.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/18_principal_component_analysis.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Dimensionality Reduction: Principal Component Analysis (PCA)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-vs-feature-reduction">Feature Selection vs Feature Reduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-reduction-example">Feature Reduction Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-formulation">General Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-our-example">Back to Our Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-for-data-whitening">PCA for data whitening</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-an-appropriate-number-of-components">Choosing an Appropriate Number of Components</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-principal-components-load-plots">Interpretation of the Principal Components - Load Plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laboratory-applications-of-pca">Laboratory: Applications of PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-and-visualization">Dimensionality Reduction and Visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-decorrelation-principal-component-regression">Data Decorrelation - Principal Component Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-compression">Data Compression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenfaces">Eigenfaces</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-goal">The Goal</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-principal-components-eigenfaces">Visualizing the Principal Components (Eigenfaces)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-compression-and-reconstruction">Data Compression and Reconstruction</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#are-these-eigenface-features-actually-better">Are These “Eigenface” Features Actually Better?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#baseline-knn-on-4096-raw-pixels">Baseline: KNN on 4096 Raw Pixels</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2-knn-on-20-smart-pca-features">Model 2: KNN on 20 “Smart” PCA Features</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="dimensionality-reduction-principal-component-analysis-pca">
<h1>Dimensionality Reduction: Principal Component Analysis (PCA)<a class="headerlink" href="#dimensionality-reduction-principal-component-analysis-pca" title="Permalink to this heading">#</a></h1>
<p>So far, we have worked with datasets with a handful of interpretable features. In the real world, we often face datasets with <em>thousands</em> or even <em>millions</em> of features.</p>
<ul class="simple">
<li><p>An 800x600 pixel color image has <span class="math notranslate nohighlight">\(800 \times 600 \times 3 = 1,440,000\)</span> features (pixels).</p></li>
<li><p>A “bag of words” model for text can easily have 50,000 features (one for each word).</p></li>
</ul>
<p>This creates two major problems:</p>
<ol class="arabic simple">
<li><p><strong>The Curse of Dimensionality:</strong> In high-dimensional space, everything is “far apart,” and distance-based models like KNN fail.</p></li>
<li><p><strong>Multicollinearity:</strong> Many features are highly redundant (e.g., adjacent pixels in an image).</p></li>
</ol>
<p>We cannot just <strong>select</strong> features (like we did with backward elimination) because every pixel in an image is <em>necessary</em>. Instead, we need to <em>summarize</em> or <em>compress</em> the data.</p>
<p>This is <strong>Dimensionality Reduction</strong>. Our goal is to transform our data from <span class="math notranslate nohighlight">\(d\)</span> (e.g., 1,440,000) dimensions to a new, smaller set of <span class="math notranslate nohighlight">\(k\)</span> (e.g., 100) “smart” features, while losing the least amount of information possible.</p>
<p>The most common way to do this is <strong>Principal Component Analysis (PCA)</strong>.</p>
<section id="feature-selection-vs-feature-reduction">
<h2>Feature Selection vs Feature Reduction<a class="headerlink" href="#feature-selection-vs-feature-reduction" title="Permalink to this heading">#</a></h2>
<p>We have seen how, when many variables are available, it makes sense to select a subset of such variables. In regression analysis, in particular, we have seen how, when features are highly correlated, we should <strong>discard some of them to reduce collinearity</strong>. Indeed, if two variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are highly correlated, one of the two is redundant to a given extent, and we can ignore it.</p>
<p>We have seen how variables can be selected in different ways:</p>
<ul class="simple">
<li><p>By looking at the correlation matrix, we can directly find those pairs of variables which are highly correlated;</p></li>
<li><p>When defining a linear or logistic regressor, we can remove those variables with a high p-value. We have seen that these are variables which do not contribute significantly to the prediction of the dependent variable, given all other variables present in the regressor;</p></li>
<li><p>Techniques such as <strong>Ridge and Lasso regression</strong> allow to perform some form of variable (or feature) selection, setting very low or zero coefficients for variables which do not contribute significantly to regression.</p></li>
</ul>
<p><strong>In general, however, when we discard a set of variables, we throw away some informative content, unless the variables we are removing can be perfectly reconstructed from the variables we are keeping, e.g., because they are linear combinations of other variables.</strong></p>
<p>Instead of selecting a subset of features to work with, <strong>feature reduction techniques</strong> aim to <strong>find a new set of features which summarize the original set of features losing a small amount of information, while maintaining a limited number of dimensions</strong>.</p>
</section>
<section id="feature-reduction-example">
<h2>Feature Reduction Example<a class="headerlink" href="#feature-reduction-example" title="Permalink to this heading">#</a></h2>
<p>Let us consider the Iris dataset. In particular, we will consider two features: <code class="docutils literal notranslate"><span class="pre">sepal</span> <span class="pre">length</span></code> and <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">length</span></code>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/ace045b1b427038b84f442b79e6c294610433869cf0407eba7596fd6c5a7bff6.png" src="../_images/ace045b1b427038b84f442b79e6c294610433869cf0407eba7596fd6c5a7bff6.png" />
</div>
</div>
<p>We can see from the plot above that the two features are highly correlated. Let us compute the Pearson coefficient and the related p-value:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PearsonRResult(statistic=np.float64(0.8717537758865832), pvalue=np.float64(1.0386674194497531e-47))
</pre></div>
</div>
</div>
</div>
<p>Let’s say we want to reduce the number of features (e.g., for computational reasons). We could think of applying <strong>feature selection</strong> and choose one fo the two, but we would surely <strong>loose some information</strong>. Instead, we could think of a <strong>linear projection of the data into a single dimension</strong> such that we loose the least possible amount of information.</p>
<p>To avoid any dependence on the units of measures and on the positioning of the points in the space, let’s first standardize the data:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/adf53719c8e16bee4434e24836ce47ca6dcd239cdde92b2743470bb6fa7e1bdf.png" src="../_images/adf53719c8e16bee4434e24836ce47ca6dcd239cdde92b2743470bb6fa7e1bdf.png" />
</div>
</div>
<p>Transforming the 2D data to a single dimension with a linear transformation can be done with the following expression:</p>
<div class="math notranslate nohighlight">
\[z = u_1 x_1 + u_2 x_2 = \mathbf{u}^T\mathbf{x}\]</div>
<p>Note that this is equivalent to projecting point <span class="math notranslate nohighlight">\((x_1,x_y)\)</span> to a line passing through the origin and with the same direction on <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>.</p>
<p>Once we choose a set of <span class="math notranslate nohighlight">\((u_1,u_2)\)</span>, we obtain a different projection, as in the example below, where we project the data to different random lines passing through the origin:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/93f4e4bd030248929251296a5ea9650f614bdfbd69ac14e327c7538d3eb9e693.png" src="../_images/93f4e4bd030248929251296a5ea9650f614bdfbd69ac14e327c7538d3eb9e693.png" />
</div>
</div>
<p>As can be seen, <strong>not all projections are the same</strong>. Indeed, some of them tend to project the points in a space in which they are very close to each other, while others allow for more “space” between the points.</p>
<p>Intuitively, we would like to project points in a way that <strong>they keep their distinctiveness as much as possible</strong>. In practice, we can quantify <strong>distinctiveness</strong> with <strong>the variance of the projected data</strong>.</p>
</section>
<section id="general-formulation">
<h2>General Formulation<a class="headerlink" href="#general-formulation" title="Permalink to this heading">#</a></h2>
<p>We have seen a simple example in the case of two variables. Let us now discuss the general formulation in the case of <span class="math notranslate nohighlight">\(D\)</span>-dimensional variables.</p>
<p>Let <span class="math notranslate nohighlight">\(\{\mathbf{x}_n\}\)</span> be a set of <span class="math notranslate nohighlight">\(N\)</span> observations, where <span class="math notranslate nohighlight">\(n=1,\ldots,N\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> is a vector of dimensionality <span class="math notranslate nohighlight">\(D\)</span>. We will define Principal Component Analysis as a projection of the data to <span class="math notranslate nohighlight">\(M \lt D\)</span> dimensions such that the variance of the projected data is maximized.</p>
<blockquote>
<div><p><strong>NOTE</strong>: to avoid any influence by the individual variances along the original axes, we will assume that <strong>the data is standardized</strong>.</p>
</div></blockquote>
<p>We will first consider the case in which <span class="math notranslate nohighlight">\(M=1\)</span>. In this case, we want to find a D-dimensional vector <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> such that, projecting the data to this vector, variance is maximized. Without loss of generality, we will choose <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{u}_1^T\mathbf{u}=1\)</span>. This corresponds to choosing <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> as a <strong>unit vector</strong>, which makes sense, as we are interested in the direction of the projection. We will find this assumption useful later.</p>
<p>Let <span class="math notranslate nohighlight">\(\overline{\mathbf{x}}\)</span> be the sample mean:</p>
<div class="math notranslate nohighlight">
\[\overline{\mathbf{x}} = \frac{1}{N}\sum_{n=1}^N x_n\]</div>
<p>The data covariance matrix <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> will be:</p>
<div class="math notranslate nohighlight">
\[\mathbf{S} = \frac{1}{N} \sum_{n=1}^N (\mathbf{x}_n - \overline{\mathbf{x}}) (\mathbf{x}_n - \overline{\mathbf{x}})^T\]</div>
<p>We know that projecting a data point to <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> is done simply by the dot product:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{u}_1^T \mathbf{x} \]</div>
<p>It is easy to see that the mean of the projected data is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{u}_1^T \overline{\mathbf{x}}\]</div>
<p>Its variance is:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{N} \sum_{n=1}^N (\mathbf{u}_1^T \mathbf{x}_n - \mathbf{u}_1^T \overline{\mathbf{x}}) (\mathbf{u}_1^T \mathbf{x}_n - \mathbf{u}_1^T \overline{\mathbf{x}})^T = \mathbf{u}_1^T \frac{1}{N} \sum_{n=1}^N (\mathbf{x}_n -  \overline{\mathbf{x}}) ( \mathbf{x}_n - \overline{\mathbf{x}})^T \mathbf{u}_1 = \mathbf{u}_1^T \mathbf{S}\mathbf{u}_1 \]</div>
<p>To find an appropriate <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span>, we need to maximize the variance of the projected data with respect to <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span>. The optimization problem can be formalized as follows:</p>
<div class="math notranslate nohighlight">
\[\arg\max \{\mathbf{u}_1^T\mathbf{S}\mathbf{u_1}\} \text{ subject to } \mathbf{u}_1^T\mathbf{u}_1=1\]</div>
<p>Note that the constraint <span class="math notranslate nohighlight">\(\mathbf{u}_1^T\mathbf{u}_1=1\)</span> is necessary. Without it, we could arbitrarily increase the variance by choosing vectors <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> with large modules, hence <span class="math notranslate nohighlight">\(\mathbf{u}_1^T\mathbf{u}_1 \to +\infty\)</span>.</p>
<p><strong>We will not see how this optimization problem is solved in details</strong>, but it can be shown that the variance is maximized when:</p>
<div class="math notranslate nohighlight">
\[\mathbf{u}_1^T \mathbf{S} \mathbf{u}_1 = \lambda_1\]</div>
<p>Note that this is equivalent to:</p>
<div class="math notranslate nohighlight">
\[\mathbf{S} \mathbf{u}_1 = \lambda_1 \mathbf{u}_1\]</div>
<p>From the result above, we find out that:</p>
<ul class="simple">
<li><p>The solution <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{S}\)</span>. The related eigenvalue  <span class="math notranslate nohighlight">\(\lambda_1\)</span> is the variance along that dimension;</p></li>
<li><p>Since we want to maximize the variance, among all eigenvectors, we should choose the one with the <strong>largest eigenvalue <span class="math notranslate nohighlight">\(\lambda_1\)</span></strong>.</p></li>
</ul>
<p>The vector <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> will be <strong>the first principal component</strong> of the data.</p>
<p><strong>We can proceed in an iterative fashion to find the other components</strong>. To obtain a valid base (a new reference system), we will choose the next component <span class="math notranslate nohighlight">\(\mathbf{u}_2\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{u}_2 \perp \mathbf{u}_1\)</span>. We can hence identify the second principal component by choosing the vector <span class="math notranslate nohighlight">\(\mathbf{u}_2\)</span> with maximizes the variance <strong>among all vectors which are orthogonal to <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span></strong>.</p>
<p>In practice, it can be shown that the <span class="math notranslate nohighlight">\(M\)</span> principal components can be found by choosing the <span class="math notranslate nohighlight">\(M\)</span> eigenvectors <span class="math notranslate nohighlight">\(\mathbf{u}_1, \ldots, \mathbf{u}_M\)</span> of the covariance matrix <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> corresponding to the largest eigenvectors <span class="math notranslate nohighlight">\(\lambda_1, \ldots, \lambda_M\)</span>.</p>
<p>Let us define the matrix <span class="math notranslate nohighlight">\(W\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{W} = \left(\begin{matrix}
\mathbf{u}_1 \\
\mathbf{u}_2 \\
\ldots \\
\mathbf{u}_M \\
\end{matrix}\right) = \left(\begin{matrix}
u_{11} &amp; u_{12} &amp; \ldots &amp; u_{1D} \\
u_{21} &amp; u_{22} &amp; \ldots &amp; u_{2D} \\
\ldots \\
u_{M1} &amp; u_{M2} &amp; \ldots &amp; u_{MD} \\
\end{matrix}\right)\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(u_{ij}\)</span> is the <span class="math notranslate nohighlight">\(j^{th}\)</span> component of vector <span class="math notranslate nohighlight">\(\mathbf{u}_i\)</span>.</p>
<p>Let:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X} = \left(\begin{matrix}
\mathbf{x}_1 \\
\mathbf{x}_2 \\
\ldots \\
\mathbf{x}_N \\
\end{matrix}\right) = \left(\begin{matrix}
x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1D} \\
x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2D} \\
\ldots \\
x_{N1} &amp; x_{N2} &amp; \ldots &amp; x_{ND} \\
\end{matrix}\right)\end{split}\]</div>
<p>be the <span class="math notranslate nohighlight">\([N \times D]\)</span> data matrix.</p>
<p>We can project the data matrix <span class="math notranslate nohighlight">\(X\)</span> to the principal components with the following formula:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Z} = \mathbf{X} \mathbf{W}^T\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> will be an <span class="math notranslate nohighlight">\([N \times M]\)</span> matrix in which the <span class="math notranslate nohighlight">\(i^{th}\)</span> row denotes the <span class="math notranslate nohighlight">\(i^{th}\)</span> element of the dataset projected to the PCA space.</p>
<p>We will call <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> latent variables as they are not observed and encode properties of the data.</p>
<p>In geometrical terms, the PCA <strong>performs a rotation of the D-dimensional data around the center of the data</strong> in a way that:</p>
<ul class="simple">
<li><p>The new axes are sorted by variance;</p></li>
<li><p>The projected data is uncorrelated.</p></li>
</ul>
</section>
<section id="back-to-our-example">
<h2>Back to Our Example<a class="headerlink" href="#back-to-our-example" title="Permalink to this heading">#</a></h2>
<p>We now have the computational tools to fully understand our example. Let us plot the data again. We will consider the mean-centered data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">standardized_data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(150, 4)
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/8f4c81f9f32d905f1fe31e8418904b334ac7579a6b7f6615a4c78e541155c430.png" src="../_images/8f4c81f9f32d905f1fe31e8418904b334ac7579a6b7f6615a4c78e541155c430.png" />
</div>
</div>
<p>We have seen that the covariance matrix plays a central role in PCA. The covariance matrix of the original data will be as follows in our case:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1.00671141, 0.87760447],
       [0.87760447, 1.00671141]])
</pre></div>
</div>
</div>
</div>
<p>The eigvenvalues and eigenvectors will be as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvalues: [0.12910694 1.88431588]

Eigenvectors:
 [[-0.70710678  0.70710678]
 [-0.70710678 -0.70710678]]
</pre></div>
</div>
</div>
</div>
<p>The rows of the eigenvector matrix identify two principal components. Let us re-order the principal components so that eigenvalues are descending (we want the first principal component to have the largest value):</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvalues: [1.88431588 0.12910694]

Eigenvectors:
 [[-0.70710678 -0.70710678]
 [-0.70710678  0.70710678]]
</pre></div>
</div>
</div>
</div>
<p>The two components identify two directions along which we can project our data, as shown in the following plot:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/5572bb1a109e20e97bf28f21a0bc6a8c387f508821e6fc75b94fa9b83e8fe4b1.png" src="../_images/5572bb1a109e20e97bf28f21a0bc6a8c387f508821e6fc75b94fa9b83e8fe4b1.png" />
</div>
</div>
<p>The eigenvectors can also be interpreted as vectors in the original space. Getting back to our 2D example, they would be shown as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c979591818520d8d4023d0d00d919e63c95808603516cb524e663b7f08838870.png" src="../_images/c979591818520d8d4023d0d00d919e63c95808603516cb524e663b7f08838870.png" />
</div>
</div>
<p>The two principal components denote the <strong>main directions of variation within the data</strong>. In the plot above, the vector sizes are proportional to the related eigenvalues (hence the variance).</p>
<p>As we can see, the two directions are orthogonal. We can also project the data maintaining the original dimensionality without truncating matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> (in practice we set <span class="math notranslate nohighlight">\(M=D\)</span>) with:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Z} = \mathbf{X}\mathbf{W}^T\]</div>
<p>This process can also see as rotating the data in a way that the the first principal component becomes the new <span class="math notranslate nohighlight">\(x\)</span> axis, while the second principal component becomes the <span class="math notranslate nohighlight">\(y\)</span> axis. In this context, <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> can be seen as a <strong>rotation matrix</strong>. The plot below compares the original and rotated data:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/8945bf73ddfea53a80a2331c88602f4d2dfd8784e163e7435e9529c35f055df4.png" src="../_images/8945bf73ddfea53a80a2331c88602f4d2dfd8784e163e7435e9529c35f055df4.png" />
</div>
</div>
<p>We can see the projection to the first principal component in two stages:</p>
<ol class="arabic simple">
<li><p>Rotating the data;</p></li>
<li><p>Dropping the <span class="math notranslate nohighlight">\(y\)</span> axis.</p></li>
</ol>
<p>Note that the same reasoning can be seen in 3D as shown in the following example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/9c46df547042693251ba190df7b8fb8759f9ae03c218422b2fbda86c007e068e.png" src="../_images/9c46df547042693251ba190df7b8fb8759f9ae03c218422b2fbda86c007e068e.png" />
</div>
</div>
</section>
<section id="pca-for-data-whitening">
<h2>PCA for data whitening<a class="headerlink" href="#pca-for-data-whitening" title="Permalink to this heading">#</a></h2>
<p>It can also be seen that PCA transforms the original data with a given covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> to a new set of data (with new axes, the principal components), such that its covariance matrix <span class="math notranslate nohighlight">\(\Sigma'\)</span> is <strong>a diagonal matrix</strong>. This means that <strong>the new variables</strong> are now <strong>decorrelated</strong>.</p>
<p>Intuitively, this makes sense: <strong>we rotated the data so that the new axes are aligned to main directions of variation and we constructed principal components to be orthogonal to each other</strong>.</p>
<p>This process is also called <strong>whitening transformation</strong>. An example of this property is shown below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/cbbc469951b3b6c1377aff5b9d5e33ec6459e83ac4f111582be567da34d02c52.png" src="../_images/cbbc469951b3b6c1377aff5b9d5e33ec6459e83ac4f111582be567da34d02c52.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[3.60543087, 1.84308349],
        [1.84308349, 2.83596216]]),
 array([[ 1.00000000e+00, -1.40945288e-16],
        [-1.40945288e-16,  1.00000000e+00]]))
</pre></div>
</div>
</div>
</div>
<p>This whitening process can be useful when we need to make sure that the input variables are <strong>decorrelated</strong>. For instance, consider the case in which we need to fit a linear regression but we have multiple correlated variables. Pre-processing data with PCA would make sure that the input variables are decorrelated, hence removing any instability during fitting, but it should be also kept in mind that <strong>the linear regression would become less interpretable because of the PCA transformation</strong>.</p>
</section>
<section id="choosing-an-appropriate-number-of-components">
<h2>Choosing an Appropriate Number of Components<a class="headerlink" href="#choosing-an-appropriate-number-of-components" title="Permalink to this heading">#</a></h2>
<p>We have seen how PCA allows to reduce the dimensionality of the data. In short, this can be done by computing the first <span class="math notranslate nohighlight">\(M\)</span> eigenvalues and the associated eigenvectors.</p>
<p>In some cases, it is clear what is the value of <span class="math notranslate nohighlight">\(M\)</span> we need to set. For example, in the case of sorting Fisher’s Iris, we set <span class="math notranslate nohighlight">\(M=1\)</span> because we needed a scalar number. In other cases, we would like to reduce the dimensionality of the data, <strong>while keeping a reasonable amount of information about the original data</strong>.</p>
<p>We have seen that the variance is related to the MSE reprojection error and hence to the informative content. We can measure <strong>how much information we are retaining by selecting <span class="math notranslate nohighlight">\(M\)</span> components by measuring the cumulative variance of the first M components</strong>.</p>
<p>We will now consider as an example the whole Fisher Iris dataset, which has four features. The four Principal Components will be as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
    </tr>
    <tr>
      <th>components</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>z1</th>
      <td>0.361387</td>
      <td>0.656589</td>
      <td>-0.582030</td>
      <td>0.315487</td>
    </tr>
    <tr>
      <th>z2</th>
      <td>-0.084523</td>
      <td>0.730161</td>
      <td>0.597911</td>
      <td>-0.319723</td>
    </tr>
    <tr>
      <th>z3</th>
      <td>0.856671</td>
      <td>-0.173373</td>
      <td>0.076236</td>
      <td>-0.479839</td>
    </tr>
    <tr>
      <th>z4</th>
      <td>0.358289</td>
      <td>-0.075481</td>
      <td>0.545831</td>
      <td>0.753657</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The covariance matrix of the transformed data will be as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
</style>
<table id="T_fd31e">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_fd31e_level0_col0" class="col_heading level0 col0" >z1</th>
      <th id="T_fd31e_level0_col1" class="col_heading level0 col1" >z2</th>
      <th id="T_fd31e_level0_col2" class="col_heading level0 col2" >z3</th>
      <th id="T_fd31e_level0_col3" class="col_heading level0 col3" >z4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_fd31e_level0_row0" class="row_heading level0 row0" >z1</th>
      <td id="T_fd31e_row0_col0" class="data row0 col0" >4.23</td>
      <td id="T_fd31e_row0_col1" class="data row0 col1" >0.00</td>
      <td id="T_fd31e_row0_col2" class="data row0 col2" >0.00</td>
      <td id="T_fd31e_row0_col3" class="data row0 col3" >0.00</td>
    </tr>
    <tr>
      <th id="T_fd31e_level0_row1" class="row_heading level0 row1" >z2</th>
      <td id="T_fd31e_row1_col0" class="data row1 col0" >0.00</td>
      <td id="T_fd31e_row1_col1" class="data row1 col1" >0.24</td>
      <td id="T_fd31e_row1_col2" class="data row1 col2" >-0.00</td>
      <td id="T_fd31e_row1_col3" class="data row1 col3" >0.00</td>
    </tr>
    <tr>
      <th id="T_fd31e_level0_row2" class="row_heading level0 row2" >z3</th>
      <td id="T_fd31e_row2_col0" class="data row2 col0" >0.00</td>
      <td id="T_fd31e_row2_col1" class="data row2 col1" >-0.00</td>
      <td id="T_fd31e_row2_col2" class="data row2 col2" >0.08</td>
      <td id="T_fd31e_row2_col3" class="data row2 col3" >-0.00</td>
    </tr>
    <tr>
      <th id="T_fd31e_level0_row3" class="row_heading level0 row3" >z4</th>
      <td id="T_fd31e_row3_col0" class="data row3 col0" >0.00</td>
      <td id="T_fd31e_row3_col1" class="data row3 col1" >0.00</td>
      <td id="T_fd31e_row3_col2" class="data row3 col2" >-0.00</td>
      <td id="T_fd31e_row3_col3" class="data row3 col3" >0.02</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>As we could expect, the covariance matrix is diagonal because the new features are decorrelated. We also note that the principal components are sorted by variance. We will define the variance of the data along the <span class="math notranslate nohighlight">\(z_i\)</span> component as:</p>
<div class="math notranslate nohighlight">
\[var(z_i)\]</div>
<p>The total variance is given by the sum of the variances:</p>
<div class="math notranslate nohighlight">
\[V = var(z_1) + var(z_2) + \ldots + var(z_D)\]</div>
<p>More in general, we will define:</p>
<div class="math notranslate nohighlight">
\[V(n) = var(z_1) + var(z_2) + \ldots + var(z_n)\]</div>
<p>Hence <span class="math notranslate nohighlight">\(V=V(D)\)</span>.</p>
<p>In our example, the total variance <span class="math notranslate nohighlight">\(V\)</span> will be:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4.57
</pre></div>
</div>
</div>
</div>
<p>We can quantify the fraction of variance explained by each component <span class="math notranslate nohighlight">\(n\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\frac{var(n)}{V(D)}\]</div>
<p>If we compute such value for all <span class="math notranslate nohighlight">\(n\)</span>, we obtain the following vector in our example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.92461872, 0.05306648, 0.01710261, 0.00521218])
</pre></div>
</div>
</div>
</div>
<p>In practice, it is common to visualize the explained variance ratio in a <strong>scree plot</strong>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/a7ca335bf746e70c160022ec2a9f77e4265c0fd321663a9108709c60116cc405.png" src="../_images/a7ca335bf746e70c160022ec2a9f77e4265c0fd321663a9108709c60116cc405.png" />
</div>
</div>
<p>As we can see, the first component retains <span class="math notranslate nohighlight">\(92.46\%\)</span> of the variance, while the second one retains only <span class="math notranslate nohighlight">\(5.3\%\)</span> of the variance. It is common to also look at the cumulative variance ratio, defined as:</p>
<div class="math notranslate nohighlight">
\[\frac{V(n)}{V(D)}\]</div>
<p>In our case:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.92461872, 0.97768521, 0.99478782, 1.        ])
</pre></div>
</div>
</div>
</div>
<p>The vector above tell us that considering the first two components accounts for about <span class="math notranslate nohighlight">\(97.77\%\)</span> of the variance.</p>
<p>This approach can guide us in choosing the right number of dimensions to keep, selecting a budget of the variance we wish to be able to retain.</p>
</section>
<section id="interpretation-of-the-principal-components-load-plots">
<h2>Interpretation of the Principal Components - Load Plots<a class="headerlink" href="#interpretation-of-the-principal-components-load-plots" title="Permalink to this heading">#</a></h2>
<p>When we transform data with PCA, the new variables have a less direct interpretation. Indeed, if a given variable is a linear combination of other variables, it is not straightforward to assign a meaning to each component.</p>
<p>However, we know that the first components are the ones which contain most of the variance. Hence, inspecting the weights that each of these components given to the original features can give some insights into which of the original features are more relevant. In this context, <strong>the weights of the principal components are also called loadings</strong>. Loadings with <strong>large absolute values are more influential in determining the value of the principal components</strong>.</p>
<p>In practice, we can assess the relevance of features with a <strong>load plot</strong>, which represents each original variable as a 2D point of coordinates given by the loadings corresponding to the first two principal components. Let us show the load plot for our example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/0db07c84856353490e67544809485e1826d965a4cafcb504eec4f7a968e2b9f1.png" src="../_images/0db07c84856353490e67544809485e1826d965a4cafcb504eec4f7a968e2b9f1.png" />
</div>
</div>
<p>In a load plot:</p>
<ul class="simple">
<li><p>variables that cluster together are positively correlated, while variables on opposite sides may be negatively correlated;</p></li>
<li><p>variables which are further from the origin have higher loadings, so they are more influential in the computation of the PCA.</p></li>
</ul>
<p>A loading plot can also be useful to perform feature selection. For instance, from the plot above all variables seem to be influential, but <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">width</span></code> and <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">length</span></code> are correlated. We could think of using as a subset of variables <code class="docutils literal notranslate"><span class="pre">sepal</span> <span class="pre">width</span></code>, <code class="docutils literal notranslate"><span class="pre">sepal</span> <span class="pre">length</span></code> and one of the other two (e.g., <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">length</span></code>).</p>
</section>
<section id="laboratory-applications-of-pca">
<h2>Laboratory: Applications of PCA<a class="headerlink" href="#laboratory-applications-of-pca" title="Permalink to this heading">#</a></h2>
<p>PCA has several applications in data analysis. We will summarize the main ones in the following.</p>
<section id="dimensionality-reduction-and-visualization">
<h3>Dimensionality Reduction and Visualization<a class="headerlink" href="#dimensionality-reduction-and-visualization" title="Permalink to this heading">#</a></h3>
<p>We have seen that data can be interpreted as a set of D-dimensional points. When <span class="math notranslate nohighlight">\(D=2\)</span>, it is often useful to visualize the data through a scatterplot. This allows us to see how the data distributes in the space. When <span class="math notranslate nohighlight">\(D&gt;2\)</span>, we can show a series a scatterplot (a pairplot or scattermatrix). However, when <span class="math notranslate nohighlight">\(D\)</span> is very large, it is usually unfeasible to visualize data with all possible scatterplots.</p>
<p>In these cases, it can be useful to transform the data with PCA and then visualize the data points as 2D points in the space identified by the first two principal components.</p>
<p>We will show an example on the multidimensional dataset <strong>DIGITS</strong>. The dataset contains small images of resolution <span class="math notranslate nohighlight">\(8 \times 8 pixels\)</span> representing handwritten digits from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(9\)</span>. We can see the dataset as containing <span class="math notranslate nohighlight">\(64\)</span> variables. Each variable indicates the pixel value at a specific location.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span><span class="o">=</span><span class="n">load_digits</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/eb0d64452815a28a3914ffc48bfd20fb0e8bf405080d3c1520707d2493277e80.png" src="../_images/eb0d64452815a28a3914ffc48bfd20fb0e8bf405080d3c1520707d2493277e80.png" />
</div>
</div>
<p>We can visualize the data with PCA by first projecting the data to <span class="math notranslate nohighlight">\(M=2\)</span> principal components, then plotting the transformed data as 2D points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span><span class="o">=</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">Y</span><span class="o">=</span><span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">legend</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="o">==</span><span class="n">c</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">Y</span><span class="p">[</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="o">==</span><span class="n">c</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">legend</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legend</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/83e7564983512b8b39a28ed2111e6fe63b1fe87afcb0d752680b1646c25ccbbb.png" src="../_images/83e7564983512b8b39a28ed2111e6fe63b1fe87afcb0d752680b1646c25ccbbb.png" />
</div>
</div>
</section>
<section id="data-decorrelation-principal-component-regression">
<h3>Data Decorrelation - Principal Component Regression<a class="headerlink" href="#data-decorrelation-principal-component-regression" title="Permalink to this heading">#</a></h3>
<p>PCA is also useful when we need to fit a linear regressor and we are in the presence of a dataset with multi-collinearity.</p>
<p>Let us consider the <code class="docutils literal notranslate"><span class="pre">auto_mpg</span></code> dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">zscore</span>
<span class="kn">from</span> <span class="nn">ucimlrepo</span> <span class="kn">import</span> <span class="n">fetch_ucirepo</span> 
<span class="kn">from</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">import</span> <span class="n">ols</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
  
<span class="c1"># fetch dataset </span>
<span class="n">auto_mpg</span> <span class="o">=</span> <span class="n">fetch_ucirepo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">auto_mpg</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">features</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">auto_mpg</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span> 
  
<span class="n">data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>displacement</th>
      <th>cylinders</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>model_year</th>
      <th>origin</th>
      <th>mpg</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>307.0</td>
      <td>8</td>
      <td>130.0</td>
      <td>3504</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>350.0</td>
      <td>8</td>
      <td>165.0</td>
      <td>3693</td>
      <td>11.5</td>
      <td>70</td>
      <td>1</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>318.0</td>
      <td>8</td>
      <td>150.0</td>
      <td>3436</td>
      <td>11.0</td>
      <td>70</td>
      <td>1</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>304.0</td>
      <td>8</td>
      <td>150.0</td>
      <td>3433</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>16.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>302.0</td>
      <td>8</td>
      <td>140.0</td>
      <td>3449</td>
      <td>10.5</td>
      <td>70</td>
      <td>1</td>
      <td>17.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>393</th>
      <td>140.0</td>
      <td>4</td>
      <td>86.0</td>
      <td>2790</td>
      <td>15.6</td>
      <td>82</td>
      <td>1</td>
      <td>27.0</td>
    </tr>
    <tr>
      <th>394</th>
      <td>97.0</td>
      <td>4</td>
      <td>52.0</td>
      <td>2130</td>
      <td>24.6</td>
      <td>82</td>
      <td>2</td>
      <td>44.0</td>
    </tr>
    <tr>
      <th>395</th>
      <td>135.0</td>
      <td>4</td>
      <td>84.0</td>
      <td>2295</td>
      <td>11.6</td>
      <td>82</td>
      <td>1</td>
      <td>32.0</td>
    </tr>
    <tr>
      <th>396</th>
      <td>120.0</td>
      <td>4</td>
      <td>79.0</td>
      <td>2625</td>
      <td>18.6</td>
      <td>82</td>
      <td>1</td>
      <td>28.0</td>
    </tr>
    <tr>
      <th>397</th>
      <td>119.0</td>
      <td>4</td>
      <td>82.0</td>
      <td>2720</td>
      <td>19.4</td>
      <td>82</td>
      <td>1</td>
      <td>31.0</td>
    </tr>
  </tbody>
</table>
<p>398 rows × 8 columns</p>
</div></div></div>
</div>
<p>If we try to fit a linear regressor with all variables, we will find multicollinearity:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">import</span> <span class="n">ols</span>
<span class="n">ols</span><span class="p">(</span><span class="s2">&quot;mpg ~ displacement + cylinders + horsepower + weight + acceleration + model_year + origin&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>  -17.2184</td> <td>    4.644</td> <td>   -3.707</td> <td> 0.000</td> <td>  -26.350</td> <td>   -8.087</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.0199</td> <td>    0.008</td> <td>    2.647</td> <td> 0.008</td> <td>    0.005</td> <td>    0.035</td>
</tr>
<tr>
  <th>cylinders</th>    <td>   -0.4934</td> <td>    0.323</td> <td>   -1.526</td> <td> 0.128</td> <td>   -1.129</td> <td>    0.142</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.0170</td> <td>    0.014</td> <td>   -1.230</td> <td> 0.220</td> <td>   -0.044</td> <td>    0.010</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.0065</td> <td>    0.001</td> <td>   -9.929</td> <td> 0.000</td> <td>   -0.008</td> <td>   -0.005</td>
</tr>
<tr>
  <th>acceleration</th> <td>    0.0806</td> <td>    0.099</td> <td>    0.815</td> <td> 0.415</td> <td>   -0.114</td> <td>    0.275</td>
</tr>
<tr>
  <th>model_year</th>   <td>    0.7508</td> <td>    0.051</td> <td>   14.729</td> <td> 0.000</td> <td>    0.651</td> <td>    0.851</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.4261</td> <td>    0.278</td> <td>    5.127</td> <td> 0.000</td> <td>    0.879</td> <td>    1.973</td>
</tr>
</table></div></div>
</div>
<p>Indeed, some variables have a large p-value, probably because they are correlated with the others.</p>
<p>To perform Principal Component Regression, we can first compute PCA <strong>on the independent variables</strong>, then fit a linear regressor. We will choose <span class="math notranslate nohighlight">\(M=4\)</span> principal components:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dd</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dd</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;mpg&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">zz</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">dd</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;mpg&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">data2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="sa">f</span><span class="s2">&quot;z</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">zz</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">zz</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="p">})</span>

<span class="n">data2</span><span class="o">=</span><span class="n">data2</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dd</span><span class="p">[</span><span class="s1">&#39;mpg&#39;</span><span class="p">])</span>

<span class="c1">#print(&quot;Cumulative explained variance:&quot;, pca.explained_variance_ratio_.cumsum())</span>

<span class="c1"># Get explained variance ratios</span>
<span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>

<span class="c1"># Calculate cumulative explained variance</span>
<span class="n">cumulative_explained_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span>

<span class="c1"># Plot scree plot with both bar plot for variance and line plot for cumulative explained variance</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Bar plot for explained variance ratio</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Scree Plot for PCA&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Principal Component&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Explained Variance Ratio&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Line plot for cumulative explained variance</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumulative_explained_variance</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">cumulative_explained_variance</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cumulative Explained Variance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Principal Components&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Explained Variance&#39;</span><span class="p">)</span>

<span class="c1"># Show legend</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">ols</span><span class="p">(</span><span class="s2">&quot;mpg ~ z1 + z2 + z3 + z4&quot;</span><span class="p">,</span> <span class="n">data2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/af4af1e1fb5ae74f4a64da9a520cf5598cd4132095616fd2ac960b06b9161186.png" src="../_images/af4af1e1fb5ae74f4a64da9a520cf5598cd4132095616fd2ac960b06b9161186.png" />
<div class="output text_html"><table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.492</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.487</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   92.20</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 16 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>9.24e-55</td>
</tr>
<tr>
  <th>Time:</th>                 <td>20:05:49</td>     <th>  Log-Likelihood:    </th> <td> -1207.7</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   386</td>      <th>  AIC:               </th> <td>   2425.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   381</td>      <th>  BIC:               </th> <td>   2445.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   23.3607</td> <td>    0.283</td> <td>   82.480</td> <td> 0.000</td> <td>   22.804</td> <td>   23.918</td>
</tr>
<tr>
  <th>z1</th>        <td>   -0.0051</td> <td>    0.000</td> <td>  -15.346</td> <td> 0.000</td> <td>   -0.006</td> <td>   -0.004</td>
</tr>
<tr>
  <th>z2</th>        <td>   -0.0320</td> <td>    0.007</td> <td>   -4.373</td> <td> 0.000</td> <td>   -0.046</td> <td>   -0.018</td>
</tr>
<tr>
  <th>z3</th>        <td>   -0.0552</td> <td>    0.018</td> <td>   -3.138</td> <td> 0.002</td> <td>   -0.090</td> <td>   -0.021</td>
</tr>
<tr>
  <th>z4</th>        <td>    0.8815</td> <td>    0.086</td> <td>   10.234</td> <td> 0.000</td> <td>    0.712</td> <td>    1.051</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 3.702</td> <th>  Durbin-Watson:     </th> <td>   1.125</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.157</td> <th>  Jarque-Bera (JB):  </th> <td>   3.435</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.214</td> <th>  Prob(JB):          </th> <td>   0.179</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.174</td> <th>  Cond. No.          </th> <td>    860.</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div></div>
</div>
<p>While the result is not as interpretable as it was before, this technique may be useful in the case of predictive analysis.</p>
</section>
<section id="data-compression">
<h3>Data Compression<a class="headerlink" href="#data-compression" title="Permalink to this heading">#</a></h3>
<p>Now let’s see a simple example of data compression using PCA. In particular, we will consider the case of compressing images. An image can be seen as high-dimensional data, where the number of dimensions is equal to the number of pixels. For example, an RGB image of size <span class="math notranslate nohighlight">\(640 \times 480\)</span> pixels has <span class="math notranslate nohighlight">\(3 \cdot 640 \cdot 480=921600\)</span> dimensions.</p>
<p>We expect that there will be redundant information in all these dimensions. One method to compress images is to divide them into fixed-size blocks (e.g., <span class="math notranslate nohighlight">\(8 \times 8\)</span>). Each of these blocks will be an element belonging to a population (the population of <span class="math notranslate nohighlight">\(8 \times 8\)</span> blocks of the image).</p>
<p>Assuming that the information in the blocks is highly correlated, we can try to compress it by applying PCA to the sample of blocks extracted from our image and choosing only a few principal components to represent the content of the blocks.</p>
<p>We will consider the following example image:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_sample_image</span>
<span class="n">flower</span> <span class="o">=</span> <span class="n">load_sample_image</span><span class="p">(</span><span class="s1">&#39;flower.jpg&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Image Size:&quot;</span><span class="p">,</span><span class="n">flower</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of dimensions:&quot;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">flower</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">flower</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image Size: (427, 640, 3)
Number of dimensions: 819840
</pre></div>
</div>
<img alt="../_images/ba01f187f53c4dbadbb1c97c4dc9c60be7a2ec37942ee73093a51bef67b048e4.png" src="../_images/ba01f187f53c4dbadbb1c97c4dc9c60be7a2ec37942ee73093a51bef67b048e4.png" />
</div>
</div>
<p>We will divide the image into RGB blocks of size <span class="math notranslate nohighlight">\(8 \times 8 \times 3\)</span> (these are <span class="math notranslate nohighlight">\(8 \times 8\)</span> RGB images). These will look as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">flower</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="mi">4270</span><span class="p">)</span>
<span class="n">tiles</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tiles</span><span class="p">)</span>
<span class="n">tiles</span><span class="o">.</span><span class="n">shape</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">141</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tiles</span><span class="p">[</span><span class="mi">1855</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">142</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tiles</span><span class="p">[</span><span class="mi">2900</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">143</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tiles</span><span class="p">[</span><span class="mi">1856</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">144</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tiles</span><span class="p">[</span><span class="mi">960</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/169a39442add329cf7db3568ddba0ede1742b0e17c124f56ba1365d4f4023a5d.png" src="../_images/169a39442add329cf7db3568ddba0ede1742b0e17c124f56ba1365d4f4023a5d.png" />
</div>
</div>
<p>If we compute the PCA of all blocks, we will obtain the following cumulative variance ratio vector:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span><span class="o">=</span><span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tiles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)[:</span><span class="mi">32</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.72033322, 0.8054948 , 0.88473857, 0.91106289, 0.92773705,
       0.94002709, 0.94742984, 0.95262262, 0.95724086, 0.96138203,
       0.96493482, 0.96813026, 0.97074765, 0.97304417, 0.97502487,
       0.97676849, 0.97827393, 0.97961611, 0.98080084, 0.98195037,
       0.98298693, 0.98399279, 0.98491245, 0.98570784, 0.98649079,
       0.98719466, 0.98779928, 0.98835152, 0.98888528, 0.98937606,
       0.98983658, 0.99027875])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bar plot for explained variance ratio</span>
<span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="n">cumulative_explained_variance</span> <span class="o">=</span> <span class="n">explained_variance_ratio</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Scree Plot for PCA&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Principal Component&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Explained Variance Ratio&#39;</span><span class="p">)</span>
<span class="c1">#plt.xticks(np.arange(1, len(explained_variance_ratio) + 1))</span>

<span class="c1"># Line plot for cumulative explained variance</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumulative_explained_variance</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">cumulative_explained_variance</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cumulative Explained Variance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Principal Components&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Explained Variance&#39;</span><span class="p">)</span>

<span class="c1"># Show legend</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/250686531c8e5b5089269ac61e524428871d7979a95ca153c14e49c2598c37a7.png" src="../_images/250686531c8e5b5089269ac61e524428871d7979a95ca153c14e49c2598c37a7.png" />
</div>
</div>
<p>As we can see, truncating at the first component allows us to retain about <span class="math notranslate nohighlight">\(72%\)</span> of the information, truncating at the second allows us to retain about <span class="math notranslate nohighlight">\(80%\)</span>, and so on, up to <span class="math notranslate nohighlight">\(32\)</span> components, which allow us to retain about <span class="math notranslate nohighlight">\(99%\)</span> of the information. Now let’s see how to compress and reconstruct the image. We will choose the first <span class="math notranslate nohighlight">\(32\)</span> components, preserving <span class="math notranslate nohighlight">\(99%\)</span> of the information.</p>
<p>If we do so, and then project the tiles to the compressed space each tile will be represented by only <span class="math notranslate nohighlight">\(32\)</span> numbers. This will lead to the following savings in space:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tiles</span><span class="p">)</span>
<span class="n">compressed_tiles</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">tiles</span><span class="p">)</span>
<span class="n">compressed_tiles</span><span class="o">.</span><span class="n">shape</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Space saved in the compression: </span><span class="si">{:0.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span><span class="o">-</span><span class="mi">100</span><span class="o">*</span><span class="n">compressed_tiles</span><span class="o">.</span><span class="n">size</span><span class="o">/</span><span class="n">tiles</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Space saved in the compression: 83.33%
</pre></div>
</div>
</div>
</div>
<p>We can reconstruct the original image by applying the inverse PCA transformation to the compressed patches. The result will be as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reconstructed_tiles</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">compressed_tiles</span><span class="p">)</span>
<span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">shape</span>

<span class="n">reconstructed_tiles</span> <span class="o">=</span> <span class="p">(</span><span class="n">reconstructed_tiles</span><span class="o">-</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>\
            <span class="o">/</span><span class="p">(</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">min</span><span class="p">())</span><span class="o">*</span><span class="mi">255</span>
<span class="n">reconstructed_tiles</span><span class="o">=</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>

<span class="n">reconstructed_flower</span><span class="o">=</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">flower</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">flower</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Reconstructed&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">reconstructed_flower</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/44aef220c1367a60e0bba025754ba93f1676d1de2e45676b890f177703650489.png" src="../_images/44aef220c1367a60e0bba025754ba93f1676d1de2e45676b890f177703650489.png" />
</div>
</div>
<p>The plot below shows how the reconstruction quality increases when more components are used:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compress</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
    <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tiles</span><span class="p">)</span>
    <span class="n">compressed_tiles</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">tiles</span><span class="p">)</span>
    <span class="n">compressed_tiles</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">saved_sapce</span> <span class="o">=</span> <span class="mi">100</span><span class="o">-</span><span class="mi">100</span><span class="o">*</span><span class="n">compressed_tiles</span><span class="o">.</span><span class="n">size</span><span class="o">/</span><span class="n">tiles</span><span class="o">.</span><span class="n">size</span>

    <span class="n">reconstructed_tiles</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">compressed_tiles</span><span class="p">)</span>
    <span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">reconstructed_tiles</span> <span class="o">=</span> <span class="p">(</span><span class="n">reconstructed_tiles</span><span class="o">-</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>\
                <span class="o">/</span><span class="p">(</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">min</span><span class="p">())</span><span class="o">*</span><span class="mi">255</span>
    <span class="n">reconstructed_tiles</span><span class="o">=</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>

    <span class="n">reconstructed_flower</span><span class="o">=</span><span class="n">reconstructed_tiles</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">flower</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reconstructed_flower</span><span class="p">,</span> <span class="n">saved_sapce</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">192</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">reconstructed</span><span class="p">,</span> <span class="n">saved_space</span> <span class="o">=</span> <span class="n">compress</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">reconstructed</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;M=</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2"> Comp: </span><span class="si">{</span><span class="n">saved_space</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a960a5f0b6107b36632376b017eefddea9fad35dd58b3f90ef6f034a7b57dc08.png" src="../_images/a960a5f0b6107b36632376b017eefddea9fad35dd58b3f90ef6f034a7b57dc08.png" />
</div>
</div>
</section>
<section id="eigenfaces">
<h3>Eigenfaces<a class="headerlink" href="#eigenfaces" title="Permalink to this heading">#</a></h3>
<p>When PCA is applied to a dataset of faces, the <strong>Principal Components (the eigenvectors)</strong> are no longer abstract directions; they can be visualized as “ghostly” faces themselves, known as <strong>“Eigenfaces”</strong>.</p>
<p>Each Eigenface represents a fundamental “component” or “feature” of human faces in the dataset (e.g., “lighting from the left,” “presence of a nose,” “smiling vs. frowning”).</p>
<section id="the-goal">
<h4>The Goal<a class="headerlink" href="#the-goal" title="Permalink to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Load</strong> a dataset of 400 human faces. Each image is 64x64, meaning we have <strong>4,096 features</strong> (pixels).</p></li>
<li><p><strong>Fit</strong> PCA to the dataset to find the “Eigenfaces.”</p></li>
<li><p><strong>Visualize</strong> these new “features.”</p></li>
<li><p><strong>Compress</strong> an image by reducing it from 4,096 dimensions to just 20.</p></li>
<li><p><strong>Reconstruct</strong> the face from the compressed 20-component version to see how much information we retained.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_olivetti_faces</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># 1. Load the faces dataset</span>
<span class="c1"># 400 images, each 64x64 (4096 features)</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_olivetti_faces</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">target</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Faces dataset shape: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 2. Fit PCA</span>
<span class="c1"># We&#39;ll ask for 20 components</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="mi">20</span>
<span class="c1"># whiten=True scales the components, often better for visualization</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># The &quot;Eigenfaces&quot; are just the principal components (the eigenvectors)</span>
<span class="n">eigenfaces</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span>
<span class="c1"># The &quot;average face&quot; that PCA centers the data on</span>
<span class="n">mean_face</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">mean_</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Eigenfaces (components) shape: </span><span class="si">{</span><span class="n">eigenfaces</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Faces dataset shape: (400, 4096)
Eigenfaces (components) shape: (20, 4096)
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualizing-the-principal-components-eigenfaces">
<h4>Visualizing the Principal Components (Eigenfaces)<a class="headerlink" href="#visualizing-the-principal-components-eigenfaces" title="Permalink to this heading">#</a></h4>
<p>Let’s plot the “average face” (the mean) and the first 12 “Eigenfaces” (PC1, PC2, etc.). These are the fundamental “building blocks” of all the faces in the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_gallery</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">titles</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n_row</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_col</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function to plot a gallery of portraits&quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">1.8</span> <span class="o">*</span> <span class="n">n_col</span><span class="p">,</span> <span class="mf">2.4</span> <span class="o">*</span> <span class="n">n_row</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">.99</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">.90</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">.35</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_row</span> <span class="o">*</span> <span class="n">n_col</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_row</span><span class="p">,</span> <span class="n">n_col</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>

<span class="c1"># 1. Plot the &quot;average face&quot; (the mean that PCA subtracted)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mean_face</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Average Face (Mean)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 2. Plot the gallery of the first 12 eigenfaces</span>
<span class="n">eigenface_titles</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Eigenface </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">eigenfaces</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
<span class="n">plot_gallery</span><span class="p">(</span><span class="n">eigenfaces</span><span class="p">,</span> <span class="n">eigenface_titles</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0f6e9bf2ba92598c21c7926bc3ee8f00bada2d83afddd16455feb7f8dba152e4.png" src="../_images/0f6e9bf2ba92598c21c7926bc3ee8f00bada2d83afddd16455feb7f8dba152e4.png" />
<img alt="../_images/7b30abb0b19d01a010f8a58e70c73ac3157a250eeab3b3ef67d0916f22afbe76.png" src="../_images/7b30abb0b19d01a010f8a58e70c73ac3157a250eeab3b3ef67d0916f22afbe76.png" />
</div>
</div>
</section>
<section id="interpretation">
<h4>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this heading">#</a></h4>
<p>As you can see, the “Eigenfaces” are not faces, but “ghostly” components.</p>
<ul class="simple">
<li><p><strong>PC1 (“Eigenface 1”)</strong> captures the main component of variance: the direction of the lighting (light from the left vs. light from the right).</p></li>
<li><p><strong>Other components</strong> learn to represent other key features: “wearing glasses vs. not,” “smiling vs. frowning,” “wide nose vs. narrow nose,” etc.</p></li>
</ul>
</section>
<section id="data-compression-and-reconstruction">
<h4>Data Compression and Reconstruction<a class="headerlink" href="#data-compression-and-reconstruction" title="Permalink to this heading">#</a></h4>
<p>Now for the magic. We can take any face and “compress” it by describing it as a <em>combination</em> of these eigenfaces.</p>
<p>We will take one test face and show its reconstruction using just 5, 20, 50, and 150 components.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Take one face from the dataset</span>
<span class="n">test_face</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># 1. Transform the face into its &quot;component weights&quot;</span>
<span class="c1"># This &quot;compresses&quot; the 4096-pixel vector into a 20-number vector</span>
<span class="n">face_weights</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_face</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># 2. Reconstruct the face using different numbers of components</span>
<span class="n">n_comps_to_try</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">reconstructed_faces</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_comps_to_try</span><span class="p">:</span>
    <span class="c1"># Get the weights for just the first &#39;n&#39; components</span>
    <span class="n">weights_n</span> <span class="o">=</span> <span class="n">face_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="n">n</span><span class="p">]</span>
    
    <span class="c1"># Get the &quot;eigenfaces&quot; (components) for just the first &#39;n&#39;</span>
    <span class="n">components_n</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="p">:]</span>
    
    <span class="c1"># &quot;Rebuild&quot; the face by summing the (weight * eigenface)</span>
    <span class="c1"># We must also add back the &quot;average face&quot; we subtracted</span>
    <span class="n">reconstructed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_n</span><span class="p">,</span> <span class="n">components_n</span><span class="p">)</span> <span class="o">+</span> <span class="n">pca</span><span class="o">.</span><span class="n">mean_</span>
    <span class="n">reconstructed_faces</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reconstructed</span><span class="p">)</span>

<span class="c1"># --- Plot the results ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">test_face</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Original (4096D)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">face</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reconstructed_faces</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">face</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reconstructed (K=</span><span class="si">{</span><span class="n">n_comps_to_try</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;PCA for Image Compression and Reconstruction&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/de7dbcf9c1f3aa520c604e234c7fb3ddace9be75ecb2fbe1d5c405a50ab5353b.png" src="../_images/de7dbcf9c1f3aa520c604e234c7fb3ddace9be75ecb2fbe1d5c405a50ab5353b.png" />
</div>
</div>
</section>
</section>
<section id="are-these-eigenface-features-actually-better">
<h3>Are These “Eigenface” Features Actually Better?<a class="headerlink" href="#are-these-eigenface-features-actually-better" title="Permalink to this heading">#</a></h3>
<p>So far, we’ve seen that PCA can <em>compress</em> a 4096-pixel image into a 20-number vector, and then reconstruct it. This is great for storage.</p>
<p>But is this new, 20-dimensional vector a <em>better feature set</em> for a machine learning model?</p>
<p>Let’s run a “bake-off” to find out. We will build a <strong>face recognition</strong> classifier (using KNN) and test it on two different feature sets:</p>
<ol class="arabic simple">
<li><p><strong>Baseline Model:</strong> KNN trained on the <strong>raw 4096-pixel</strong> vectors.</p></li>
<li><p><strong>PCA Model:</strong> KNN trained on our <strong>“smart” 20-component</strong> vectors.</p></li>
</ol>
<p><strong>Our Hypothesis:</strong> The PCA model will be <em>faster</em> to train (fewer features) and <em>more accurate</em> (less noise).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_olivetti_faces</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="c1"># --- 1. Load the data ---</span>
<span class="c1"># This was the missing step. We must load the data first.</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_olivetti_faces</span><span class="p">()</span>
<span class="n">X_faces</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">data</span>
<span class="n">y_faces</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">target</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded faces dataset. Shape: </span><span class="si">{</span><span class="n">X_faces</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># --- 2. Create our single Train/Test split ---</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_faces</span><span class="p">,</span> <span class="n">y_faces</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_faces</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training on </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples, testing on </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loaded faces dataset. Shape: (400, 4096)
Training on 280 samples, testing on 120 samples.
</pre></div>
</div>
</div>
</div>
<section id="baseline-knn-on-4096-raw-pixels">
<h4>Baseline: KNN on 4096 Raw Pixels<a class="headerlink" href="#baseline-knn-on-4096-raw-pixels" title="Permalink to this heading">#</a></h4>
<p>First, our baseline. We build a pipeline that scales the 4096 pixel values and then runs the KNN classifier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create a simple pipeline</span>
<span class="n">pipe_knn_raw</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># 2. Fit and time the model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training KNN on 4096 RAW pixel features...&quot;</span><span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">pipe_knn_raw</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1"># 3. Evaluate on the test set</span>
<span class="n">y_pred_raw</span> <span class="o">=</span> <span class="n">pipe_knn_raw</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">acc_raw</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_raw</span><span class="p">)</span>
<span class="n">time_raw</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Accuracy: </span><span class="si">{</span><span class="n">acc_raw</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training KNN on 4096 RAW pixel features...
  Accuracy: 79.17%
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-2-knn-on-20-smart-pca-features">
<h4>Model 2: KNN on 20 “Smart” PCA Features<a class="headerlink" href="#model-2-knn-on-20-smart-pca-features" title="Permalink to this heading">#</a></h4>
<p>Now, we’ll build a complete pipeline that does all the steps at once:</p>
<ol class="arabic simple">
<li><p>Scales the data.</p></li>
<li><p>Performs PCA to reduce 4096D <span class="math notranslate nohighlight">\(\to\)</span> 150D.</p></li>
<li><p>Trains the KNN classifier on the new 150D features.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create the full PCA pipeline</span>
<span class="n">pipe_knn_pca</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;pca&#39;</span><span class="p">,</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># 2. Fit and time the model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training KNN on 150 PCA features...&quot;</span><span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">pipe_knn_pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1"># 3. Evaluate on the test set</span>
<span class="n">y_pred_pca</span> <span class="o">=</span> <span class="n">pipe_knn_pca</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">acc_pca</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_pca</span><span class="p">)</span>
<span class="n">time_pca</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Accuracy: </span><span class="si">{</span><span class="n">acc_pca</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training KNN on 150 PCA features...
  Accuracy: 83.33%
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h3>
<p>Let’s look at the results.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Model</p></th>
<th class="head text-left"><p>Features</p></th>
<th class="head text-left"><p>Accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Baseline</strong></p></td>
<td class="text-left"><p>4096 Raw Pixels</p></td>
<td class="text-left"><p>~79.15%</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>PCA Model</strong></p></td>
<td class="text-left"><p>150 PCA Features</p></td>
<td class="text-left"><p><strong>~83.33%</strong></p></td>
</tr>
</tbody>
</table>
<p>PCA didn’t just <em>compress</em> the data; it <strong>improved</strong> it. By creating a cleaner, low-dimensional feature space, it allowed our simple KNN classifier to find the true patterns and make far more accurate predictions.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Parts of Chapter 12 of [1]</p></li>
</ul>
<p>[1] Bishop, Christopher M., and Nasser M. Nasrabadi. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="17_density_estimation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Density Estimation</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-vs-feature-reduction">Feature Selection vs Feature Reduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-reduction-example">Feature Reduction Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-formulation">General Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-our-example">Back to Our Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-for-data-whitening">PCA for data whitening</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-an-appropriate-number-of-components">Choosing an Appropriate Number of Components</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-principal-components-load-plots">Interpretation of the Principal Components - Load Plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laboratory-applications-of-pca">Laboratory: Applications of PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-and-visualization">Dimensionality Reduction and Visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-decorrelation-principal-component-regression">Data Decorrelation - Principal Component Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-compression">Data Compression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenfaces">Eigenfaces</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-goal">The Goal</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-principal-components-eigenfaces">Visualizing the Principal Components (Eigenfaces)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-compression-and-reconstruction">Data Compression and Reconstruction</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#are-these-eigenface-features-actually-better">Are These “Eigenface” Features Actually Better?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#baseline-knn-on-4096-raw-pixels">Baseline: KNN on 4096 Raw Pixels</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2-knn-on-20-smart-pca-features">Model 2: KNN on 20 “Smart” PCA Features</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>