

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Linear Regression &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/09_linear_regression';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Introduction to Predictive Analysis" href="08_predictive_modeling.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamentals of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">Introduction to the Labs and Work Environment Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_python_crash_course.html">Python Crash Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_python_data_science_crash_course.html">Python for Data Science Crash Course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro_data_analysis.html">Data Analysis Key Concepts, Loading and Inspecting the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="02_describing_and_visualizing_the_data.html">Describing and Visualizing the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_probability_for_data_analysis.html">Probability for Data Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_association_between_variables.html">Association between variables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05_data_distributions.html">Data Distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_statistical_inference.html">Statistical Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="07_storytelling_with_data.html">Storytelling with Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_predictive_modeling.html">Introduction to Predictive Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear Regression</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes2526/blob/master/lecturenotes/lectures/09_linear_regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526/issues/new?title=Issue%20on%20page%20%2Flectures/09_linear_regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/09_linear_regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-auto-mpg-dataset">The Auto MPG Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-models">Regression Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">Simple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-coefficients-ordinary-least-squares-ols">Estimating the Coefficients - Ordinary Least Squares (OLS)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">Empirical Risk Minimization</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-coefficients-of-linear-regression">Interpretation of the Coefficients of Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-of-the-coefficient-estimates">Accuracy of the Coefficient Estimates</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-of-the-regression-coefficients">Confidence Intervals of the Regression Coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-tests-for-the-significance-of-coefficients">Statistical Tests for the Significance of Coefficients</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-model-accuracy">Assessing Model Accuracy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-building-block-residuals-and-rss">The Building Block: Residuals and RSS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-understanding-goodness-of-fit">Metrics for Understanding (Goodness-of-Fit)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-standard-error-rse">1. Residual Standard Error (RSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-r-2-statistic">2. The <span class="math notranslate nohighlight">\(R^2\)</span> Statistic</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-prediction-test-set-error">Metrics for Prediction (Test Set Error)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">1. Mean Squared Error (MSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">2. Root Mean Squared Error (RMSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">3. Mean Absolute Error (MAE)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-examples">Visual Examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-plots-checking-model-validity">Diagnostic Plots: Checking Model Validity</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-residuals-vs-fitted-plot">1. The Residuals vs. Fitted Plot</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-q-q-plot-quantile-quantile-plot">2. The Q-Q Plot (Quantile-Quantile Plot)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">Multiple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometrical-interpretation">Geometrical Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-interpretation">Statistical Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-regression-coefficients">Estimating the Regression Coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-f-test">The F-Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection-by-backward-elimination">Variable Selection by Backward Elimination</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collinearity-and-the-instability-of-least-squares">Collinearity and the Instability of Least Squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-2">Adjusted <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qualitative-predictors">Qualitative Predictors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictors-with-only-two-levels">Predictors with Only Two Levels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictors-with-more-than-two-levels">Predictors with More than Two Levels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-in-practice">Linear Regression in Practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Simple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-the-top-table-model-metadata">Part 1: The Top Table (Model Metadata)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-the-coefficients-table-the-core">Part 2: The Coefficients Table (The Core)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-the-bottom-table-residual-diagnostics">Part 3: The Bottom Table (Residual Diagnostics)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-diagnostics">Residual Diagnostics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-diagnostic-plots">Interpreting the Diagnostic Plots</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Multiple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-summary-table-mlr">Interpreting the Summary Table (MLR)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection">Variable Selection</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h1>
<p>Linear regression is a fundamental and widely used statistical technique in data analysis and machine learning. It is a powerful tool for <strong>modeling and understanding the relationships between variables</strong> and <strong>make predictions on unseen data</strong>. At its core, linear regression aims to model a linear relationship between a <strong>dependent variable</strong> (the one you want to predict) and <strong>one or more independent variables</strong> (the ones used for prediction). This technique allows us to make predictions, infer associations, and gain insights into how changes in independent variables influence the target variable. Linear regression is both intuitive and versatile, making it a valuable tool for tasks ranging from simple trend analysis to more complex predictive modeling and hypothesis testing.</p>
<p>In this context, we will explore the concepts and applications of linear regression, its different types, and how to implement it using Python.</p>
<section id="the-auto-mpg-dataset">
<h2>The Auto MPG Dataset<a class="headerlink" href="#the-auto-mpg-dataset" title="Permalink to this heading">#</a></h2>
<p>We will consider the <a class="reference external" href="https://archive.ics.uci.edu/dataset/9/auto+mpg">Auto MPG</a> dataset, which contains <span class="math notranslate nohighlight">\(398\)</span> measurements of <span class="math notranslate nohighlight">\(8\)</span> different properties of different cars:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>displacement</th>
      <th>cylinders</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>model_year</th>
      <th>origin</th>
      <th>mpg</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>307.0</td>
      <td>8</td>
      <td>130.0</td>
      <td>3504</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>350.0</td>
      <td>8</td>
      <td>165.0</td>
      <td>3693</td>
      <td>11.5</td>
      <td>70</td>
      <td>1</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>318.0</td>
      <td>8</td>
      <td>150.0</td>
      <td>3436</td>
      <td>11.0</td>
      <td>70</td>
      <td>1</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>304.0</td>
      <td>8</td>
      <td>150.0</td>
      <td>3433</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>16.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>302.0</td>
      <td>8</td>
      <td>140.0</td>
      <td>3449</td>
      <td>10.5</td>
      <td>70</td>
      <td>1</td>
      <td>17.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>393</th>
      <td>140.0</td>
      <td>4</td>
      <td>86.0</td>
      <td>2790</td>
      <td>15.6</td>
      <td>82</td>
      <td>1</td>
      <td>27.0</td>
    </tr>
    <tr>
      <th>394</th>
      <td>97.0</td>
      <td>4</td>
      <td>52.0</td>
      <td>2130</td>
      <td>24.6</td>
      <td>82</td>
      <td>2</td>
      <td>44.0</td>
    </tr>
    <tr>
      <th>395</th>
      <td>135.0</td>
      <td>4</td>
      <td>84.0</td>
      <td>2295</td>
      <td>11.6</td>
      <td>82</td>
      <td>1</td>
      <td>32.0</td>
    </tr>
    <tr>
      <th>396</th>
      <td>120.0</td>
      <td>4</td>
      <td>79.0</td>
      <td>2625</td>
      <td>18.6</td>
      <td>82</td>
      <td>1</td>
      <td>28.0</td>
    </tr>
    <tr>
      <th>397</th>
      <td>119.0</td>
      <td>4</td>
      <td>82.0</td>
      <td>2720</td>
      <td>19.4</td>
      <td>82</td>
      <td>1</td>
      <td>31.0</td>
    </tr>
  </tbody>
</table>
<p>398 rows × 8 columns</p>
</div></div></div>
</div>
<p>Here is a description of the different variables:</p>
<ul class="simple">
<li><p><strong>Displacement</strong>: The engine’s displacement (in cubic inches), which indicates the engine’s size and power.</p></li>
<li><p><strong>Cylinders</strong>: The number of cylinders in the engine of the car. This is a categorical variable.</p></li>
<li><p><strong>Horsepower</strong>: The engine’s horsepower, a measure of the engine’s performance.</p></li>
<li><p><strong>Weight</strong>: The weight of the car in pounds.</p></li>
<li><p><strong>Acceleration</strong>: The car’s acceleration (in seconds) from 0 to 60 miles per hour.</p></li>
<li><p><strong>Model Year</strong>: The year the car was manufactured. This is often converted into a categorical variable representing the car’s age.</p></li>
<li><p><strong>Origin</strong>: The car’s country of origin or manufacturing.
Car Name: The name or identifier of the car model.</p></li>
<li><p><strong>MPG (Miles per Gallon)</strong>: The fuel efficiency of the car in miles per gallon. It is the variable to be predicted in regression analysis.</p></li>
</ul>
<p>We will start by exploring the relationship between the variables <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">MPG</span></code>. Let’s visualize the related scatterplot:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c698fdd8eaead0645e86bb5763a6a2517f497b724b3c0467b19903be31ac8474.png" src="../_images/c698fdd8eaead0645e86bb5763a6a2517f497b724b3c0467b19903be31ac8474.png" />
</div>
</div>
</section>
<section id="regression-models">
<h2>Regression Models<a class="headerlink" href="#regression-models" title="Permalink to this heading">#</a></h2>
<p>Regression models, in general, aim to <strong>study the relationship between two variables</strong>, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, by defining a mathematical model <span class="math notranslate nohighlight">\(f\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[Y=f(X) + \epsilon\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f\)</span> is a deterministic function which can be used to <strong>predict the values of <span class="math notranslate nohighlight">\(Y\)</span> from the values of <span class="math notranslate nohighlight">\(X\)</span></strong>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is an <strong>error term</strong>, i.e., a variable capturing everything that is not captured by the deterministic function <span class="math notranslate nohighlight">\(f\)</span>. It can be due to different reasons, the main of which are:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(f\)</span> is not an accurate deterministic function of the process. Since we don’t know the “true” function <span class="math notranslate nohighlight">\(f\)</span> and we are only estimating it, we may obtain a suboptimal <span class="math notranslate nohighlight">\(f\)</span> for which <span class="math notranslate nohighlight">\(Y \neq f(X)\)</span>. The error term captures the differences between our predictions and the true values.</p></li>
<li><p><span class="math notranslate nohighlight">\(Y\)</span> cannot only be predicted from <span class="math notranslate nohighlight">\(X\)</span>, but some other variable is needed to correctly predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X\)</span>. For instance, <span class="math notranslate nohighlight">\(X\)</span> could be “years of education” and <span class="math notranslate nohighlight">\(Y\)</span> can be “income”. While may expect that “income” is not completely predicted from “years of education”. This can happen also because we don’t always have observations for all relevant variables.</p></li>
<li><p>the problem has inherent stochasticity which cannot be entirely modeled within the deterministic function <span class="math notranslate nohighlight">\(f\)</span>. For instance, consider the problem of predicting the rate of wins in poker based on the expertise of the player. The expertise surely allows to predict the rate of wins, but wins partially depend also on random factors, such as how the deck was shuffled.</p></li>
</ul>
</li>
</ul>
<p>Note that, often, we model <span class="math notranslate nohighlight">\(f\)</span> in a way that we have its <strong>analytical form</strong>. This is very powerful. If we have the analytical form of the function <span class="math notranslate nohighlight">\(f\)</span> which <strong>explains</strong> how <span class="math notranslate nohighlight">\(Y\)</span> is influenced from <span class="math notranslate nohighlight">\(X\)</span> (<strong>can be predicted from <span class="math notranslate nohighlight">\(X\)</span></strong>), then we can really understand deeply the connection between the two variables!</p>
<p>The function <span class="math notranslate nohighlight">\(f\)</span> can take different forms. The most common one is the <strong>linear form</strong> that we will see in the next section. While the linear form is very simple (and hence we can anticipate it will be a limited model in many cases), it has the great advantage to be <strong>easy to interpret</strong>.</p>
</section>
<section id="simple-linear-regression">
<h2>Simple Linear Regression<a class="headerlink" href="#simple-linear-regression" title="Permalink to this heading">#</a></h2>
<p>Simple linear regression aims to model the <strong>linear relationship</strong> between two variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. In our example dataset, we will consider <span class="math notranslate nohighlight">\(X=\text{horsepower}\)</span> and <span class="math notranslate nohighlight">\(Y=\text{mpg}\)</span>.</p>
<p>Since we are trying to model a linear relationship, we can imagine <strong>a line passing through the data</strong>. The simple linear regression model is defined as:</p>
<div class="math notranslate nohighlight">
\[Y \approx \beta_0 + \beta_1X\]</div>
<p>In our example:</p>
<div class="math notranslate nohighlight">
\[\text{mpg} \approx \beta_0 + \beta_1 \text{horsepower}\]</div>
<p>It is often common to introduce a <strong>“noise” variable</strong> which captures the randomness due to which the expression above is approximated and write:</p>
<div class="math notranslate nohighlight">
\[Y = \beta_0 + \beta_1X + \epsilon\]</div>
<p>As we will see later, we expect <span class="math notranslate nohighlight">\(\epsilon\)</span> to be <strong>small and randomly distributed</strong>.</p>
<p>Given the model above, we will call:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span>, the <strong>independent variable</strong> or <strong>regressor</strong>;</p></li>
<li><p><span class="math notranslate nohighlight">\(Y\)</span>, the <strong>dependent variable</strong> or <strong>regressed variable</strong>.</p></li>
</ul>
<p>The values <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are called <strong>coefficients</strong> or <strong>parameters</strong> of the model.</p>
<p>The mathematical model above has a geometrical interpretation. Indeed, specific values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> identify a given line in the 2D plane, as shown in the plot below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/48cceafcc4a71519e9307e5ad806a28b2d5667d1bef1614cc857b2ce152e672f.png" src="../_images/48cceafcc4a71519e9307e5ad806a28b2d5667d1bef1614cc857b2ce152e672f.png" />
</div>
</div>
<p>We hence aim to estimate two appropriate values <span class="math notranslate nohighlight">\(\hat \beta_0\)</span> and <span class="math notranslate nohighlight">\(\hat \beta_1\)</span> from data in a way that they provide a model which represents well our data. In the case of our example, we expect the geometrical model to have this aspect:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1902656f8953fe0f54ef9145573487d4bcab7bfa2551eb40bde7a0894b954a6c.png" src="../_images/1902656f8953fe0f54ef9145573487d4bcab7bfa2551eb40bde7a0894b954a6c.png" />
</div>
</div>
<p>This line will also be called the <strong>regression line</strong>.</p>
<section id="estimating-the-coefficients-ordinary-least-squares-ols">
<h3>Estimating the Coefficients - Ordinary Least Squares (OLS)<a class="headerlink" href="#estimating-the-coefficients-ordinary-least-squares-ols" title="Permalink to this heading">#</a></h3>
<p>To estimate the coefficients of our optimal model, we should first define what is a good model. We will say that <strong>a good model is one that predicts well the <span class="math notranslate nohighlight">\(Y\)</span> variable from the <span class="math notranslate nohighlight">\(X\)</span> one</strong>. We already know from the example above that, since the relationship is not perfectly linear, the model will make some mistakes.</p>
<p>Let <span class="math notranslate nohighlight">\(\{(x_i,y_i)\}\)</span> be our set of observations. Let</p>
<div class="math notranslate nohighlight">
\[\hat y_i = \hat \beta_0 + \hat \beta_1 x_i\]</div>
<p>be the prediction of the model for the observation <span class="math notranslate nohighlight">\(x_i\)</span>. For each data point <span class="math notranslate nohighlight">\((x_i,y_i)\)</span>, we will define the deviation of the prediction from the <span class="math notranslate nohighlight">\(y_i\)</span> value as follows:</p>
<div class="math notranslate nohighlight">
\[e_i = y_i - \hat y_i\]</div>
<p>We use the symbol <span class="math notranslate nohighlight">\(e\)</span> for error and also call these numbers <strong>the residuals</strong>, as they are the <strong>differences (or residuals) between our prediction and the ground truth value</strong>.</p>
<p>These numbers will be positive or negative based on whether we underestimate or overestimate the <span class="math notranslate nohighlight">\(y_i\)</span> values, while <strong>they will be zero when the prediction is perfect</strong>.</p>
<p>As a global error indicator for the model, given the data, we will define the <strong>residual sum of squares (RSS)</strong> as:</p>
<div class="math notranslate nohighlight">
\[RSS = e_1^2 + e_2^2 + \ldots + e_n^2 \]</div>
<p>or equivalently:</p>
<div class="math notranslate nohighlight">
\[RSS = (y_1 - \hat \beta_0 - \hat \beta_1 x_1)^2 + (y_2 - \hat \beta_0 - \hat \beta_1 x_2)^2 + \ldots + (y_n - \hat \beta_0 - \hat \beta_1 x_n)^2\]</div>
<p>This number will be the sum of the square values of the dashed segments in the plot below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/0ce8c3d3eaa7202d76b066d9f2abe220b19ca48b44df50d3aebe8683c1821d47.png" src="../_images/0ce8c3d3eaa7202d76b066d9f2abe220b19ca48b44df50d3aebe8683c1821d47.png" />
</div>
</div>
<p>Intuitively, if we minimize these numbers, we will find the line which <strong>best fits the data</strong>.</p>
<p>We can obtain estimates for <span class="math notranslate nohighlight">\(\hat \beta_0\)</span> and <span class="math notranslate nohighlight">\(\hat \beta_1\)</span> by minimizing the RSS using an approach called <strong>ordinary least squares</strong>.</p>
<p>We can write the RSS as a function of the parameters to estimate:</p>
<div class="math notranslate nohighlight">
\[RSS(\beta_0, \beta_1) = \sum_{i=1}^n(y_i - \beta_0 -  \beta_1 x_i)^2 \]</div>
<p>This is also called a <strong>cost function</strong> or <strong>loss function</strong>.</p>
<p>We aim to find:</p>
<div class="math notranslate nohighlight">
\[(\hat \beta_0, \hat \beta_1) = \arg \min_{\beta_0, \beta_1} RSS(\beta_0, \beta_1)\]</div>
<p>The minimum can be found setting:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial RSS(\beta_0, \beta_1)}{\partial \beta_0} = 0\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial RSS(\beta_0, \beta_1)}{\partial \beta_1} = 0\]</div>
<p>Doing the math, it can be shown that:</p>
<div class="math notranslate nohighlight">
\[\hat \beta_1 = \frac{\sum_{i=1}^n(x_i - \overline x)(y_i - \overline y)}{\sum_{i=1}^n(x_i - \overline x)}\]</div>
<div class="math notranslate nohighlight">
\[\hat \beta_0 = \overline y - \hat \beta_1 \overline x\]</div>
<section id="empirical-risk-minimization">
<h4>Empirical Risk Minimization<a class="headerlink" href="#empirical-risk-minimization" title="Permalink to this heading">#</a></h4>
<p>It is worth noting that we can obtain the same result by following an Empirical Risk Minimization scheme.</p>
<p>In particular, we can choose to use their squared values as our <strong>cost function</strong>:</p>
<div class="math notranslate nohighlight">
\[L(h(x_i),y_i) = (h(x_i)-y_i)^2 = e_i^2\]</div>
<p>and define the Empirical Risk as follows:</p>
<div class="math notranslate nohighlight">
\[R_{emp} = \frac{1}{N} \sum_{i=1}^N (h(x_i)-y_i)^2 = \frac{1}{N} RSS\]</div>
<p>where <span class="math notranslate nohighlight">\(h=f\)</span>.</p>
<p>Note that, since:</p>
<div class="math notranslate nohighlight">
\[\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h) = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ \frac{1}{N} \sum_{i=1}^N (h(x)-y)^2 = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ \frac{1}{N} RSS =  \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ RSS\]</div>
<p>we can solve the optimization problem with <strong>Ordinary Least Squares (OLS)</strong> (the same method we used previously to minimize RSS).</p>
<p>From a <strong>learning perspective</strong>, solving the optimization problem <span class="math notranslate nohighlight">\(\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h)\)</span> corresponds to <strong>finding the optimal set of parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> minimizing the empirical risk</strong>, which corresponds to minimizing the Residual Sum of Squares, as previously defined.</p>
</section>
</section>
<section id="interpretation-of-the-coefficients-of-linear-regression">
<h3>Interpretation of the Coefficients of Linear Regression<a class="headerlink" href="#interpretation-of-the-coefficients-of-linear-regression" title="Permalink to this heading">#</a></h3>
<p>Using the formulas above, we find the following values for the example above:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>beta_0: 39.94
beta_1: -0.16
</pre></div>
</div>
</div>
</div>
<p>These parameters identify the following line:</p>
<div class="math notranslate nohighlight">
\[y = 39.94 - 0.15 x\]</div>
<p>The plot below shows the line on the data:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/a6a5ac76f3f5efcc040ce9a461880afe348e913ccad284ecb65686e5050989ea.png" src="../_images/a6a5ac76f3f5efcc040ce9a461880afe348e913ccad284ecb65686e5050989ea.png" />
</div>
</div>
<p>Apart from the geometric interpretation, the coefficients of a linear regressor have an important <strong>statistical interpretation</strong>. In particular:</p>
<p><strong>The intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> is the value of <span class="math notranslate nohighlight">\(y\)</span> that we get when the input value <span class="math notranslate nohighlight">\(x\)</span> is equal to zero <span class="math notranslate nohighlight">\(x=0\)</span> (i.e., <span class="math notranslate nohighlight">\(f(0)\)</span>)</strong>. This value <strong>may not always make sense</strong>. For instance, in the example above, we have: <span class="math notranslate nohighlight">\(\beta_0 = 39.94\)</span>, which means that, <strong>when the horsepower is <span class="math notranslate nohighlight">\(0\)</span>, then the consumption in mpg is equal to <span class="math notranslate nohighlight">\(39.94\)</span></strong>.</p>
<p><strong>The coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> indicates the steepness of the curve</strong>. If <span class="math notranslate nohighlight">\(\beta_1\)</span> is large, then the curve is steep. This indicates that a small change in <span class="math notranslate nohighlight">\(x\)</span> is associated to a large change in <span class="math notranslate nohighlight">\(y\)</span>. In general, we can see that:</p>
<div class="math notranslate nohighlight">
\[f(x+1)-f(x)=\beta_0+\beta_1 (x+1)-\beta_0-\beta_1 x=\beta_1 (x+1-x)=\beta_1\]</div>
<p>which reveals that <strong>when we observe an increment of one unit of x, we observe an increment of <span class="math notranslate nohighlight">\(\beta_1\)</span> units in y</strong>. In our example, <span class="math notranslate nohighlight">\(\beta_1=-0.15\)</span>, hence we can say that, for cars with one additional unit of horsepower, we observe an drop in mpg <span class="math notranslate nohighlight">\(-0.15\)</span> units.</p>
</section>
<section id="accuracy-of-the-coefficient-estimates">
<h3>Accuracy of the Coefficient Estimates<a class="headerlink" href="#accuracy-of-the-coefficient-estimates" title="Permalink to this heading">#</a></h3>
<p>Recall that we are trying to model the relationship between two random variables <span class="math notranslate nohighlight">\(X\)</span> as <span class="math notranslate nohighlight">\(Y\)</span> with a simple linear model:</p>
<div class="math notranslate nohighlight">
\[Y = \beta_0 + \beta_1X+\epsilon\]</div>
<p>This means that, once we find appropriate values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>, we expect these to summarize the <strong>linear relationship in the population</strong> or the <strong>population regression line</strong>. Also, recall that these values are obtained using two formulas which are based on realizations of <span class="math notranslate nohighlight">\(X\)</span> of <span class="math notranslate nohighlight">\(Y\)</span> and can be hence seen as <strong>estimators</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat \beta_1 = \frac{\sum_{i=1}^n(x_i - \overline x)(y_i - \overline y)}{\sum_{i=1}^n(x_i - \overline x)^2}\]</div>
<div class="math notranslate nohighlight">
\[\hat \beta_0 = \overline y - \hat \beta_1 \overline x\]</div>
<p>We now recall that, being estimates, they provide values related to a given <strong>realization of the random variables</strong>.</p>
<p>Let us consider an ideal population for which:</p>
<div class="math notranslate nohighlight">
\[Y=2x+1\]</div>
<p>Ideally, given a sample from the population, we expect to obtain <span class="math notranslate nohighlight">\(\hat \beta_0 \approx 1\)</span> and <span class="math notranslate nohighlight">\(\hat \beta_1 \approx 2\)</span>. In practice, different samples may lead to different estimates and hence different regression lines, as shown in the plot below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/3a7327559c0897cf55c851565c7d2b1441b4e834a4d189cf361a475310aa758c.png" src="../_images/3a7327559c0897cf55c851565c7d2b1441b4e834a4d189cf361a475310aa758c.png" />
</div>
</div>
<p>Each of the first three subplots shows a different sample drawn from the population, with its corresponding estimated regression line, along with the true population regression line. The last subplot compares the different estimated lines with the population regression line and the average regression line (in red). Coefficient estimates are shown in the subplot titles.</p>
<p>As can be noted, each estimate can be inaccurate, while the average regression line is very close to the population regression line. This is due to the fact that our estimators for the parameters of the regression coefficients <strong>have non-zero variance</strong>. In practice, it can be shown that these estimators are <strong>unbiased</strong> (hence the average regression line is close to the population line).</p>
<section id="confidence-intervals-of-the-regression-coefficients">
<h4>Confidence Intervals of the Regression Coefficients<a class="headerlink" href="#confidence-intervals-of-the-regression-coefficients" title="Permalink to this heading">#</a></h4>
<p>Since the formulas to compute regression parameters can be seen as estimators, we can compute <strong>standard errors</strong> and <strong>confidence intervals</strong>.</p>
<p>For instance, for our model</p>
<div class="math notranslate nohighlight">
\[horsepower \approx \beta_0 + mpg \cdot \beta_1\]</div>
<p>We will have the following confidence interval at a <span class="math notranslate nohighlight">\(95\%\)</span> confidence level (which is a common CL):</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>COEFFICIENT</p></th>
<th class="head"><p>STD ERROR</p></th>
<th class="head"><p>CONFIDENCE INTERVAL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_0\)</span></p></td>
<td><p>39.94</p></td>
<td><p>0.717</p></td>
<td><p><span class="math notranslate nohighlight">\([38.53, 41.35]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta_1\)</span></p></td>
<td><p>-0.1578</p></td>
<td><p>0.006</p></td>
<td><p><span class="math notranslate nohighlight">\([-0.17, -0.15]\)</span></p></td>
</tr>
</tbody>
</table>
<p>From the table above, we can say that:</p>
<ul class="simple">
<li><p>The value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> for <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code> lies somewhere between <span class="math notranslate nohighlight">\(38.53\)</span> and <span class="math notranslate nohighlight">\(41.35\)</span>;</p></li>
<li><p>An increase of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> by one unit is associated to a decrease of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> between <span class="math notranslate nohighlight">\(-0.17\)</span> and <span class="math notranslate nohighlight">\(.015\)</span>.</p></li>
</ul>
<p>It is also common to see plots like the following one:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/a8a5008ee14a89e4c3b495d0a645ecd65b138b465db520207c558254410a714b.png" src="../_images/a8a5008ee14a89e4c3b495d0a645ecd65b138b465db520207c558254410a714b.png" />
</div>
</div>
<p>In the plot above, the shading around the line illustrates the variability induced by the confidence intervals estimated for the coefficients.</p>
</section>
<section id="statistical-tests-for-the-significance-of-coefficients">
<h4>Statistical Tests for the Significance of Coefficients<a class="headerlink" href="#statistical-tests-for-the-significance-of-coefficients" title="Permalink to this heading">#</a></h4>
<p>Seeing the aforementioned formulas as estimators also allows to perform <strong>hypothesis tests</strong> on the coefficients. In practice, it is common to perform a statistical test to assess whether the coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> is significantly different from zero. It is interesting to check this because, if <span class="math notranslate nohighlight">\(\beta_1\)</span> was equal to zero, then there would not be any correlation between the variables (and hence the linear regressor would not be useful). Indeed, if <span class="math notranslate nohighlight">\(\beta_1=0\)</span>:</p>
<div class="math notranslate nohighlight">
\[Y=\beta_0 + \epsilon\]</div>
<p>Hence <span class="math notranslate nohighlight">\(Y\)</span> cannot be predicted from <span class="math notranslate nohighlight">\(X\)</span> and the two variables are not associated.</p>
<p>The <strong>null hypothesis</strong> of the test is as follows:</p>
<div class="math notranslate nohighlight">
\[H_0: \text{There is no association between } X \text{ and } Y \Leftrightarrow \beta_1=0\]</div>
<p>While the <strong>alternative hypothesis</strong> is formulated as follows:</p>
<div class="math notranslate nohighlight">
\[H_1: \text{There is some association between } X \text{ and } Y \Leftrightarrow \beta_1 \neq 0\]</div>
<p>We won’t see the mathematical details of this test, but it works as usual: we compute some statistic <span class="math notranslate nohighlight">\(t\)</span> following a known distribution (a t-Student distribution in this case), then compute a <strong>p-value</strong> and reject the null hypothesis if this is under the significance level <span class="math notranslate nohighlight">\(p &lt; \alpha\)</span> (often set to <span class="math notranslate nohighlight">\(\alpha=0.05\)</span>).</p>
<p>A similar test is conducted to check whether <span class="math notranslate nohighlight">\(\beta_0\)</span> is significantly different from zero. But in this case it is not a big deal if the test fails as <span class="math notranslate nohighlight">\(\beta_0=0\)</span> is a perfectly reasonable result (the regression line passes through the origin).</p>
<p>Let’s see the updated table from the same example:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>COEFFICIENT</p></th>
<th class="head"><p>STD ERROR</p></th>
<th class="head"><p>t</p></th>
<th class="head"><p>P&gt;|t|</p></th>
<th class="head"><p>CONFIDENCE INTERVAL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(39.94\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.717\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(55.66\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([38.53, 41.35]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.1578\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.006\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-24.49\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([-0.17, -0.15]\)</span></p></td>
</tr>
</tbody>
</table>
<p>From the table above, we can conclude that both <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are significantly different than <span class="math notranslate nohighlight">\(0\)</span> (the p-value is equal to zero). This can also be noted by the fact that the confidence intervals do not contain the zero number.</p>
</section>
</section>
</section>
<section id="assessing-model-accuracy">
<h2>Assessing Model Accuracy<a class="headerlink" href="#assessing-model-accuracy" title="Permalink to this heading">#</a></h2>
<p>The hypothesis tests (like the t-test for <span class="math notranslate nohighlight">\(\beta_1\)</span>) tell us <em>if</em> a statistically significant relationship exists. They do not, however, tell us <strong>how well the model fits the data</strong> or <strong>how accurate its predictions are</strong>.</p>
<p>To measure performance, we use different metrics that generally fall into two categories, aligning with our “two goals”:</p>
<ol class="arabic simple">
<li><p><strong>Metrics for Understanding (Goodness-of-Fit):</strong> These are used in the “Statistical” approach to measure how well the model explains the data it was trained on. (e.g., RSE, R²)</p></li>
<li><p><strong>Metrics for Prediction (Predictive Accuracy):</strong> These are used in the “Machine Learning” approach to measure how well the model performs on new, unseen data. (e.g., MSE, RMSE, MAE)</p></li>
</ol>
<section id="the-building-block-residuals-and-rss">
<h3>The Building Block: Residuals and RSS<a class="headerlink" href="#the-building-block-residuals-and-rss" title="Permalink to this heading">#</a></h3>
<p>All regression metrics are built upon the <strong>residuals</strong>—the errors from the model. A residual (<span class="math notranslate nohighlight">\(e_i\)</span>) is the difference between the observed value (<span class="math notranslate nohighlight">\(y_i\)</span>) and the predicted value (<span class="math notranslate nohighlight">\(\hat{y}_i\)</span>):</p>
<div class="math notranslate nohighlight">
\[e_i = y_i - \hat{y}_i\]</div>
<p>The <strong>Residual Sum of Squares (RSS)</strong> which we already saw is the foundational quantity that OLS (Ordinary Least Squares) minimizes. It is the sum of all squared residuals:</p>
<div class="math notranslate nohighlight">
\[RSS = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2\]</div>
</section>
<section id="metrics-for-understanding-goodness-of-fit">
<h3>Metrics for Understanding (Goodness-of-Fit)<a class="headerlink" href="#metrics-for-understanding-goodness-of-fit" title="Permalink to this heading">#</a></h3>
<p>When our goal is <strong>understanding</strong> (inference), we typically evaluate the model on the entire dataset it was trained on.</p>
<section id="residual-standard-error-rse">
<h4>1. Residual Standard Error (RSE)<a class="headerlink" href="#residual-standard-error-rse" title="Permalink to this heading">#</a></h4>
<p>The RSE is the <em>statistical</em> estimate of the standard deviation of the irreducible error, <span class="math notranslate nohighlight">\(\epsilon\)</span>. It measures the “typical” error of the model.</p>
<div class="math notranslate nohighlight">
\[RSE = \sqrt{\frac{RSS}{n-2}} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n (y_i-\hat y_i)^2}\]</div>
<ul class="simple">
<li><p><strong>Why <span class="math notranslate nohighlight">\(n-2\)</span>?</strong> We divide by the “degrees of freedom” (<span class="math notranslate nohighlight">\(n-2\)</span>) because we estimated <em>two</em> parameters (<span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>). This makes RSE an unbiased estimate of <span class="math notranslate nohighlight">\(\sigma\)</span> (the standard deviation of <span class="math notranslate nohighlight">\(\epsilon\)</span>).</p></li>
<li><p><strong>Interpretation:</strong> RSE is an <strong>absolute measure</strong> of the model’s “lack of fit,” expressed in the <strong>same units as <span class="math notranslate nohighlight">\(Y\)</span></strong>. A smaller RSE means the model fits the data better.</p></li>
</ul>
<p>To know if an RSE value is “good,” you must compare it to the scale of <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<ul class="simple">
<li><p><strong>Example:</strong> For our <code class="docutils literal notranslate"><span class="pre">mpg</span></code> data, <span class="math notranslate nohighlight">\(RSE = 4.91\)</span>. The mean value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> is <span class="math notranslate nohighlight">\(\overline{y} = 23.52\)</span>. This means our model’s typical error (<span class="math notranslate nohighlight">\(4.91\)</span>) is about <strong>20%</strong> of the average value, which may be acceptable or not, depending on the context.</p></li>
</ul>
</section>
<section id="the-r-2-statistic">
<h4>2. The <span class="math notranslate nohighlight">\(R^2\)</span> Statistic<a class="headerlink" href="#the-r-2-statistic" title="Permalink to this heading">#</a></h4>
<p>The RSE is an absolute measure. It’s often more useful to have a <em>relative</em> measure that tells us what proportion of the data’s variance our model can explain. This is the <strong><span class="math notranslate nohighlight">\(R^2\)</span> (Coefficient of Determination)</strong>.</p>
<p>To calculate <span class="math notranslate nohighlight">\(R^2\)</span>, we first define a “baseline” model that just predicts the average <span class="math notranslate nohighlight">\(Y\)</span> for every <span class="math notranslate nohighlight">\(X\)</span>:
$<span class="math notranslate nohighlight">\(\hat{y}_{baseline} = \overline{y}\)</span>$</p>
<p>The error of this “dumb” model is the <strong>Total Sum of Squares (TSS)</strong>:
$<span class="math notranslate nohighlight">\(TSS = \sum_{i=1}^n(y_i-\overline y)^2\)</span>$</p>
<ul class="simple">
<li><p><strong>TSS</strong> measures the <strong>total variance</strong> in <span class="math notranslate nohighlight">\(Y\)</span> (before any regression).</p></li>
<li><p><strong>RSS</strong> measures the <strong>unexplained variance</strong> in <span class="math notranslate nohighlight">\(Y\)</span> (what’s left over <em>after</em> regression).</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(R^2\)</span> statistic is the proportion of total variance that is “explained” by the model:</p>
<div class="math notranslate nohighlight">
\[R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}\]</div>
<ul class="simple">
<li><p><strong>Interpretation:</strong> <span class="math notranslate nohighlight">\(R^2\)</span> is a value between 0 and 1 (or 0% to 100%).</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(R^2 = 0\)</span>: Our model is no better than just guessing the mean (<span class="math notranslate nohighlight">\(RSS = TSS\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(R^2 = 1\)</span>: Our model perfectly explains all the variance (<span class="math notranslate nohighlight">\(RSS = 0\)</span>).</p></li>
<li><p><strong>Example:</strong> For our <code class="docutils literal notranslate"><span class="pre">mpg</span></code> data, <span class="math notranslate nohighlight">\(R^2 = 0.61\)</span>. This means <strong>“61% of the variance in <code class="docutils literal notranslate"><span class="pre">mpg</span></code> can be explained by <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>.”</strong></p></li>
</ul>
</li>
</ul>
<p>For simple linear regression (one <span class="math notranslate nohighlight">\(X\)</span>), <span class="math notranslate nohighlight">\(R^2\)</span> is also equal to the square of the Pearson correlation coefficient: <span class="math notranslate nohighlight">\(R^2 = \rho^2\)</span>.</p>
</section>
</section>
<section id="metrics-for-prediction-test-set-error">
<h3>Metrics for Prediction (Test Set Error)<a class="headerlink" href="#metrics-for-prediction-test-set-error" title="Permalink to this heading">#</a></h3>
<p>When our goal is <strong>prediction</strong> (the ML approach), we do not care about <span class="math notranslate nohighlight">\(R^2\)</span> or RSE on the training data. We <em>only</em> care about the model’s performance on <strong>new, unseen data</strong> (the <strong>Test Set</strong>).</p>
<p>The following metrics are used to evaluate the model on the test set. (We will use <span class="math notranslate nohighlight">\(m\)</span> to denote the number of samples in the test set).</p>
<section id="mean-squared-error-mse">
<h4>1. Mean Squared Error (MSE)<a class="headerlink" href="#mean-squared-error-mse" title="Permalink to this heading">#</a></h4>
<p>The MSE is the average of the squared errors on the test set. It is the “predictive” version of the <span class="math notranslate nohighlight">\(R_{emp}\)</span> (Empirical Risk) we saw earlier.</p>
<div class="math notranslate nohighlight">
\[MSE_{test} = \frac{1}{m}\sum_{j=1}^{m} (y_j - \hat{y}_j)^2\]</div>
<ul class="simple">
<li><p><strong>Interpretation:</strong> This is an <strong>error measure</strong>, so a good model has a <strong>small MSE</strong>.</p></li>
<li><p><strong>Problem:</strong> The units are <strong>squared</strong> (e.g., if <span class="math notranslate nohighlight">\(y\)</span> is in meters, MSE is in square-meters). This is not intuitive.</p></li>
</ul>
</section>
<section id="root-mean-squared-error-rmse">
<h4>2. Root Mean Squared Error (RMSE)<a class="headerlink" href="#root-mean-squared-error-rmse" title="Permalink to this heading">#</a></h4>
<p>The RMSE is the fix for the MSE’s unit problem and is the most common metric for predictive regression. It is simply the square root of the MSE.</p>
<div class="math notranslate nohighlight">
\[RMSE_{test} = \sqrt{MSE_{test}} = \sqrt{\frac{1}{m}\sum_{j=1}^{m} (y_j - \hat{y}_j)^2}\]</div>
<ul class="simple">
<li><p><strong>Interpretation:</strong> RMSE is in the <strong>same units as <span class="math notranslate nohighlight">\(Y\)</span></strong> (e.g., meters). It can be read as “On average, our model’s predictions on the test set are wrong by about [RMSE value].”</p></li>
<li><p><strong>Key Property:</strong> Because it squares errors before averaging, the RMSE <strong>penalizes large errors</strong> (outliers) more heavily than small errors.</p></li>
</ul>
</section>
<section id="mean-absolute-error-mae">
<h4>3. Mean Absolute Error (MAE)<a class="headerlink" href="#mean-absolute-error-mae" title="Permalink to this heading">#</a></h4>
<p>The MAE is an alternative to RMSE that is also highly interpretable.</p>
<div class="math notranslate nohighlight">
\[MAE_{test} = \frac{1}{m}\sum_{j=1}^{m} |y_j - \hat{y}_j|\]</div>
<ul class="simple">
<li><p><strong>Interpretation:</strong> MAE is also in the <strong>same units as <span class="math notranslate nohighlight">\(Y\)</span></strong>. It measures the “average absolute error” of the predictions.</p></li>
<li><p><strong>Key Property:</strong> Unlike RMSE, MAE does <em>not</em> disproportionately penalize large errors. It is <strong>more robust to outliers</strong>.</p></li>
</ul>
</section>
</section>
<section id="visual-examples">
<h3>Visual Examples<a class="headerlink" href="#visual-examples" title="Permalink to this heading">#</a></h3>
<p>The plot below shows examples of linear regression fits with the different evaluation measures that we saw:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/6f6c3e5e0e732354712b7b5c96d64a51e934ce283389c5430447a75cb4737518.png" src="../_images/6f6c3e5e0e732354712b7b5c96d64a51e934ce283389c5430447a75cb4737518.png" />
</div>
</div>
</section>
<section id="diagnostic-plots-checking-model-validity">
<h3>Diagnostic Plots: Checking Model Validity<a class="headerlink" href="#diagnostic-plots-checking-model-validity" title="Permalink to this heading">#</a></h3>
<p>The metrics above (<span class="math notranslate nohighlight">\(R^2\)</span>, <span class="math notranslate nohighlight">\(RMSE\)</span>) are “scoring” metrics. They give you a single number to summarize <em>how much</em> error you have.</p>
<p>However, they don’t tell you <em>why</em> you have error, or if your model is fundamentally <strong>invalid</strong>. A model can have a “good” <span class="math notranslate nohighlight">\(R^2\)</span> but still be flawed. We use <strong>residual plots</strong> as <em>diagnostic</em> tools to check this.</p>
<section id="the-residuals-vs-fitted-plot">
<h4>1. The Residuals vs. Fitted Plot<a class="headerlink" href="#the-residuals-vs-fitted-plot" title="Permalink to this heading">#</a></h4>
<p>This is the most important diagnostic plot. We plot the <strong>Residuals (<span class="math notranslate nohighlight">\(e_i\)</span>)</strong> on the y-axis against the <strong>Fitted (Predicted) Values (<span class="math notranslate nohighlight">\(\hat{y}_i\)</span>)</strong> on the x-axis.</p>
<p><strong>What we <em>want</em> to see:</strong> A boring, random cloud of points with no pattern, centered around the <span class="math notranslate nohighlight">\(y=0\)</span> line. This indicates our model has captured the pattern and the remaining errors are just random noise.</p>
<p><strong>What we <em>don’t</em> want to see (Red Flags):</strong></p>
<ul class="simple">
<li><p><strong>A “U-Shape” or any Clear Pattern:</strong> This means our model is <strong>wrong</strong>. The relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> was <strong>not linear</strong>, and our simple linear model has failed to capture the true non-linear pattern.</p>
<ul>
<li><p><strong>Diagnosis:</strong> Violates the <strong>Linearity</strong> assumption.</p></li>
<li><p><strong>Fix:</strong> You need a more complex model (e.g., polynomial regression).</p></li>
</ul>
</li>
<li><p><strong>A “Fanning” or “Cone” Shape:</strong> The residuals get bigger (more spread out) as the predicted value <span class="math notranslate nohighlight">\(\hat{y}\)</span> gets bigger.</p>
<ul>
<li><p><strong>Diagnosis:</strong> This is called <strong>Heteroscedasticity</strong> (non-constant variance).</p></li>
<li><p><strong>Problem:</strong> This violates a core assumption of linear regression. It means our p-values and confidence intervals are unreliable because the model is less accurate for high-value predictions.</p></li>
</ul>
</li>
</ul>
<p>The figure below shows some examples of what to expect:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c01114a1ebdb6cb26d9218118a2f6476b02f4f3c416317f91e7645f24fcfc613.png" src="../_images/c01114a1ebdb6cb26d9218118a2f6476b02f4f3c416317f91e7645f24fcfc613.png" />
</div>
</div>
</section>
<section id="the-q-q-plot-quantile-quantile-plot">
<h4>2. The Q-Q Plot (Quantile-Quantile Plot)<a class="headerlink" href="#the-q-q-plot-quantile-quantile-plot" title="Permalink to this heading">#</a></h4>
<p>This is a plot used to check if the residuals are <strong>normally distributed</strong>. This is another key assumption for all our statistical tests (p-values, t-statistics).</p>
<ul class="simple">
<li><p><strong>Purpose:</strong> To check the <strong>Normality</strong> assumption.</p></li>
<li><p><strong>How to read it:</strong> If the residuals are truly normally distributed, the points will fall perfectly along the diagonal line.</p></li>
<li><p><strong>Problem:</strong> If the points “S-curve” or peel away from the line, it means the residuals are not normal, and our p-values and confidence intervals may be inaccurate.</p></li>
</ul>
<p>The figure below shows examples of “good” and “bad” residuals:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/ea42c5955922563e6884d47e96d2fcd4468ab729cfc0c768bfa8708458fff127.png" src="../_images/ea42c5955922563e6884d47e96d2fcd4468ab729cfc0c768bfa8708458fff127.png" />
</div>
</div>
</section>
</section>
</section>
<section id="multiple-linear-regression">
<h2>Multiple Linear Regression<a class="headerlink" href="#multiple-linear-regression" title="Permalink to this heading">#</a></h2>
<p>In the <strong>MPG example</strong>, we saw that about <span class="math notranslate nohighlight">\(61\%\)</span> of the variance in <span class="math notranslate nohighlight">\(Y\)</span> was explained by the model. One may wonder why about <span class="math notranslate nohighlight">\(39\%\)</span> of the variance could not be explained. Some common reasons may be:</p>
<ul class="simple">
<li><p>There is stochasticity in the data which prevents us to learn an accurate function to predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X\)</span>;</p></li>
<li><p>The relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is far from linear, so we cannot predict <span class="math notranslate nohighlight">\(Y\)</span> accurately;</p></li>
<li><p>The prediction of <span class="math notranslate nohighlight">\(Y\)</span> also depends on other variables.</p></li>
</ul>
<p>While in general the unexplained variance is due to a combination of the aforementioned factors, the third one is often very common and relatively easy to fix. In our case, we are trying to predict <code class="docutils literal notranslate"><span class="pre">mpg</span></code> from <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>. However, we can easily imagine how other variables may contribute to the estimation of <code class="docutils literal notranslate"><span class="pre">mpg</span></code>. For instance, two cars with the same <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> but different <code class="docutils literal notranslate"><span class="pre">weight</span></code> may have different values of <code class="docutils literal notranslate"><span class="pre">mpg</span></code>. We can hence try to find a model with <strong>also uses <code class="docutils literal notranslate"><span class="pre">weight</span></code> to predict <code class="docutils literal notranslate"><span class="pre">mpg</span></code></strong>. This is simply done by adding an additional coefficient for the new variable:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight\]</div>
<p>The obtained model is called <strong>multiple linear regression</strong>. If we fit this model (we will see how to estimate coefficients in this case), we obtain the following <span class="math notranslate nohighlight">\(R^2\)</span> value:</p>
<div class="math notranslate nohighlight">
\[R^2=0.71\]</div>
<p>An increment of <span class="math notranslate nohighlight">\(+0.1\)</span>!</p>
<p>In general, we can include as many variables as we think is relevant to add and define the following model:</p>
<div class="math notranslate nohighlight">
\[Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_n X_n\]</div>
<p>For instance, the following model:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight + \beta_3 model\_year\]</div>
<p>Has an <span class="math notranslate nohighlight">\(R^2\)</span> value of:</p>
<div class="math notranslate nohighlight">
\[R^2=0.808\]</div>
<section id="geometrical-interpretation">
<h3>Geometrical Interpretation<a class="headerlink" href="#geometrical-interpretation" title="Permalink to this heading">#</a></h3>
<p>The multiple regression model based on two variable has a geometrical interpretation. Indeed, the equation:</p>
<div class="math notranslate nohighlight">
\[Z = \beta_0 + \beta_1 X + \beta_2 Y\]</div>
<p>identifies a plane in the 3D space. We can visualize the plane identified by the <span class="math notranslate nohighlight">\(mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight\)</span> model as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/cs/p62_d78d49n3ddj0xlfh1h7r0000gn/T/ipykernel_70719/9283374.py:11: FutureWarning: The &#39;delim_whitespace&#39; keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep=&#39;\s+&#39;`` instead
  auto_df = pd.read_csv(url, delim_whitespace=True, names=column_names)
</pre></div>
</div>
<img alt="../_images/fd56b8b098e9c7b748cb00483420f8311585e02d428e903ed99f2401302d28e9.png" src="../_images/fd56b8b098e9c7b748cb00483420f8311585e02d428e903ed99f2401302d28e9.png" />
</div>
</div>
<p>The dashed lines indicate the residuals. The best fit of the model minimizes again the sum of squared residuals. This model makes predictions selecting the <span class="math notranslate nohighlight">\(Z\)</span> value which intersect the plane for given values of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>In general, when we consider <span class="math notranslate nohighlight">\(n\)</span> variables, the linear regressor will be <strong>a (n-1)-dimensional hyperplane in the n-dimensional space</strong>.</p>
</section>
<section id="statistical-interpretation">
<h3>Statistical Interpretation<a class="headerlink" href="#statistical-interpretation" title="Permalink to this heading">#</a></h3>
<p>The statistical interpretation of a multiple linear regression model is very similar to the interpretation of a simple linear regression model. Given the general model:</p>
<div class="math notranslate nohighlight">
\[y=\beta_0 + \beta_1 x_1 + \ldots + \beta_i x_i + \ldots + \beta_n x_n\]</div>
<p>we can interpret the coefficients as follows:</p>
<ul class="simple">
<li><p>The value of <span class="math notranslate nohighlight">\(\beta_0\)</span> indicates the value of <span class="math notranslate nohighlight">\(y\)</span> when all independent variables are set to zero;</p></li>
<li><p>The value of <span class="math notranslate nohighlight">\(\beta_i\)</span> indicates the increment of <span class="math notranslate nohighlight">\(y\)</span> that we expect to see when <span class="math notranslate nohighlight">\(x_i\)</span> increments by one unit, <strong>provided that all other values <span class="math notranslate nohighlight">\(x_j | j\neq i\)</span> are constant</strong>.</p></li>
</ul>
<p>In the considered example:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight\]</div>
<p>we obtain the following estimates for the coefficients:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_0\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_2\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(45.64\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.05\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.01\)</span></p></td>
</tr>
</tbody>
</table>
<p>We can interpret these estimates as follows:</p>
<ul class="simple">
<li><p>Cars with zero <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and zero <code class="docutils literal notranslate"><span class="pre">weight</span></code> will have an <code class="docutils literal notranslate"><span class="pre">mpg</span></code> of <span class="math notranslate nohighlight">\(45.64\)</span> (<span class="math notranslate nohighlight">\(\approx 19.4 Km/l\)</span>).</p></li>
<li><p>An increment of one unit of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is associated to a decrement of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> of <span class="math notranslate nohighlight">\(-0.05\)</span> units, provided that <code class="docutils literal notranslate"><span class="pre">weight</span></code> is constant. This makes sense: cars with more <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> will probably consume more fuel.</p></li>
<li><p>An increment of one unit of <code class="docutils literal notranslate"><span class="pre">weight</span></code> is associated to a decrement of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> of <code class="docutils literal notranslate"><span class="pre">-0.01</span></code> units, provided that <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is constant. This makes sense: heavier cars will consume more fuel.</p></li>
</ul>
<p>Let’s compare the estimates above with the estimates of our previous model:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower\]</div>
<p>In that case, we obtained:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_0\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(39.94\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.16\)</span></p></td>
</tr>
</tbody>
</table>
<p>We can note that the coefficients are different. This happens because, when we add more variables, <strong>the model explains variance in a different way</strong>. If we think more about it, this is coherent with the interpretation of the coefficients. Indeed:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(39.94\)</span> is the expected value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code>, but all other variables have unknown values. <span class="math notranslate nohighlight">\(45.64\)</span> is the expected value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code> and <code class="docutils literal notranslate"><span class="pre">weight=0</span></code>. This is different, as in the second case we are (virtually) looking at a subset of data for which both horsepower and weight are zero, while in the first case, we are only looking at data for which <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code>, but <code class="docutils literal notranslate"><span class="pre">weight</span></code> can be any value. In some sense, we can see <span class="math notranslate nohighlight">\(39.94\)</span> as an average value for different values of <code class="docutils literal notranslate"><span class="pre">weight</span></code> (and all other unobserved variables).</p></li>
<li><p><span class="math notranslate nohighlight">\(-0.16\)</span> is the expected increment of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when we observe an increment of one unit of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and we don’t know anything about the values of the other variables. <span class="math notranslate nohighlight">\(-0.05\)</span> is the expected increment of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code> are held constant, so, again, we are (virtually) looking at a different subset of the data in which the relationship between <code class="docutils literal notranslate"><span class="pre">mpg</span></code> and <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> may be a bit different.</p></li>
</ul>
<p>Note that, also in the case of multiple regression, we can estimate confidence intervals and perform statistical tests. In our example, we will get this table:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>COEFFICIENT</p></th>
<th class="head"><p>STD ERROR</p></th>
<th class="head"><p>t</p></th>
<th class="head"><p>P&gt;|t|</p></th>
<th class="head"><p>CONFIDENCE INTERVAL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(45.64\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.793\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(57.54\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([44.08, 47.20]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.05\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.011\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-4.26\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([-0.07, -0.03]\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.01\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.001\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-11.53\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([-0.007, -0.005]\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="estimating-the-regression-coefficients">
<h3>Estimating the Regression Coefficients<a class="headerlink" href="#estimating-the-regression-coefficients" title="Permalink to this heading">#</a></h3>
<p>Given the general model:</p>
<div class="math notranslate nohighlight">
\[y=\beta_0 + \beta_1 x_1 + \ldots + \beta_i x_i + \ldots + \beta_n x_n\]</div>
<p>We can define our <strong>cost function</strong> again as the residual sum of squares:</p>
<div class="math notranslate nohighlight">
\[RSS(\beta_0,\ldots,\beta_n) = \sum_{i=1}^m (y_i - \beta_0+\beta_1x_1^{(i)} + \ldots + \beta_n x_n^{(i)})^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(m\)</span> is the total number of observations and <span class="math notranslate nohighlight">\(x_j^{(i)}\)</span> is the <span class="math notranslate nohighlight">\(j^{th}\)</span> variable (<span class="math notranslate nohighlight">\(x_j\)</span>) of the <span class="math notranslate nohighlight">\(i^{th}\)</span> observations.</p>
<p>The <span class="math notranslate nohighlight">\(\hat \beta_0,\ldots,\hat \beta_n\)</span> values that minimize the loss function above are the <strong>multiple least square coefficient estimates</strong>.</p>
<p>To find these optimal values, it is convenient to use matrix notation. Given <span class="math notranslate nohighlight">\(m\)</span> observations, we will have <span class="math notranslate nohighlight">\(m\)</span> equations:</p>
<div class="math notranslate nohighlight">
\[y^{(1)} = \beta_0 + \beta_1 x_1^{(1)} + \ldots + \beta_n x_n^{(1)} + e^{(1)}\]</div>
<div class="math notranslate nohighlight">
\[y^{(2)} = \beta_0 + \beta_1 x_1^{(2)} + \ldots + \beta_n x_n^{(2)} + e^{(2)}\]</div>
<div class="math notranslate nohighlight">
\[\ldots\]</div>
<div class="math notranslate nohighlight">
\[y^{(m)} = \beta_0 + \beta_1 x_1^{(m)} + \ldots + \beta_n x_n^{(m)} + e^{(m)}\]</div>
<p>We can write the <span class="math notranslate nohighlight">\(m\)</span> equations in matrix form as follows:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{e}\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{y} = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix},
\mathbf{X}=
\begin{bmatrix}
1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \ldots &amp; x_n^{(1)} \\
1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \ldots &amp;  x_n^{(2)} \\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_1^{(m)} &amp; x_2^{(m)} &amp; \ldots &amp;  x_n^{(m)} \\
\end{bmatrix},
\mathbf{\beta} = \begin{bmatrix}
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{n}
\end{bmatrix},
\mathbf{e} = \begin{bmatrix}
e^{(1)} \\
e^{(2)} \\
\vdots \\
e^{(m)}
\end{bmatrix}
\end{split}\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is s <strong>design matrix</strong>.</p>
<p>In the notation above, we want to minimize:</p>
<div class="math notranslate nohighlight">
\[RSS(\mathbf{\beta}) = \sum_{i=1}^m (e^{(i)})^2 = \mathbf{e}^T \mathbf{e}\]</div>
<p>It can be shown that, by the <strong>least squares method</strong>, the RSS is minimized by the estimate:</p>
<div class="math notranslate nohighlight">
\[\mathbf{\hat \beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</div>
</section>
<section id="the-f-test">
<h3>The F-Test<a class="headerlink" href="#the-f-test" title="Permalink to this heading">#</a></h3>
<p>When fitting a multiple linear regressor, it is common to perform a statistical test to check whether at least one of the regression coefficients is significantly different from zero (in the population). This test is called an <span class="math notranslate nohighlight">\(F-test\)</span>. We define the <strong>null and alternative hypotheses</strong> as follows:</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_1=\beta_2=\ldots=\beta_n=0\]</div>
<div class="math notranslate nohighlight">
\[H_a: \exists j\ s.t.\ \beta_j \neq 0\]</div>
<p>The null hypothesis (the one we want to reject with this test) is that all coefficients are zero in the population. If this is true, than the multiple regressor is not reliable and we should discard it. Note that we exclude <span class="math notranslate nohighlight">\(\beta_0\)</span> from these coefficients because it represents the position of the regression line (intercept) and does not denote correlation. The alternative hypothesis is that at least one of the coefficients is different from zero.</p>
<p>We will not see the mathematical details, but in this case, we compute an <strong>F-statistic</strong> which follows a <strong>F-distribution</strong>.</p>
<p>The test is hence carried out as usual, finding a p-value which indicates the probability to observe a statistic larger than the observed one if all regression coefficients are zero in the population.</p>
<p>In our example of regressing <code class="docutils literal notranslate"><span class="pre">mpg</span></code> from <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code>, we will find:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(R^2\)</span></p></th>
<th class="head"><p>F-statistic</p></th>
<th class="head"><p>Prob(F-statistic)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.706</p></td>
<td><p>467.9</p></td>
<td><p>3.06e-104</p></td>
</tr>
</tbody>
</table>
<p>This indicates that the regressor is statistically relevant as the p-value (Prob(F-statistic)) is very small (under the significance level <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>).</p>
</section>
<section id="variable-selection-by-backward-elimination">
<h3>Variable Selection by Backward Elimination<a class="headerlink" href="#variable-selection-by-backward-elimination" title="Permalink to this heading">#</a></h3>
<p>Let’s now try to fit a multiple linear regressor on our dataset by including all variables. Our dependent variable will be <code class="docutils literal notranslate"><span class="pre">mpg</span></code>, while the set of dependent variables will be:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;displacement&#39; &#39;cylinders&#39; &#39;horsepower&#39; &#39;weight&#39; &#39;acceleration&#39;
 &#39;model_year&#39; &#39;origin&#39;]
</pre></div>
</div>
</div>
</div>
<p>We obtain the following measures of fit:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(R^2\)</span></p></th>
<th class="head"><p>F-statistic</p></th>
<th class="head"><p>Prob(F-statistic)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.821</p></td>
<td><p>252.4</p></td>
<td><p>2.04e-139</p></td>
</tr>
</tbody>
</table>
<p>The regressor has a good <span class="math notranslate nohighlight">\(R^2\)</span> and the p-value of the F-test is very small. We can conclude that there is some relationship between the independent variables and the dependent one.</p>
<p>The estimates of the regression coefficients will be:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>  -17.2184</td> <td>    4.644</td> <td>   -3.707</td> <td> 0.000</td> <td>  -26.350</td> <td>   -8.087</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.0170</td> <td>    0.014</td> <td>   -1.230</td> <td> 0.220</td> <td>   -0.044</td> <td>    0.010</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.0065</td> <td>    0.001</td> <td>   -9.929</td> <td> 0.000</td> <td>   -0.008</td> <td>   -0.005</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.0199</td> <td>    0.008</td> <td>    2.647</td> <td> 0.008</td> <td>    0.005</td> <td>    0.035</td>
</tr>
<tr>
  <th>cylinders</th>    <td>   -0.4934</td> <td>    0.323</td> <td>   -1.526</td> <td> 0.128</td> <td>   -1.129</td> <td>    0.142</td>
</tr>
<tr>
  <th>acceleration</th> <td>    0.0806</td> <td>    0.099</td> <td>    0.815</td> <td> 0.415</td> <td>   -0.114</td> <td>    0.275</td>
</tr>
<tr>
  <th>model_year</th>   <td>    0.7508</td> <td>    0.051</td> <td>   14.729</td> <td> 0.000</td> <td>    0.651</td> <td>    0.851</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.4261</td> <td>    0.278</td> <td>    5.127</td> <td> 0.000</td> <td>    0.879</td> <td>    1.973</td>
</tr>
</table></div></div>
</div>
<p>From the table, we can see that not all predictors have a p-value below the significance level <span class="math notranslate nohighlight">\(\alpha=0.05\)</span>. In particular:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">horsepower</span></code> has a large p-value of <span class="math notranslate nohighlight">\(0.22\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cylinders</span></code> has a large p-value of <span class="math notranslate nohighlight">\(0.128\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">acceleration</span></code> has a large p-value of <span class="math notranslate nohighlight">\(0.415\)</span>.</p></li>
</ul>
<p>This means that, within the current regressor, there is no meaningful relationship between these variables and <code class="docutils literal notranslate"><span class="pre">mpg</span></code>. A legitimate question is</p>
<blockquote>
<div><p>How is it possible that <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is not associated to <code class="docutils literal notranslate"><span class="pre">mpg</span></code> in this regressor if it was associated to it before?!</p>
</div></blockquote>
<p>However, we should recall that, when we consider a different set of variables, the interpretation of the coefficients changes. So, even if in the previous models, <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> was correlated to <code class="docutils literal notranslate"><span class="pre">mpg</span></code>, now it is not correlated anymore. We can imagine that the relationship between these variables is now explained by the other variables which we have introduced.</p>
<p>Even if the model is statistically significant, it does make sense to get rid of the variables with poor relationships with <code class="docutils literal notranslate"><span class="pre">mpg</span></code>. After all, if we remove a variable, the estimates of the other coefficients may change.</p>
<p>A common way to remove these variables is by <strong>backward selection</strong> or <strong>backward elimination</strong>. This consists in iteratively removing the variable with the largest p-value. We remove one variable at a time and re-compute the results, iterating until all variables have a small p-value.</p>
<p>let’s start by removing <code class="docutils literal notranslate"><span class="pre">acceleration</span></code>. This is the result:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>  -15.5635</td> <td>    4.175</td> <td>   -3.728</td> <td> 0.000</td> <td>  -23.773</td> <td>   -7.354</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.0239</td> <td>    0.011</td> <td>   -2.205</td> <td> 0.028</td> <td>   -0.045</td> <td>   -0.003</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.0062</td> <td>    0.001</td> <td>  -10.883</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.005</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.0193</td> <td>    0.007</td> <td>    2.579</td> <td> 0.010</td> <td>    0.005</td> <td>    0.034</td>
</tr>
<tr>
  <th>cylinders</th>    <td>   -0.5067</td> <td>    0.323</td> <td>   -1.570</td> <td> 0.117</td> <td>   -1.141</td> <td>    0.128</td>
</tr>
<tr>
  <th>model_year</th>   <td>    0.7475</td> <td>    0.051</td> <td>   14.717</td> <td> 0.000</td> <td>    0.648</td> <td>    0.847</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.4282</td> <td>    0.278</td> <td>    5.138</td> <td> 0.000</td> <td>    0.882</td> <td>    1.975</td>
</tr>
</table></div></div>
</div>
<p>Note that the coefficients have changed. We now remove <code class="docutils literal notranslate"><span class="pre">cylinders</span></code>, which has the largest p-value of <span class="math notranslate nohighlight">\(0.117\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>  -16.6939</td> <td>    4.120</td> <td>   -4.051</td> <td> 0.000</td> <td>  -24.795</td> <td>   -8.592</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.0219</td> <td>    0.011</td> <td>   -2.033</td> <td> 0.043</td> <td>   -0.043</td> <td>   -0.001</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.0063</td> <td>    0.001</td> <td>  -11.124</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.005</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.0114</td> <td>    0.006</td> <td>    2.054</td> <td> 0.041</td> <td>    0.000</td> <td>    0.022</td>
</tr>
<tr>
  <th>model_year</th>   <td>    0.7484</td> <td>    0.051</td> <td>   14.707</td> <td> 0.000</td> <td>    0.648</td> <td>    0.848</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.3853</td> <td>    0.277</td> <td>    4.998</td> <td> 0.000</td> <td>    0.840</td> <td>    1.930</td>
</tr>
</table></div></div>
</div>
<p>All variables now have an acceptable p-value (<span class="math notranslate nohighlight">\(\alpha=0.05\)</span>). We are done. Note that, by removing the two variables, <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> now has an acceptable p-value. This indicates that one of the removed variables was redundant with respect to <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>.</p>
</section>
<section id="collinearity-and-the-instability-of-least-squares">
<h3>Collinearity and the Instability of Least Squares<a class="headerlink" href="#collinearity-and-the-instability-of-least-squares" title="Permalink to this heading">#</a></h3>
<p>Let’s reflect again on the issue we saw in our model: the <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> p-value was significant in the simple model but became non-significant (<span class="math notranslate nohighlight">\(p &gt; 0.05\)</span>) in the multiple regression. This isn’t a contradiction; it’s a classic symptom of <strong>collinearity</strong>.</p>
<p><strong>Collinearity</strong> is a high linear correlation between two predictor variables. The more general (and common) problem is <strong>multicollinearity</strong>, which occurs when one predictor <span class="math notranslate nohighlight">\(X_j\)</span> can be accurately predicted by a linear combination of <em>other</em> predictors in the model (e.g., <span class="math notranslate nohighlight">\(X_j \approx \beta_0 + \beta_1 X_1 + \ldots\)</span>).</p>
<p>The primary problem with multicollinearity is that it makes our coefficient estimates <strong>unstable and unreliable</strong>. The standard errors of the coefficients for the correlated predictors become massively inflated. This is what we saw: <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> became non-significant because <code class="docutils literal notranslate"><span class="pre">weight</span></code> (which is highly correlated with it) “stole” all of its explanatory power.</p>
<p>This instability can be explained by <strong>looking directly at the mathematical solution for the OLS coefficients</strong> <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta} = (X^T X)^{-1} X^T Y\]</div>
<p>The key to this entire equation is the <strong><span class="math notranslate nohighlight">\((X^T X)^{-1}\)</span></strong> term, which is the inverse of the “design matrix.” The <span class="math notranslate nohighlight">\(X^T X\)</span> matrix is, in essence, a matrix of the covariances between all of our predictor variables.</p>
<ol class="arabic simple">
<li><p><strong>Perfect Collinearity (Singularity):</strong> If one variable is a perfect linear combination of another (e.g., <span class="math notranslate nohighlight">\(X_1 = 2 \cdot X_2\)</span>), the columns of <span class="math notranslate nohighlight">\(X\)</span> are not linearly independent. This causes the <span class="math notranslate nohighlight">\(X^T X\)</span> matrix to be <strong>singular</strong>, meaning its determinant is zero and its inverse <strong>does not exist</strong>. There is no unique solution for <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>.</p></li>
<li><p><strong>Multicollinearity (Ill-Conditioning):</strong> In our <code class="docutils literal notranslate"><span class="pre">mpg</span></code> case, the variables aren’t <em>perfectly</em> correlated, but <em>highly</em> correlated. This means <span class="math notranslate nohighlight">\(X^T X\)</span> is not perfectly singular, but it is <strong>“ill-conditioned”</strong> or <strong>“near-singular”</strong>. Its determinant is <em>almost</em> zero.</p></li>
</ol>
<p>When we compute the inverse, <span class="math notranslate nohighlight">\((X^T X)^{-1}\)</span>, we are “dividing by almost zero.” This causes the numbers <em>inside</em> the inverse matrix to explode, becoming massive.</p>
<p>The formula for the <strong>standard error</strong> of each coefficient <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> is directly proportional to the values in this inverse matrix. Therefore, if the values in <span class="math notranslate nohighlight">\((X^T X)^{-1}\)</span> are huge, the standard errors for our coefficients become <strong>inflated</strong>. This leads to a tiny t-statistic (<span class="math notranslate nohighlight">\(t = \hat{\beta}_j / SE(\hat{\beta}_j)\)</span>) and a massive, non-significant p-value. This is exactly what happened to <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> (or <code class="docutils literal notranslate"><span class="pre">numpy</span></code>) encounters a <em>perfectly</em> singular <span class="math notranslate nohighlight">\(X^T X\)</span> matrix (e.g., you forget to drop one dummy variable, creating perfect multicollinearity), it cannot compute the standard inverse.</p>
<p>Instead of crashing, it computes the <strong>Moore-Penrose Pseudoinverse</strong>, denoted as <span class="math notranslate nohighlight">\((X^T X)^+\)</span>. This is a generalization of the inverse that finds the <em>one</em> solution <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> that has the smallest <span class="math notranslate nohighlight">\(\ell_2\)</span>-norm. In practice, <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> will identify this redundancy, report that the matrix is singular, and often arbitrarily set one of the redundant variable’s coefficients to zero to provide a stable (but not unique) set of coefficients.</p>
</section>
<section id="adjusted-r-2">
<h3>Adjusted <span class="math notranslate nohighlight">\(R^2\)</span><a class="headerlink" href="#adjusted-r-2" title="Permalink to this heading">#</a></h3>
<p>While in the case of simple regression we saw that <span class="math notranslate nohighlight">\(R^2=\rho(x,y)^2\)</span> (where <span class="math notranslate nohighlight">\(\rho\)</span> is the correlation coefficient), in the case of multiple regression, it turns out that:</p>
<div class="math notranslate nohighlight">
\[R^2 = \rho(Y, \hat Y)^2\]</div>
<p>In general, having more variables in the linear regressor will reduce the error term and increase the covariance between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\hat Y\)</span>, hence increasing the <span class="math notranslate nohighlight">\(R^2\)</span>. However, in general having a small increase in <span class="math notranslate nohighlight">\(R^2\)</span> when we add a new variable may not be good. Indeed, we could prefer a simpler model with a slightly smaller <span class="math notranslate nohighlight">\(R^2\)</span> value.</p>
<p><strong>This is again a problem of bias-variance tradeoff.</strong></p>
<p>To express this, we can compute the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\overline R^2 = 1- \frac{m-1}{m-n-1} (1-R)^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(m\)</span> is the number of data points and <span class="math notranslate nohighlight">\(n\)</span> is the number of independent variables. The <span class="math notranslate nohighlight">\(\overline R^2\)</span> re-balances the <span class="math notranslate nohighlight">\(R^2\)</span> accounting for the introduction of additional variables.</p>
<p>For instance the last model we fit has an <span class="math notranslate nohighlight">\(R^2=0.820\)</span> and <span class="math notranslate nohighlight">\(\overline R^2=0.818\)</span>. In general, when using multiple regressor and especially for variable selection, it is good practice to use the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> rather than standard <span class="math notranslate nohighlight">\(R^2\)</span>.</p>
</section>
</section>
<section id="qualitative-predictors">
<h2>Qualitative Predictors<a class="headerlink" href="#qualitative-predictors" title="Permalink to this heading">#</a></h2>
<p>So far, we have studied relationships between continuous variables. In practice, linear regression allows to also study relationship between <strong>continuous dependent variables</strong> and <strong>qualitative independent variables</strong>. We will consider another dataset similar to the <strong>Auto MPG</strong> dataset:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mpg</th>
      <th>horsepower</th>
      <th>fuelsystem</th>
      <th>fueltype</th>
      <th>length</th>
      <th>cylinders</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>21</td>
      <td>111.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>168.8</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21</td>
      <td>111.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>168.8</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19</td>
      <td>154.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>171.2</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>24</td>
      <td>102.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>176.6</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>18</td>
      <td>115.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>176.6</td>
      <td>5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>200</th>
      <td>23</td>
      <td>114.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>188.8</td>
      <td>4</td>
    </tr>
    <tr>
      <th>201</th>
      <td>19</td>
      <td>160.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>188.8</td>
      <td>4</td>
    </tr>
    <tr>
      <th>202</th>
      <td>18</td>
      <td>134.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>188.8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>203</th>
      <td>26</td>
      <td>106.0</td>
      <td>idi</td>
      <td>diesel</td>
      <td>188.8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>204</th>
      <td>19</td>
      <td>114.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>188.8</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>205 rows × 6 columns</p>
</div></div></div>
</div>
<p>In this case, besides having numerical variables, we also have qualitative ones such as <code class="docutils literal notranslate"><span class="pre">fuelsystem</span></code> and <code class="docutils literal notranslate"><span class="pre">fueltype</span></code>. Let’s see what are their unique values:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fuelsystem: [&#39;mpfi&#39; &#39;2bbl&#39; &#39;mfi&#39; &#39;1bbl&#39; &#39;spfi&#39; &#39;4bbl&#39; &#39;idi&#39; &#39;spdi&#39;]
fueltype: [&#39;gas&#39; &#39;diesel&#39;]
</pre></div>
</div>
</div>
</div>
<p>We will not see the meaning of all the values of <code class="docutils literal notranslate"><span class="pre">fuelsystem</span></code>, while the values of <code class="docutils literal notranslate"><span class="pre">fueltype</span></code> are self-explanatory.</p>
<section id="predictors-with-only-two-levels">
<h3>Predictors with Only Two Levels<a class="headerlink" href="#predictors-with-only-two-levels" title="Permalink to this heading">#</a></h3>
<p>We will first see the case in which qualitative predictors only have two levels. To handle these as independent variables, we can define a new <strong>dummy variable</strong> which will encode <span class="math notranslate nohighlight">\(1\)</span> as one of the two levels and <span class="math notranslate nohighlight">\(0\)</span> as the other one. For instance, we can introduce a <code class="docutils literal notranslate"><span class="pre">fueltype[T.gas]</span></code> variable defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}fueltype[T.gas] = \begin{cases} 1 &amp; \text{if } fueltype=gas \\ 0 &amp; \text{otherwise}\end{cases}\end{split}\]</div>
<p>If we fit the model:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 fueltype[T.gas]\]</div>
<p>We obtain an <span class="math notranslate nohighlight">\(R^2=0.661\)</span> with <span class="math notranslate nohighlight">\(Prob(F-statistic) \approx 0\)</span> and the following estimates for the regression parameters:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>   41.2379</td> <td>    1.039</td> <td>   39.705</td> <td> 0.000</td> <td>   39.190</td> <td>   43.286</td>
</tr>
<tr>
  <th>fueltype[T.gas]</th> <td>   -2.7658</td> <td>    0.918</td> <td>   -3.013</td> <td> 0.003</td> <td>   -4.576</td> <td>   -0.956</td>
</tr>
<tr>
  <th>horsepower</th>      <td>   -0.1295</td> <td>    0.007</td> <td>  -18.758</td> <td> 0.000</td> <td>   -0.143</td> <td>   -0.116</td>
</tr>
</table></div></div>
</div>
<p>How do we interpret this result?</p>
<ul class="simple">
<li><p>The value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code> <code class="docutils literal notranslate"><span class="pre">fueltype=diesel</span></code> (i.e., <code class="docutils literal notranslate"><span class="pre">fueltype[T.gas]=0</span></code>) is <span class="math notranslate nohighlight">\(41.2379\)</span>;</p></li>
<li><p>An increase of one unit of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is associated to a decrease of <span class="math notranslate nohighlight">\(0.1295\)</span> units of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> provided that <code class="docutils literal notranslate"><span class="pre">fueltype=diesel</span></code>;</p></li>
<li><p>For gas vehicles we expect to see a decrease of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> equal to <span class="math notranslate nohighlight">\(2.7658\)</span> with respect to diesel vehicles.</p></li>
</ul>
</section>
<section id="predictors-with-more-than-two-levels">
<h3>Predictors with More than Two Levels<a class="headerlink" href="#predictors-with-more-than-two-levels" title="Permalink to this heading">#</a></h3>
<p>When predictors have <span class="math notranslate nohighlight">\(n\)</span> levels, we need to introduce <strong>multiple dummy variables</strong>. Specifically, we need to introduce <span class="math notranslate nohighlight">\(n-1\)</span> binary variables. For instance, if the levels of the variable <code class="docutils literal notranslate"><span class="pre">income</span></code> are <code class="docutils literal notranslate"><span class="pre">low</span></code>, <code class="docutils literal notranslate"><span class="pre">medium</span></code> and <code class="docutils literal notranslate"><span class="pre">high</span></code>, we could introduce two variables <code class="docutils literal notranslate"><span class="pre">income[T.low]</span></code> and <code class="docutils literal notranslate"><span class="pre">income[T.medium]</span></code>. These are sufficient to express all possible values of <code class="docutils literal notranslate"><span class="pre">income</span></code> as shown in the table below:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><code class="docutils literal notranslate"><span class="pre">income</span></code></p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">income[T.low]</span></code></p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">income[T.medium]</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">low</span></code></p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">medium</span></code></p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">high</span></code></p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>Note that we could have introduced a new variable <code class="docutils literal notranslate"><span class="pre">income[T.high]</span></code> but this would have been redundant and so <strong>correlated to the other two variables</strong>, which is something we know we have to avoid in linear regression.</p>
<p>If we fit the model which predicts <code class="docutils literal notranslate"><span class="pre">mpg</span></code> from <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">fuelsystem</span></code> we obtain <span class="math notranslate nohighlight">\(R^2=0.734\)</span>, <span class="math notranslate nohighlight">\(Prob(F-statistic) \approx 0\)</span> and the following estimates for the regression coefficients:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>          <td>   38.8638</td> <td>    1.234</td> <td>   31.504</td> <td> 0.000</td> <td>   36.431</td> <td>   41.297</td>
</tr>
<tr>
  <th>fuelsystem[T.2bbl]</th> <td>   -1.6374</td> <td>    1.127</td> <td>   -1.453</td> <td> 0.148</td> <td>   -3.860</td> <td>    0.585</td>
</tr>
<tr>
  <th>fuelsystem[T.4bbl]</th> <td>  -12.0875</td> <td>    2.263</td> <td>   -5.341</td> <td> 0.000</td> <td>  -16.551</td> <td>   -7.624</td>
</tr>
<tr>
  <th>fuelsystem[T.idi]</th>  <td>   -0.3894</td> <td>    1.300</td> <td>   -0.299</td> <td> 0.765</td> <td>   -2.954</td> <td>    2.175</td>
</tr>
<tr>
  <th>fuelsystem[T.mfi]</th>  <td>   -5.8285</td> <td>    3.661</td> <td>   -1.592</td> <td> 0.113</td> <td>  -13.049</td> <td>    1.392</td>
</tr>
<tr>
  <th>fuelsystem[T.mpfi]</th> <td>   -5.4942</td> <td>    1.202</td> <td>   -4.570</td> <td> 0.000</td> <td>   -7.865</td> <td>   -3.123</td>
</tr>
<tr>
  <th>fuelsystem[T.spdi]</th> <td>   -5.2446</td> <td>    1.612</td> <td>   -3.254</td> <td> 0.001</td> <td>   -8.423</td> <td>   -2.066</td>
</tr>
<tr>
  <th>fuelsystem[T.spfi]</th> <td>   -6.1522</td> <td>    3.615</td> <td>   -1.702</td> <td> 0.090</td> <td>  -13.282</td> <td>    0.978</td>
</tr>
<tr>
  <th>horsepower</th>         <td>   -0.0968</td> <td>    0.009</td> <td>  -11.248</td> <td> 0.000</td> <td>   -0.114</td> <td>   -0.080</td>
</tr>
</table></div></div>
</div>
<p>As we can see, we have added different correlation coefficients in order to deal with the different levels. Not all predictors have a low p-value, so we can remove those with backward elimination. We will see some more examples in the laboratory.</p>
</section>
</section>
<section id="linear-regression-in-practice">
<h2>Linear Regression in Practice<a class="headerlink" href="#linear-regression-in-practice" title="Permalink to this heading">#</a></h2>
<p>We will now see an example of how to compute linear regression in Python. We will again consider the MPG dataset.</p>
<p>We’ll load the “mpg” dataset from Seaborn. As in the previous example, the <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> column has missing values (marked as <code class="docutils literal notranslate"><span class="pre">?</span></code> in the original file, which <code class="docutils literal notranslate"><span class="pre">pandas</span></code> reads as <code class="docutils literal notranslate"><span class="pre">None</span></code> or <code class="docutils literal notranslate"><span class="pre">NaN</span></code> after coercion) and is read as an <code class="docutils literal notranslate"><span class="pre">object</span></code> type.</p>
<p>We will perform the same cleaning steps:</p>
<ol class="arabic simple">
<li><p>Drop the <code class="docutils literal notranslate"><span class="pre">name</span></code> column (a unique identifier).</p></li>
<li><p>Convert <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> to a numeric type (errors will become <code class="docutils literal notranslate"><span class="pre">NaN</span></code>).</p></li>
<li><p>Drop the rows with <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Import statsmodels.</span>
<span class="c1"># smf allows us to use the &quot;formula API&quot; (e.g., &#39;y ~ x&#39;)</span>
<span class="c1"># sm gives us access to more functions, like diagnostic plots</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="c1"># Load the dataset</span>
<span class="n">mpg</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;mpg&#39;</span><span class="p">)</span>

<span class="c1"># Clean the data</span>
<span class="n">mpg</span> <span class="o">=</span> <span class="n">mpg</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;horsepower&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;horsepower&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;coerce&#39;</span><span class="p">)</span>

<span class="c1"># Drop rows with missing values</span>
<span class="n">mpg</span> <span class="o">=</span> <span class="n">mpg</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of cleaned dataset: </span><span class="si">{</span><span class="n">mpg</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">mpg</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of cleaned dataset: (392, 8)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mpg</th>
      <th>cylinders</th>
      <th>displacement</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>model_year</th>
      <th>origin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18.0</td>
      <td>8</td>
      <td>307.0</td>
      <td>130.0</td>
      <td>3504</td>
      <td>12.0</td>
      <td>70</td>
      <td>usa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15.0</td>
      <td>8</td>
      <td>350.0</td>
      <td>165.0</td>
      <td>3693</td>
      <td>11.5</td>
      <td>70</td>
      <td>usa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18.0</td>
      <td>8</td>
      <td>318.0</td>
      <td>150.0</td>
      <td>3436</td>
      <td>11.0</td>
      <td>70</td>
      <td>usa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.0</td>
      <td>8</td>
      <td>304.0</td>
      <td>150.0</td>
      <td>3433</td>
      <td>12.0</td>
      <td>70</td>
      <td>usa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.0</td>
      <td>8</td>
      <td>302.0</td>
      <td>140.0</td>
      <td>3449</td>
      <td>10.5</td>
      <td>70</td>
      <td>usa</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<section id="id1">
<h3>Simple Linear Regression<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>Let’s start with our first question: <strong>“Is there a statistically significant relationship between <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">mpg</span></code>?”</strong></p>
<p>We use the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> formula API (<code class="docutils literal notranslate"><span class="pre">smf.ols</span></code>) to create a model. The formula <code class="docutils literal notranslate"><span class="pre">&quot;mpg</span> <span class="pre">~</span> <span class="pre">horsepower&quot;</span></code> is read as “<code class="docutils literal notranslate"><span class="pre">mpg</span></code> <em>as a function of</em> <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>.” <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> automatically adds an intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>) for us.</p>
<p>Unlike <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, we <strong>do not</strong> create a <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> here because our goal is not a test set score, but the interpretation of parameters calculated on the entire data sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define and fit the SLR model</span>
<span class="c1"># OLS stands for &quot;Ordinary Least Squares&quot;</span>
<span class="n">model_slr</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ horsepower&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the full summary table</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_slr</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.606
Model:                            OLS   Adj. R-squared:                  0.605
Method:                 Least Squares   F-statistic:                     599.7
Date:                Sat, 01 Nov 2025   Prob (F-statistic):           7.03e-81
Time:                        10:53:18   Log-Likelihood:                -1178.7
No. Observations:                 392   AIC:                             2361.
Df Residuals:                     390   BIC:                             2369.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     39.9359      0.717     55.660      0.000      38.525      41.347
horsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145
==============================================================================
Omnibus:                       16.432   Durbin-Watson:                   0.920
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.305
Skew:                           0.492   Prob(JB):                     0.000175
Kurtosis:                       3.299   Cond. No.                         322.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.summary()</span></code> output is dense, but it’s the heart of statistical analysis. Let’s break it into three parts:</p>
<section id="part-1-the-top-table-model-metadata">
<h4>Part 1: The Top Table (Model Metadata)<a class="headerlink" href="#part-1-the-top-table-model-metadata" title="Permalink to this heading">#</a></h4>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Field</p></th>
<th class="head text-left"><p>Meaning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">R-squared</span></code></strong></p></td>
<td class="text-left"><p><strong>0.606</strong> (or 60.6%) - As we saw, this is the “proportion of variance explained.” It tells us <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> alone explains 60.6% of the variability in <code class="docutils literal notranslate"><span class="pre">mpg</span></code>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">Adj.</span> <span class="pre">R-squared</span></code></strong></p></td>
<td class="text-left"><p><strong>0.605</strong> - A “corrected” version of <span class="math notranslate nohighlight">\(R^2\)</span> that penalizes for adding useless variables. In SLR, it’s almost identical to <span class="math notranslate nohighlight">\(R^2\)</span>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">Method:</span></code></strong></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">Least</span> <span class="pre">Squares</span></code> - The method we used (OLS).</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">No.</span> <span class="pre">Observations:</span></code></strong></p></td>
<td class="text-left"><p><strong>392</strong> - The number of rows (<span class="math notranslate nohighlight">\(n\)</span>) used to build the model.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">F-statistic:</span></code></strong></p></td>
<td class="text-left"><p><strong>599.7</strong> - A score that tests the <em>overall</em> significance of the model.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">Prob</span> <span class="pre">(F-statistic):</span></code></strong></p></td>
<td class="text-left"><p><strong>7.03e-81</strong> - A p-value associated with the F-statistic. This is a tiny number (<span class="math notranslate nohighlight">\(0.000...\)</span> with 80 zeros). Since it is <span class="math notranslate nohighlight">\(&lt; 0.05\)</span>, it tells us our model as a whole is statistically significant (i.e., it’s far better than a null model that just guesses the mean).</p></td>
</tr>
</tbody>
</table>
</section>
<section id="part-2-the-coefficients-table-the-core">
<h4>Part 2: The Coefficients Table (The Core)<a class="headerlink" href="#part-2-the-coefficients-table-the-core" title="Permalink to this heading">#</a></h4>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Field</p></th>
<th class="head text-left"><p><code class="docutils literal notranslate"><span class="pre">Intercept</span></code> (<span class="math notranslate nohighlight">\(\beta_0\)</span>)</p></th>
<th class="head text-left"><p><code class="docutils literal notranslate"><span class="pre">horsepower</span></code> (<span class="math notranslate nohighlight">\(\beta_1\)</span>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">coef</span></code></strong></p></td>
<td class="text-left"><p><strong>39.9359</strong></p></td>
<td class="text-left"><p><strong>-0.1578</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">err</span></code></strong></p></td>
<td class="text-left"><p>0.717</p></td>
<td class="text-left"><p>0.006</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">t</span></code></strong></p></td>
<td class="text-left"><p>55.660</p></td>
<td class="text-left"><p>-24.489</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>**`P&gt;</p></td>
<td class="text-left"><p>t</p></td>
<td class="text-left"><p>`**</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong><code class="docutils literal notranslate"><span class="pre">[0.025</span> <span class="pre">...</span> <span class="pre">0.975]</span></code></strong></p></td>
<td class="text-left"><p>[38.525, 41.347]</p></td>
<td class="text-left"><p>[-0.170, -0.146]</p></td>
</tr>
</tbody>
</table>
<p><strong>Interpreting the Coefficients:</strong></p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Intercept</span> <span class="pre">(coef)</span></code>:</strong> 39.93. This is our <span class="math notranslate nohighlight">\(\beta_0\)</span>. Theoretically, it’s the predicted value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is 0. (In this context, it has no real practical interpretation, but it’s mathematically necessary).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">horsepower</span> <span class="pre">(coef)</span></code>:</strong> -0.1578. This is our <span class="math notranslate nohighlight">\(\beta_1\)</span>, the slope. It’s the most important part.</p>
<ul>
<li><p><strong>Interpretation:</strong> “For every <strong>1-unit increase</strong> in <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>, we expect a <strong>0.1578 decrease</strong> in <code class="docutils literal notranslate"><span class="pre">mpg</span></code>, holding all else constant.”</p></li>
</ul>
</li>
</ul>
<p><strong>Interpreting the Significance (p-value):</strong></p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">horsepower</span> <span class="pre">(P&gt;|t|)</span></code>:</strong> This is <code class="docutils literal notranslate"><span class="pre">&lt;</span> <span class="pre">0.001</span></code>. This is the p-value for the null hypothesis <span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span>.</p>
<ul>
<li><p><strong>Conclusion:</strong> Because the p-value is much smaller than <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, we <strong>reject the null hypothesis</strong>. We can conclude that <strong>there is a statistically significant relationship</strong> between <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">mpg</span></code>.</p></li>
</ul>
</li>
</ul>
<p><strong>Interpreting the Confidence Interval (CI):</strong></p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">horsepower</span> <span class="pre">([0.025</span> <span class="pre">...</span> <span class="pre">0.975])</span></code>:</strong> [-0.170, -0.146]</p>
<ul>
<li><p><strong>Interpretation:</strong> “We are 95% confident that the <em>true</em> value of the slope (<span class="math notranslate nohighlight">\(\beta_1\)</span>) in the population lies between -0.170 and -0.146.”</p></li>
<li><p><strong>Note:</strong> The interval <strong>does not include 0</strong>, which confirms our conclusion from the p-value.</p></li>
</ul>
</li>
</ul>
</section>
<section id="part-3-the-bottom-table-residual-diagnostics">
<h4>Part 3: The Bottom Table (Residual Diagnostics)<a class="headerlink" href="#part-3-the-bottom-table-residual-diagnostics" title="Permalink to this heading">#</a></h4>
<p>This table provides tests to check if the residuals (<span class="math notranslate nohighlight">\(\epsilon\)</span>) meet the regression assumptions (like normality).</p>
</section>
</section>
<section id="residual-diagnostics">
<h3>Residual Diagnostics<a class="headerlink" href="#residual-diagnostics" title="Permalink to this heading">#</a></h3>
<p>The coefficients table (p-values, CIs, etc.) is <strong>only valid if</strong> our model meets the <strong>assumptions of linear regression</strong>. The two main checks we must perform are:</p>
<ol class="arabic simple">
<li><p><strong>Linearity and Homoscedasticity:</strong> Residuals should be randomly scattered around 0 with no clear pattern (e.g., no “U-shape” or “funnel”).</p></li>
<li><p><strong>Normality:</strong> The residuals should follow a normal distribution.</p></li>
</ol>
<p>We use diagnostic plots to check these.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the &quot;fitted&quot; (predicted) values and the residuals</span>
<span class="n">fitted_values</span> <span class="o">=</span> <span class="n">model_slr</span><span class="o">.</span><span class="n">fittedvalues</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">model_slr</span><span class="o">.</span><span class="n">resid</span>

<span class="c1"># 1. Residuals vs. Fitted Plot (for Linearity and Homoscedasticity)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">fitted_values</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">residuals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Residuals vs. Fitted Values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Fitted Values (Predicted MPG)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>

<span class="c1"># 2. Q-Q Plot (for Normality)</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span> <span class="c1"># &#39;s&#39; stands for &#39;standardized&#39; line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Q-Q Plot of Residuals&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/59081e09f114e326e73e03dd9511ca47b39386554ca7f2b28a246cb9a246f957.png" src="../_images/59081e09f114e326e73e03dd9511ca47b39386554ca7f2b28a246cb9a246f957.png" />
</div>
</div>
<section id="interpreting-the-diagnostic-plots">
<h4>Interpreting the Diagnostic Plots<a class="headerlink" href="#interpreting-the-diagnostic-plots" title="Permalink to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Residuals vs. Fitted (Left Plot):</strong></p>
<ul class="simple">
<li><p><strong>There is a problem!</strong> The points are not randomly scattered. They form a sort of <strong>“U-shape”</strong> (a parabola).</p></li>
<li><p><strong>Diagnosis:</strong> This indicates the <strong>Linearity</strong> assumption is violated. The true relationship between <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">mpg</span></code> is <em>not</em> a straight line (it’s curvilinear).</p></li>
<li><p><strong>Consequence:</strong> Our SLR model is too simple (it is <em>underfit</em>). Our coefficient estimates and p-values, while technically significant, are from a model we know is wrong.</p></li>
</ul>
</li>
<li><p><strong>Q-Q Plot (Right Plot):</strong></p>
<ul class="simple">
<li><p>The points peel away from the red line at the ends (the tails). This suggests our residuals have “heavy tails” (more outliers) than a perfect normal distribution.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="id2">
<h3>Multiple Linear Regression<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>Given our SLR model was flawed, let’s try to build a better, more complete model. We will use Multiple Linear Regression (MLR) to predict <code class="docutils literal notranslate"><span class="pre">mpg</span></code> using more variables.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">horsepower</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span></code> (which is highly correlated with <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_year</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">origin</span></code> (this is a categorical variable!)</p></li>
</ul>
<p>Using the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> formula API, we can simply add (<code class="docutils literal notranslate"><span class="pre">+</span></code>) variables. For categorical variables like <code class="docutils literal notranslate"><span class="pre">origin</span></code>, we wrap them in <code class="docutils literal notranslate"><span class="pre">C()</span></code> to tell <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> to treat them as such (it will automatically create the <em>dummy variables</em> / one-hot encoding).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define and &quot;fit&quot; (train) the MLR model</span>
<span class="n">formula_mlr</span> <span class="o">=</span> <span class="s1">&#39;mpg ~ horsepower + weight + model_year + C(origin)&#39;</span>
<span class="n">model_mlr</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula_mlr</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the full summary table</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_mlr</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.819
Model:                            OLS   Adj. R-squared:                  0.817
Method:                 Least Squares   F-statistic:                     350.3
Date:                Sat, 01 Nov 2025   Prob (F-statistic):          5.21e-141
Time:                        10:57:15   Log-Likelihood:                -1025.7
No. Observations:                 392   AIC:                             2063.
Df Residuals:                     386   BIC:                             2087.
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
======================================================================================
                         coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------------
Intercept            -15.2661      4.099     -3.724      0.000     -23.326      -7.206
C(origin)[T.japan]     0.3279      0.568      0.577      0.564      -0.789       1.444
C(origin)[T.usa]      -1.9546      0.519     -3.769      0.000      -2.974      -0.935
horsepower            -0.0085      0.009     -0.908      0.365      -0.027       0.010
weight                -0.0056      0.000    -12.622      0.000      -0.006      -0.005
model_year             0.7544      0.052     14.631      0.000       0.653       0.856
==============================================================================
Omnibus:                       32.098   Durbin-Watson:                   1.250
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               55.453
Skew:                           0.519   Prob(JB):                     9.09e-13
Kurtosis:                       4.523   Cond. No.                     7.54e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 7.54e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<section id="interpreting-the-summary-table-mlr">
<h4>Interpreting the Summary Table (MLR)<a class="headerlink" href="#interpreting-the-summary-table-mlr" title="Permalink to this heading">#</a></h4>
<p><strong>Overall Model Comparison (SLR vs MLR)</strong></p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">R-squared</span></code>:</strong> Jumped from 0.606 to <strong>0.821</strong>!</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Adj.</span> <span class="pre">R-squared</span></code>:</strong> Jumped from 0.605 to <strong>0.819</strong>.</p>
<ul>
<li><p><strong>Why look at Adj. R-squared?</strong> The regular <span class="math notranslate nohighlight">\(R^2\)</span> will <em>always</em> increase when you add a new variable, even if it’s useless. The <em>Adjusted</em> <span class="math notranslate nohighlight">\(R^2\)</span> corrects for this. It’s the correct metric to use when comparing models with different numbers of predictors.</p></li>
<li><p><strong>Conclusion:</strong> Our MLR model explains 81.9% of the variance in <code class="docutils literal notranslate"><span class="pre">mpg</span></code>, a massive improvement over the SLR model’s 60.5%.</p></li>
</ul>
</li>
</ul>
<p><strong>Coefficient Analysis (MLR)</strong></p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">coef</span></code></p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">P&gt;|t|</span></code></p></th>
<th class="head"><p>Interpretation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong><code class="docutils literal notranslate"><span class="pre">Intercept</span></code></strong></p></td>
<td><p>-17.2184</p></td>
<td><p>&lt; 0.001</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-odd"><td><p><strong><code class="docutils literal notranslate"><span class="pre">C(origin)[T.2]</span></code></strong></p></td>
<td><p>2.5670</p></td>
<td><p>&lt; 0.001</p></td>
<td><p>Being from origin “2” (European) vs “1” (USA) is associated with an <strong>increase of 2.57 mpg</strong>, holding <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>, <code class="docutils literal notranslate"><span class="pre">weight</span></code>, and <code class="docutils literal notranslate"><span class="pre">year</span></code> constant.</p></td>
</tr>
<tr class="row-even"><td><p><strong><code class="docutils literal notranslate"><span class="pre">C(origin)[T.3]</span></code></strong></p></td>
<td><p>2.5562</p></td>
<td><p>&lt; 0.001</p></td>
<td><p>Being from origin “3” (Japanese) vs “1” (USA) is associated with an <strong>increase of 2.56 mpg</strong>, holding all else constant.</p></td>
</tr>
<tr class="row-odd"><td><p><strong><code class="docutils literal notranslate"><span class="pre">horsepower</span></code></strong></p></td>
<td><p>-0.0044</p></td>
<td><p>0.601</p></td>
<td><p><strong>NOT SIGNIFICANT!</strong></p></td>
</tr>
<tr class="row-even"><td><p><strong><code class="docutils literal notranslate"><span class="pre">weight</span></code></strong></p></td>
<td><p>-0.0065</p></td>
<td><p>&lt; 0.001</p></td>
<td><p>For every 1-unit increase in <code class="docutils literal notranslate"><span class="pre">weight</span></code> (lb), we expect a <strong>decrease of 0.0065 mpg</strong>, holding all else constant.</p></td>
</tr>
<tr class="row-odd"><td><p><strong><code class="docutils literal notranslate"><span class="pre">model_year</span></code></strong></p></td>
<td><p>0.7508</p></td>
<td><p>&lt; 0.001</p></td>
<td><p>For every 1 <code class="docutils literal notranslate"><span class="pre">model_year</span></code> (e.g., from 76 to 77), we expect an <strong>increase of 0.75 mpg</strong>, holding all else constant. (Cars are getting more efficient).</p></td>
</tr>
</tbody>
</table>
<p><strong>Collinearity</strong></p>
<p>Why did <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> become not significant (<span class="math notranslate nohighlight">\(p = 0.601\)</span>)?</p>
<p>In the SLR model, <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> was a “proxy” for many things: engine size, car weight, etc. But now, <code class="docutils literal notranslate"><span class="pre">weight</span></code> is in the model! <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code> are highly correlated (a phenomenon called <strong>multicollinearity</strong>). If you look closely, we even received a warning about this.</p>
<p>The MLR model has discovered that <code class="docutils literal notranslate"><span class="pre">weight</span></code> is the <em>better</em> predictor, and once the influence of weight is accounted for, <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> does not provide (in this model) any significant <em>additional</em> information.</p>
<p><strong>This is the power of MLR:</strong> it allows us to isolate the unique impact of one variable <em>while controlling for</em> the impact of others.</p>
</section>
<section id="variable-selection">
<h4>Variable Selection<a class="headerlink" href="#variable-selection" title="Permalink to this heading">#</a></h4>
<p>Let’s now refine our regressor by removing variables which are not significant. We would remove <code class="docutils literal notranslate"><span class="pre">C(origin)[T.japan]</span></code> which has the largest p-value, but this is linked to <code class="docutils literal notranslate"><span class="pre">C(origin)[T.usa]</span></code>, so we will instead remove the <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>, which has the second-largest p-value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define and &quot;fit&quot; (train) the MLR model</span>
<span class="n">formula_mlr</span> <span class="o">=</span> <span class="s1">&#39;mpg ~  weight + model_year + C(origin)&#39;</span>
<span class="n">model_mlr</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula_mlr</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the full summary table</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_mlr</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                    mpg   R-squared:                       0.819
Model:                            OLS   Adj. R-squared:                  0.817
Method:                 Least Squares   F-statistic:                     437.9
Date:                Sat, 01 Nov 2025   Prob (F-statistic):          3.53e-142
Time:                        11:08:07   Log-Likelihood:                -1026.1
No. Observations:                 392   AIC:                             2062.
Df Residuals:                     387   BIC:                             2082.
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
======================================================================================
                         coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------------
Intercept            -16.3306      3.927     -4.158      0.000     -24.052      -8.610
C(origin)[T.japan]     0.2382      0.559      0.426      0.670      -0.861       1.337
C(origin)[T.usa]      -1.9763      0.518     -3.815      0.000      -2.995      -0.958
weight                -0.0059      0.000    -22.647      0.000      -0.006      -0.005
model_year             0.7698      0.049     15.818      0.000       0.674       0.866
==============================================================================
Omnibus:                       32.293   Durbin-Watson:                   1.251
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               58.234
Skew:                           0.507   Prob(JB):                     2.26e-13
Kurtosis:                       4.593   Cond. No.                     7.22e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 7.22e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>We still have a large p-value for one of the levels of origin, but we have to stop here since we cannot just remove one of the levels, and at least another level has shown significance.</p>
<p>We can now check the diagnostic plot for the new model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fitted_values_mlr</span> <span class="o">=</span> <span class="n">model_mlr</span><span class="o">.</span><span class="n">fittedvalues</span>
<span class="n">residuals_mlr</span> <span class="o">=</span> <span class="n">model_mlr</span><span class="o">.</span><span class="n">resid</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">fitted_values_mlr</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">residuals_mlr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Residuals vs. Fitted (MLR)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Fitted Values (Predicted MPG)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">residuals_mlr</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Q-Q Plot of Residuals (MLR)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/447413b46cd6155b5041d387da3adf2a63361982f09cd1c7b3a533682c42b974.png" src="../_images/447413b46cd6155b5041d387da3adf2a63361982f09cd1c7b3a533682c42b974.png" />
</div>
</div>
<p>As we can see, there have been some improvements in the left part of the Q-Q plot, but the model is still not perfect. We will have to go beyond a linear model (next lecture) to solve the issue.</p>
</section>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Chapter 3 of [1]</p></li>
<li><p>Parts of chapter 11 of [2]</p></li>
</ul>
<p>[1] Heumann, Christian, and Michael Schomaker Shalabh. Introduction to statistics and data analysis. Springer International Publishing Switzerland, 2016.</p>
<p>[2] James, Gareth Gareth Michael. An introduction to statistical learning: with applications in Python, 2023.<a class="reference external" href="https://www.statlearning.com">https://www.statlearning.com</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="08_predictive_modeling.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction to Predictive Analysis</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-auto-mpg-dataset">The Auto MPG Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-models">Regression Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">Simple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-coefficients-ordinary-least-squares-ols">Estimating the Coefficients - Ordinary Least Squares (OLS)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">Empirical Risk Minimization</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-coefficients-of-linear-regression">Interpretation of the Coefficients of Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-of-the-coefficient-estimates">Accuracy of the Coefficient Estimates</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-of-the-regression-coefficients">Confidence Intervals of the Regression Coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-tests-for-the-significance-of-coefficients">Statistical Tests for the Significance of Coefficients</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-model-accuracy">Assessing Model Accuracy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-building-block-residuals-and-rss">The Building Block: Residuals and RSS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-understanding-goodness-of-fit">Metrics for Understanding (Goodness-of-Fit)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-standard-error-rse">1. Residual Standard Error (RSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-r-2-statistic">2. The <span class="math notranslate nohighlight">\(R^2\)</span> Statistic</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-prediction-test-set-error">Metrics for Prediction (Test Set Error)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">1. Mean Squared Error (MSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">2. Root Mean Squared Error (RMSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">3. Mean Absolute Error (MAE)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-examples">Visual Examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnostic-plots-checking-model-validity">Diagnostic Plots: Checking Model Validity</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-residuals-vs-fitted-plot">1. The Residuals vs. Fitted Plot</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-q-q-plot-quantile-quantile-plot">2. The Q-Q Plot (Quantile-Quantile Plot)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">Multiple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometrical-interpretation">Geometrical Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-interpretation">Statistical Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-regression-coefficients">Estimating the Regression Coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-f-test">The F-Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection-by-backward-elimination">Variable Selection by Backward Elimination</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collinearity-and-the-instability-of-least-squares">Collinearity and the Instability of Least Squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-2">Adjusted <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qualitative-predictors">Qualitative Predictors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictors-with-only-two-levels">Predictors with Only Two Levels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictors-with-more-than-two-levels">Predictors with More than Two Levels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-in-practice">Linear Regression in Practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Simple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-the-top-table-model-metadata">Part 1: The Top Table (Model Metadata)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-the-coefficients-table-the-core">Part 2: The Coefficients Table (The Core)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-the-bottom-table-residual-diagnostics">Part 3: The Bottom Table (Residual Diagnostics)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-diagnostics">Residual Diagnostics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-diagnostic-plots">Interpreting the Diagnostic Plots</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Multiple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-summary-table-mlr">Interpreting the Summary Table (MLR)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection">Variable Selection</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>