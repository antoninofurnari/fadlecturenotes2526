

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Classification Task, Evaluation Measures, and K-Nearest Neighbor &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/11_classification_knn';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Logistic Regression - Statistical View" href="12_logistic_regression.html" />
    <link rel="prev" title="Beyond Linear Regression" href="10_beyond_linear_regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamentals of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">Introduction to the Labs and Work Environment Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_python_crash_course.html">Python Crash Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_python_data_science_crash_course.html">Python for Data Science Crash Course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro_data_analysis.html">Data Analysis Key Concepts, Loading and Inspecting the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="02_describing_and_visualizing_the_data.html">Describing and Visualizing the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_probability_for_data_analysis.html">Probability for Data Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_association_between_variables.html">Association between variables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05_data_distributions.html">Data Distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_statistical_inference.html">Statistical Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="07_storytelling_with_data.html">Storytelling with Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_predictive_modeling.html">Introduction to Predictive Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_linear_regression.html">Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 11</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_beyond_linear_regression.html">Beyond Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 12</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Classification Task, Evaluation Measures, and K-Nearest Neighbor</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 13</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_logistic_regression.html">Logistic Regression - Statistical View</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 14</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_multiclass_logistic_regression.html">Multiclass Logistic Regression and Predictive View</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 15</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="14_map_naive_bayes.html">Generative Classifiers and Naive Bayes</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes2526/blob/master/lecturenotes/lectures/11_classification_knn.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526/issues/new?title=Issue%20on%20page%20%2Flectures/11_classification_knn.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/11_classification_knn.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Classification Task, Evaluation Measures, and K-Nearest Neighbor</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-definition">Task Definition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-measures">Evaluation Measures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy">Accuracy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-rate">Error Rate</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-imbalanced-dataset">Example - Imbalanced Dataset</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-types">Error Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion Matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-spam-detector">Example 1 – Spam Detector</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-imbalanced-dataset">Example 2 – Imbalanced Dataset</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-and-recall">Precision and Recall</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-precision-vs-high-recall">High Precision vs High Recall</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-spam-detector">Example – Spam Detector</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#f-1-score"><span class="math notranslate nohighlight">\(F_{1}\)</span> Score</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Example - Spam Detector</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix-for-multi-class-classification">Confusion Matrix for Multi-Class Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-receiver-operating-characteristic-curve-and-area-under-the-curve-auc-measures">ROC (Receiver Operating Characteristic) Curve and Area Under the Curve (AUC) Measures</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-classifier-the-threshold-model">A Simple Classifier: The Threshold Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-roc-curve-auc">Interpreting the ROC Curve &amp; AUC</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-choose-an-optimal-threshold">How to Choose an Optimal Threshold</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#final-evaluation-at-the-optimal-threshold">Final Evaluation at the Optimal Threshold</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbor-classification">K-Nearest Neighbor Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-nearest-neighbor-or-1-nn-classification-algorithm">The Nearest Neighbor (or 1-NN) Classification Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-k-nearest-neighbour-classification-algorithm">The K-Nearest Neighbour Classification Algorithm</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-curse-of-dimensionality">The Curse of Dimensionality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-empty-space-problem-2d-vs-3d">The “Empty Space” Problem (2D vs. 3D)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-impact-on-knn">The Impact on KNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#takeways">Takeways</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deeper-look-knn-as-a-discriminative-classifier">A Deeper Look: KNN as a Discriminative Classifier</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-classifiers">Generative Classifiers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discriminative-classifiers">Discriminative Classifiers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#knn-example-the-fisher-iris-dataset">KNN Example: The Fisher Iris Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fisher-s-iris-dataset">Fisher’s Iris Dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-and-exploring-the-data">Loading and Exploring the Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-for-modeling">Preparing for Modeling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-knn-model">Training the KNN Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-model">Evaluating the Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-results">Interpreting the Results</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#macro-vs-micro-averaging">Macro vs. Micro Averaging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-map-decision-boundary-of-k-nn-and-importance-of-parameter-k">Classification Map/Decision Boundary Of K-NN and Importance of Parameter K</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-best-k">Finding the best <span class="math notranslate nohighlight">\(K\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-prepare-data">Step 1: Prepare Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-run-cross-validation-to-find-best-k">Step 2: Run Cross-Validation to Find Best K</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-final-training-and-evaluation">Step 3: Final Training and Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-knn-for-regression">K-Nearest Neighbors (KNN) for Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-bias-variance-tradeoff-overfit-vs-underfit">Visualizing the Bias-Variance Tradeoff (Overfit vs. Underfit)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-plot">Interpretation of the Plot</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-optimal-k-with-cross-validation">Finding the Optimal <span class="math notranslate nohighlight">\(K\)</span> with Cross-Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#final-evaluation">Final Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="classification-task-evaluation-measures-and-k-nearest-neighbor">
<h1>Classification Task, Evaluation Measures, and K-Nearest Neighbor<a class="headerlink" href="#classification-task-evaluation-measures-and-k-nearest-neighbor" title="Permalink to this heading">#</a></h1>
<p>Classification models are another broad class of predictive models. Following the Machine Learning terminology, it is common to talk about classification or regression “tasks”. These are the two main classes of tasks belonging to the broader class of supervised learning.</p>
<section id="task-definition">
<h2>Task Definition<a class="headerlink" href="#task-definition" title="Permalink to this heading">#</a></h2>
<p>Recall that a predictive model is a function defined as follows:</p>
<div class="math notranslate nohighlight">
\[h : \mathcal{X} \to \mathcal{Y}\]</div>
<p>If in the case of regression <span class="math notranslate nohighlight">\(\mathcal{X}=\Re^n\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}=\Re^m\)</span>, in the case of classification, the output targets are discrete values <strong>which are generally referred to as “classes”</strong>. Without loss of generality, if there are <span class="math notranslate nohighlight">\(M\)</span> classes, then we define <span class="math notranslate nohighlight">\(\mathcal{Y}=\{1,\ldots,M\}\)</span>. If the inputs are numerical vectors (it does not have to always be this way, but we can usually employ a representation function to map the input to a numerical vector), a classification model can be defined as:</p>
<div class="math notranslate nohighlight">
\[h : \Re^n \to \{0,\ldots,M-1\}\]</div>
<p>Also in this case, we will assume to have a set of data to train and evaluate our model</p>
<div class="math notranslate nohighlight">
\[\text{D}=\{(x_i,y_i)\}_{i=1}^N\]</div>
<p>Note that in this case <span class="math notranslate nohighlight">\(x_i\in \Re^n\)</span> and <span class="math notranslate nohighlight">\(y_i \in \{0,\ldots,M-1\}\)</span>. Here, the values <span class="math notranslate nohighlight">\(y_i\)</span> are generally called “<strong>labels</strong>”, while the values <span class="math notranslate nohighlight">\(\hat y = h(x)\)</span> predicted using the classifier <span class="math notranslate nohighlight">\(h\)</span> are called <strong>predicted labels</strong>.</p>
<p>We can find the optimal model <span class="math notranslate nohighlight">\(h\)</span> by minimizing the empirical risk. A possible loss function is the following one:</p>
<div class="math notranslate nohighlight">
\[\begin{split}L(\hat y, y) = \begin{cases}1 &amp;\text{ if }&amp; \hat y \neq y \\ 0 &amp;\text{ if }&amp; \hat y=y \end{cases}\end{split}\]</div>
<p>Hence, the empirical risk will be the fraction of incorrect predictions:</p>
<div class="math notranslate nohighlight">
\[R_{emp}(h) = \frac{1}{N} \sum_{i=1}^N L(\hat y_i, y_i) = \frac{\text{number of incorrect predictions}}{N}\]</div>
<p>The empirical risk computed as defined above is also known as <strong>error rate</strong>. It is a number comprised between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> which can also be interpreted as a percentage.</p>
<p>The classifier can be learned by minimizing the empirical risk:</p>
<div class="math notranslate nohighlight">
\[\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h)\]</div>
<section id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">#</a></h3>
<p>Classifiers support decision making any time that observations need to be categorized in one of a predefined set of classes. Examples of such problems are:</p>
<ul class="simple">
<li><p>Detecting spam emails (<strong>spam vs legitimate email classification</strong>).</p></li>
<li><p>Classifying social media posts as being about politics or something else
(<strong>politics vs non-politics classification</strong>).</p></li>
<li><p>Recognizing the object depicted in an image out of 1000 different
objects (<strong>object recognition</strong>).</p></li>
</ul>
<p>For example, we can define spam detection as follows:</p>
<ul class="simple">
<li><p><em>Task</em>: given an e-mail, classify it as spam or non-spam.</p></li>
<li><p><em>Input example</em> <span class="math notranslate nohighlight">\(\mathbf{e}\)</span>: the text of the e-mail. This can be a
sequence of characters of arbitrary length. We can use some <strong>representation function</strong> to map an email <span class="math notranslate nohighlight">\(\mathbf{e}\)</span> to a vector of real numbers <span class="math notranslate nohighlight">\(\mathbf{x} \in \Re^n\)</span>. We also assume that a training set pairing the vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with labels <span class="math notranslate nohighlight">\(y \in \{ 0,1\}\)</span> is available.</p></li>
<li><p><em>Classifier</em>: a function <span class="math notranslate nohighlight">\(h:\Re^n \rightarrow \{ 0,1\}\)</span>.</p></li>
<li><p><em>Output</em>: a predicted label  <span class="math notranslate nohighlight">\(\widehat{y} \in \{ 0,1\}\)</span> indicating
if the e-mail is legitimate or spam. Here we have a <strong>binary
classification task</strong>, hence <span class="math notranslate nohighlight">\(M = 2\)</span>.</p></li>
</ul>
</section>
</section>
<section id="evaluation-measures">
<h2>Evaluation Measures<a class="headerlink" href="#evaluation-measures" title="Permalink to this heading">#</a></h2>
<p>As with the case of regression, we need to define evaluation measures. These will be useful to  <strong>guide
training</strong> (e.g., modify the parameters in order to improve the performance of the algorithm on the training set), <strong>tune hyper-parameters</strong> and to <strong>finally assess that the algorithm works on unseen data</strong> (the test set).</p>
<p>As in the case of regression, we will consider the set of <strong>ground truth test labels</strong>:</p>
<div class="math notranslate nohighlight">
\[Y_{TE} = \left\{ y^{(i)}|\left( \mathbf{x}^{(i)},y^{(i)} \right) \in TE \right\}_{i}\]</div>
<p>and the set of <strong>predicted test labels</strong> :</p>
<div class="math notranslate nohighlight">
\[{\widehat{Y}}_{TE} = \left\{ h\left( \mathbf{x}^{(i)} \right)|\left( \mathbf{x}^{(i)},y^{(i)} \right) \in TE \right\}_{i}\]</div>
<p>which will be used as inputs of our performance measures.</p>
<section id="accuracy">
<h3>Accuracy<a class="headerlink" href="#accuracy" title="Permalink to this heading">#</a></h3>
<p>Accuracy is a very common performance measure. We define accuracy as
<strong>the percentage of test examples for which our algorithm has predicted
the correct label</strong>:</p>
<div class="math notranslate nohighlight">
\[Accuracy\left( Y_{TE},{\widehat{Y}}_{TE} \right) = \frac{\left| \left\{ y^{(i)} : y^{(i)} = {\widehat{y}}^{(i)} \right\} \right|}{|Y_{TE}|}\]</div>
<p>For instance, if the test set contains <span class="math notranslate nohighlight">\(100\)</span> examples and for <span class="math notranslate nohighlight">\(70\)</span> of
them we have predicted the correct label, then we will have:</p>
<div class="math notranslate nohighlight">
\[Accuracy\left( Y_{TE},{\widehat{Y}}_{TE} \right) = \frac{70}{100} = 0.7\]</div>
<p>Note that accuracy is always a number comprised between 0 and 1. We can
see the accuracy as a percentage. For instance, in the example above we
could say that we have an accuracy of <span class="math notranslate nohighlight">\(70\%\)</span>.</p>
</section>
<section id="error-rate">
<h3>Error Rate<a class="headerlink" href="#error-rate" title="Permalink to this heading">#</a></h3>
<p>The error rate is closely associated to accuracy and defined as:</p>
<div class="math notranslate nohighlight">
\[ErrorRate\left( Y_{TE},{\widehat{Y}}_{TE} \right) = \frac{\left| \left\{ y^{(i)} : y^{(i)} \neq {\widehat{y}}^{(i)} \right\} \right|}{|Y_{TE}|} =  1- Accuracy\left( Y_{TE},{\widehat{Y}}_{TE} \right)\]</div>
<section id="example-imbalanced-dataset">
<h4>Example - Imbalanced Dataset<a class="headerlink" href="#example-imbalanced-dataset" title="Permalink to this heading">#</a></h4>
<p>To see the limits of accuracy, let us consider a dataset containing
<span class="math notranslate nohighlight">\(10000\)</span> elements of two classes distributed as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(500\)</span> data points from class <span class="math notranslate nohighlight">\(0\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(9500\)</span> data points from class <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
</ul>
<p>Let us now consider a naïve classifier which always predicts class 1:</p>
<div class="math notranslate nohighlight">
\[f\left( \mathbf{x} \right) = 1\]</div>
<p>Intuitively, we see that this classifier is not a good one, as it
discards its input and just predicts the most frequent class. However,
it can be easily seen that its accuracy is <span class="math notranslate nohighlight">\(0.95\)</span>.</p>
</section>
</section>
<section id="error-types">
<h3>Error Types<a class="headerlink" href="#error-types" title="Permalink to this heading">#</a></h3>
<p>The main limitation of the accuracy is that it counts the number of
mistakes made by the algorithm, but it does not consider which types of
mistakes it makes. In general, on each example, we can make two kinds of mistakes:</p>
<ul class="simple">
<li><p><strong>Type 1</strong>: we classify an example as belonging to the considered
class, but it does not. These kinds of misclassifications are often
called <strong>False Positives (FP)</strong>.</p></li>
<li><p><strong>Type 2</strong>: we classify the example as not belonging to the
considered class, but it does belong to it. These kinds of
misclassifications are often called <strong>False Negatives (FN)</strong>.</p></li>
</ul>
<p>We can also have two kinds of correct predictions:</p>
<ul class="simple">
<li><p><strong>True Positives (TP)</strong>: these are elements from the considered
class which have been classified as actually belonging to that
class.</p></li>
<li><p><strong>True Negatives (TN)</strong>: these are elements which are not from the
considered class and have been classified as actually not belonging
to that class.</p></li>
</ul>
</section>
<section id="confusion-matrix">
<h3>Confusion Matrix<a class="headerlink" href="#confusion-matrix" title="Permalink to this heading">#</a></h3>
<p>To have a more complete view of how our classifier is performing, we can
put these numbers in a table which we will call a <strong>confusion matrix</strong>:</p>
<p><img alt="" src="../_images/confusion.png" /></p>
<p>In the matrix, the rows indicate the true labels, whereas the columns
indicate the predicted labels. A good confusion matrix has large numbers
in the main diagonal (TP and TN) and low numbers in the rest of the
matrix (where the errors are).</p>
<p>The accuracy can be recovered from the confusion matrix as follows:</p>
<div class="math notranslate nohighlight">
\[Accuracy = \frac{TP + TN}{TP + FN + FP + TN}\]</div>
<p>which consists in <strong>summing the numbers on the diagonal and dividing by
the sum of all numbers</strong>. We can see the computation of the accuracy
from the confusion matrix graphically as follows:</p>
<p><img alt="" src="../_images/confusion2.png" /></p>
<section id="example-1-spam-detector">
<h4>Example 1 – Spam Detector<a class="headerlink" href="#example-1-spam-detector" title="Permalink to this heading">#</a></h4>
<p>Let us consider a spam detector which correctly detects <span class="math notranslate nohighlight">\(40\)</span> out of <span class="math notranslate nohighlight">\(50\)</span>
spam emails, while it only recognizes <span class="math notranslate nohighlight">\(30\)</span> out of <span class="math notranslate nohighlight">\(50\)</span> legitimate
emails. The confusion matrix associated to this classifier will be as
follows:</p>
<p><img alt="" src="../_images/spam.png" /></p>
<p>Its accuracy will be:</p>
<div class="math notranslate nohighlight">
\[Accuracy = \frac{40 + 30}{40 + 10 + 20 + 30} = \frac{70}{100} = 0.7\]</div>
</section>
<section id="example-2-imbalanced-dataset">
<h4>Example 2 – Imbalanced Dataset<a class="headerlink" href="#example-2-imbalanced-dataset" title="Permalink to this heading">#</a></h4>
<p>Let us now consider the example of our imbalanced dataset with <span class="math notranslate nohighlight">\(9500\)</span>
data points of class 1 and <span class="math notranslate nohighlight">\(500\)</span> data points of class 0. The confusion
matrix of the naïve classifier <span class="math notranslate nohighlight">\(f\left( \mathbf{x} \right) = 1\)</span> will be:</p>
<p><img alt="" src="../_images/imbalanced.png" /></p>
<p>If we compute the accuracy of this classifier, we will obtain a good
performance:</p>
<div class="math notranslate nohighlight">
\[Accuracy = \frac{9500}{9500 + 500} = 0.95\]</div>
<p>However, looking at the confusion matrix, <strong>it is clear that something
is wrong, and our classifier is not working well</strong>.</p>
</section>
</section>
<section id="precision-and-recall">
<h3>Precision and Recall<a class="headerlink" href="#precision-and-recall" title="Permalink to this heading">#</a></h3>
<p>The confusion matrix allows to understand if there is an issue with the
classifier in the case of imbalanced data. However, it is still
convenient to have scalar measures which can tell us something about how
the classifier is doing. In practice, it is common to define two
complementary measures: precision and recall.</p>
<p><strong>Precision</strong> measures <strong>how many of the examples which have been
classified as positives were actually positives</strong> and is defined as
follows:</p>
<div class="math notranslate nohighlight">
\[Precision = \frac{TP}{TP + FP}\]</div>
<p><strong>Recall</strong> measures <strong>how many of the examples which are positives, have
been correctly classified as positives</strong> and is defined as follows:</p>
<div class="math notranslate nohighlight">
\[Recall = \frac{TP}{TP + FN}\]</div>
<p>We can see graphically the computation of precision and recall as
follows:</p>
<p><img alt="" src="../_images/precision_recall.png" /></p>
<section id="high-precision-vs-high-recall">
<h4>High Precision vs High Recall<a class="headerlink" href="#high-precision-vs-high-recall" title="Permalink to this heading">#</a></h4>
<p>These values capture different properties of the classifier. Depending
on the application, we may want to have a higher precision or a higher
recall. For example:</p>
<ul class="simple">
<li><p>Consider a <strong>spam detector</strong>: we may want to have a very <strong>high
precision</strong>, even at the cost of a <strong>low recall</strong>. Indeed, we want
to make sure that if we classify an e-mail as spam (and hence we
filter it out), it is actually spam (hence a high precision). This
is acceptable even if sometimes we let a spam email get through the
filter (hence a low recall).</p></li>
<li><p>Consider a <strong>medical pre-screening</strong> test which is used to assess if
a patient is likely to have a given pathology. The test is cheap
(e.g., a blood test) and can be made on a large sample of patients.
If the test is positive, we then perform a more expensive but
accurate test. In this case, we want to have a <strong>high recall</strong>.
Indeed, if a patient has the pathology, we want to detect it and
send the patient for the second, more accurate test (hence a high
precision). This is acceptable even if sometimes we have false
positives (hence a low precision). Indeed, if we wrongly detect a
pathology, the second test will give the correct result.</p></li>
</ul>
<p>Precision and recall can often have contrasting values (e.g., we can
obtain a high precision but a low recall and vice versa), hence it is
generally <strong>necessary to look at both numbers together</strong>.</p>
</section>
<section id="example-spam-detector">
<h4>Example – Spam Detector<a class="headerlink" href="#example-spam-detector" title="Permalink to this heading">#</a></h4>
<p>Let us consider again the spam example, with the classifier obtaining
this confusion matrix:</p>
<p><img alt="" src="../_images/spam.png" /></p>
<p>From the confusion matrix, we see that:</p>
<ul class="simple">
<li><p>TP=40.</p></li>
<li><p>FN=10.</p></li>
<li><p>FP=20.</p></li>
<li><p>TN=30.</p></li>
</ul>
<p>We can compute the following precision and recall values:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Precision = \frac{40}{20 + 30} = 0.8\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(Recall = \frac{40}{40 + 20} = 0.67\)</span>.</p></li>
</ul>
<p>Like the accuracy, precision and recall are telling us that the
classifier is not perfect. Interestingly, these measures are telling us
that <strong>while most of the detected e-mails are actually spam, not all
spam e-mails are correctly detected.</strong> Considering this application, we
may want to have a very high precision, (i.e., if we detected an e-mail
as spam, we want to make sure that it is actually spam) <strong>even at the
cost of a lower recall.</strong></p>
</section>
</section>
<section id="f-1-score">
<h3><span class="math notranslate nohighlight">\(F_{1}\)</span> Score<a class="headerlink" href="#f-1-score" title="Permalink to this heading">#</a></h3>
<p>We have seen that precision and recall describe different aspects of the
classifier and hence it is often a good idea to look at them jointly.
However, it is often convenient to have a single number which classifies
both numbers.</p>
<p>The <span class="math notranslate nohighlight">\(F_{1}\)</span> score allows to do exactly this, by computing the <strong>harmonic
mean</strong> of precision and recall:</p>
<div class="math notranslate nohighlight">
\[F_{1} = 2 \cdot \frac{precision \cdot recall}{precision + recall}\]</div>
<p>We can note that, in order to obtain a large <span class="math notranslate nohighlight">\(F_{1}\)</span> score, we need to
obtain <strong>both a large precision and a large recall.</strong> This is a property
of the harmonic mean, as it is illustrated in the following example
which compares the arithmetic mean (precision/2+recall/2) to the
harmonic mean (the <span class="math notranslate nohighlight">\(F_{1}\)</span> score):</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/6678341ea27a2652e70eed0fb63179561aeebb8bd354142805d99d147e2aeed9.png" src="../_images/6678341ea27a2652e70eed0fb63179561aeebb8bd354142805d99d147e2aeed9.png" />
</div>
</div>
<p>The example above shows the isocurves obtained by considering given
precision and recall values. As can be noted, to obtain a large <span class="math notranslate nohighlight">\(F_{1}\)</span>
score, we need to have both a large precision and a large recall.</p>
<section id="id1">
<h4>Example - Spam Detector<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h4>
<p>Let us consider again the spam example, with the classifier obtaining
this confusion function:</p>
<p><img alt="" src="../_images/spam.png" /></p>
<p>Starting from the precision and recall values previously computed:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Precision = \frac{40}{20 + 30} = 0.8\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(Recall = \frac{40}{40 + 20} = 0.67\)</span>.</p></li>
</ul>
<p>we can compute the <span class="math notranslate nohighlight">\(F_{1}\)</span> score as:</p>
<div class="math notranslate nohighlight">
\[F_{1} = 2\frac{precision \cdot recall}{recall + recall} = \frac{1.072}{1.47} = 0.72\]</div>
<p>Note that, since the dataset is balanced, this value is not very
different from the accuracy of <span class="math notranslate nohighlight">\(0.7\)</span>.</p>
</section>
</section>
<section id="confusion-matrix-for-multi-class-classification">
<h3>Confusion Matrix for Multi-Class Classification<a class="headerlink" href="#confusion-matrix-for-multi-class-classification" title="Permalink to this heading">#</a></h3>
<p>We have seen the confusion matrix in the case of binary classification.
However, it should be noted that the confusion matrix generalizes to the
case in which there are <span class="math notranslate nohighlight">\(M\)</span> classes. In that case, the confusion matrix
is <span class="math notranslate nohighlight">\(M \times M\)</span> and its general element <span class="math notranslate nohighlight">\(C_{ij}\)</span> indicates the number of
elements <strong>belonging to class i, which have been classified as belonging
to class j</strong>. An example of a confusion matrix in the case of three
classes is the following:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/e188a825852ac5baab586c023c222b77c262353fcb98511ad8b6093ee8de003c.png" src="../_images/e188a825852ac5baab586c023c222b77c262353fcb98511ad8b6093ee8de003c.png" />
</div>
</div>
<p>Similar to the binary case, we expect to have large numbers on the
diagonal and small number in all other cells. The concepts of precision,
recall and <span class="math notranslate nohighlight">\(F_{1}\)</span> score generalize considering a binary classification
task for each of the classes (i.e., distinguishing each class from all
the others). Hence, in the example shown above, we would have three
<span class="math notranslate nohighlight">\(F_{1}\)</span> scores:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1.         0.8        0.66666667 0.85714286]
</pre></div>
</div>
</div>
</div>
</section>
<section id="roc-receiver-operating-characteristic-curve-and-area-under-the-curve-auc-measures">
<h3>ROC (Receiver Operating Characteristic) Curve and Area Under the Curve (AUC) Measures<a class="headerlink" href="#roc-receiver-operating-characteristic-curve-and-area-under-the-curve-auc-measures" title="Permalink to this heading">#</a></h3>
<p>As previously mentioned, some binary classifiers output a probability or a confidence score and allow to obtain class predictions by thresholding on such probabilities or scores. When the confidence value is not a probability, it might not be easy to interpret it and find a good threshold. Even when the classifier outputs a probability, the optimal threshold might not be <span class="math notranslate nohighlight">\(0.5\)</span>. For instance, we may want to build an intrusion detection system which is more or less sensitive to potential intrusions.</p>
<p>The ROC curve allows to evaluate the performance of a classifier independently from the threshold. Specifically, let</p>
<div class="math notranslate nohighlight">
\[c(\mathbf{x})\]</div>
<p>be a function predicting a confidence value from an input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. For instance, in the case of the logistic regressor:</p>
<div class="math notranslate nohighlight">
\[c(\mathbf{x}) = \sigma(\beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n)\]</div>
<p>We will define our classification function as:</p>
<div class="math notranslate nohighlight">
\[h_\theta(\mathbf{x}) = [\sigma(\beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n) \geq \theta]\]</div>
<p>where <span class="math notranslate nohighlight">\([\cdot]\)</span> denotes the Iverson brackets and <span class="math notranslate nohighlight">\(\theta \in \Re\)</span> is a real-valued threshold.</p>
<p>Depending on the chosen value of the threshold, we will have a given number of true positives, true negatives, false positives and false negatives:</p>
<div class="math notranslate nohighlight">
\[TP_\theta\]</div>
<div class="math notranslate nohighlight">
\[TN_\theta\]</div>
<div class="math notranslate nohighlight">
\[FP_\theta\]</div>
<div class="math notranslate nohighlight">
\[FN_\theta\]</div>
<p>We will define the true positive rate (TPR) and false positive rate (FPR) as follows:</p>
<div class="math notranslate nohighlight">
\[TPR_\theta = \frac{TP_\theta}{TP_\theta+FN_\theta}\]</div>
<div class="math notranslate nohighlight">
\[FPR_\theta = \frac{FP_\theta}{FP_\theta+TN_\theta}\]</div>
<p>In practice:</p>
<ul class="simple">
<li><p>The TPR is the fraction of true positives over all positive elements - <strong>this is the same as the recall</strong>;</p></li>
<li><p>The FPR on the contrary is the fraction of false positive predictions over all negative elements.</p></li>
</ul>
<p>We note that:</p>
<ul class="simple">
<li><p>If we pick <strong>low threshold values, both the TPR and the FPR will be equal to 1</strong>. Indeed, with a small enough threshold, all elements will be classified as positives and there will be no predicted negatives, so the TPR will be equal to 1. At the same time, the FPR will be 1 because we will have no true negatives;</p></li>
<li><p>If we pick <strong>high threshold values, both the TPR and the TNR will be zero</strong>. Indeed, with a large enough threshold, all elements will be classified as negatives and there will be no positive predictions, so the TPR will be zero. At the same time, since all elements will be classified as negatives and there will be no false positive predictions, the FPR will be equal to <span class="math notranslate nohighlight">\(0\)</span>;</p></li>
</ul>
<p>An ROC curve is obtained by picking a threshold value <span class="math notranslate nohighlight">\(\theta\)</span> and plotting a 2D point <span class="math notranslate nohighlight">\((TPR_\theta, TNR_\theta)\)</span>. By varying the threshold <span class="math notranslate nohighlight">\(\theta\)</span>, we obtain a curve which tell us what is the trade-off between TPR and TNR regardless of the threshold.</p>
<p>The following plot show an example of an ROC curve for a binary classifier on the breast cancer dataset:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/83711cab489351b4ff2fb89c267154e429ebe3ca6f115674ce45a51cc3558953.png" src="../_images/83711cab489351b4ff2fb89c267154e429ebe3ca6f115674ce45a51cc3558953.png" />
</div>
</div>
<p>We know that the two points <span class="math notranslate nohighlight">\((0,0)\)</span> and <span class="math notranslate nohighlight">\((1,1)\)</span> belong to the curve. Ideally, starting from a low threshold (with TPR=FPR=1), as we move the threshold up enough, <strong>we would expect the FPR to decrease (we are discarding false positives), while the TPR is still high (we are not discarding true positives)</strong>. Hence the ideal curve should be a rectangular curve touching point <span class="math notranslate nohighlight">\((0,1)\)</span>. In practice, we can measure the area under the ROC curve to see how well the classifier is doing. This value is generally referred to as “AUC”.</p>
<p>The dashed line indicates the performance of a random predictor. Note the the area under the curve identified by this dashed line will be equal to <span class="math notranslate nohighlight">\(0.5\)</span>. Any curve which is systematically below this line indicates a classifier which is doing thresholding in the wrong way (i.e. we should invert the sign of the thresholding). For instance, this is the curve of the same classifier when we invert the sign of thresholding:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/9da49d3f0e3c396be11a7bc7898c40adccf76e915edef84382da07cac7abb8ff.png" src="../_images/9da49d3f0e3c396be11a7bc7898c40adccf76e915edef84382da07cac7abb8ff.png" />
</div>
</div>
<p>We can use ROC curves to compare two different classifiers as shown in the following:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/524e0a56b09a0f53ab9e49e9717695058a743d6bdf1e33b7502e7837a5d28841.png" src="../_images/524e0a56b09a0f53ab9e49e9717695058a743d6bdf1e33b7502e7837a5d28841.png" />
</div>
</div>
</section>
</section>
<section id="a-simple-classifier-the-threshold-model">
<h2>A Simple Classifier: The Threshold Model<a class="headerlink" href="#a-simple-classifier-the-threshold-model" title="Permalink to this heading">#</a></h2>
<p>Let’s start with the simplest possible classifier. We’ll use a dataset of heights and weights to predict a person’s <code class="docutils literal notranslate"><span class="pre">sex</span></code> (Male or Female).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>

<span class="c1"># Load the dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;http://antoninofurnari.it/downloads/height_weight.csv&#39;</span><span class="p">)</span>

<span class="c1"># Let&#39;s look at the height distributions</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;sex&#39;</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Height Distribution by Sex&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/23478a576469c7447e6cced7943a9e802da1421eea7c924cdae7e8b910af7187.png" src="../_images/23478a576469c7447e6cced7943a9e802da1421eea7c924cdae7e8b910af7187.png" />
</div>
</div>
<p>The plot clearly shows that, on average, males are taller than females. This suggests we could build a simple classifier:</p>
<p><strong>“If a person’s <code class="docutils literal notranslate"><span class="pre">height</span></code> is above a certain <em>threshold</em>, we predict ‘Male’.”</strong></p>
<p>Let’s arbitrarily pick a threshold of <strong>170 cm</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Our &quot;model&quot; is just a simple threshold</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mi">170</span>

<span class="c1"># 1. Get the predicted labels (True for &#39;Male&#39;, False for &#39;Female&#39;)</span>
<span class="c1"># This is our model&#39;s prediction, ŷ</span>
<span class="n">male_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span>

<span class="c1"># 2. Get the &quot;Ground Truth&quot; labels</span>
<span class="c1"># This is the true y</span>
<span class="n">male_gt</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;M&#39;</span><span class="p">)</span>

<span class="c1"># Now, let&#39;s see how well our simple &quot;model&quot; did.</span>
<span class="c1"># We will build a Confusion Matrix.</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">male_pred</span><span class="p">)</span>

<span class="c1"># Let&#39;s visualize it</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Predicted: Female&#39;</span><span class="p">,</span> <span class="s1">&#39;Predicted: Male&#39;</span><span class="p">],</span>
            <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;True: Female&#39;</span><span class="p">,</span> <span class="s1">&#39;True: Male&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Confusion Matrix (Threshold = </span><span class="si">{</span><span class="n">threshold</span><span class="si">}</span><span class="s1"> cm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">male_pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1efb1866388e31d185f57c6302dfd388362acb3645b95cd3b4ae6a2e5e7fb8ee.png" src="../_images/1efb1866388e31d185f57c6302dfd388362acb3645b95cd3b4ae6a2e5e7fb8ee.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

       False       0.88      0.79      0.83      2285
        True       0.78      0.87      0.82      1946

    accuracy                           0.83      4231
   macro avg       0.83      0.83      0.83      4231
weighted avg       0.83      0.83      0.83      4231
</pre></div>
</div>
</div>
</div>
<p>Our model has only a single threshold parameter. As we can see, changing it affects performance. For instance, setting the threshold ot <span class="math notranslate nohighlight">\(180cm\)</span> gives this confusion matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Our &quot;model&quot; is just a simple threshold</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mi">180</span>

<span class="c1"># 1. Get the predicted labels (True for &#39;Male&#39;, False for &#39;Female&#39;)</span>
<span class="c1"># This is our model&#39;s prediction, ŷ</span>
<span class="n">male_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span>

<span class="c1"># 2. Get the &quot;Ground Truth&quot; labels</span>
<span class="c1"># This is the true y</span>
<span class="n">male_gt</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;M&#39;</span><span class="p">)</span>

<span class="c1"># Now, let&#39;s see how well our simple &quot;model&quot; did.</span>
<span class="c1"># We will build a Confusion Matrix.</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">male_pred</span><span class="p">)</span>

<span class="c1"># Let&#39;s visualize it</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Predicted: Female&#39;</span><span class="p">,</span> <span class="s1">&#39;Predicted: Male&#39;</span><span class="p">],</span>
            <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;True: Female&#39;</span><span class="p">,</span> <span class="s1">&#39;True: Male&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Confusion Matrix (Threshold = </span><span class="si">{</span><span class="n">threshold</span><span class="si">}</span><span class="s1"> cm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">male_pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a3b25273f9a7f3583d6a3382b85ee208110373b04e92b10f74afbecf8b2d178a.png" src="../_images/a3b25273f9a7f3583d6a3382b85ee208110373b04e92b10f74afbecf8b2d178a.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

       False       0.68      1.00      0.81      2285
        True       1.00      0.44      0.61      1946

    accuracy                           0.74      4231
   macro avg       0.84      0.72      0.71      4231
weighted avg       0.83      0.74      0.72      4231
</pre></div>
</div>
</div>
</div>
<p>The model is “worse” (more elements off diagonal), but it also found many more females. In practice, moving the threshold changes the balance between true positives and false positives. In practice, we can compute the ROC curve to visualize this trade-off:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The &quot;ground truth&quot; labels (1 for Male, 0 for Female)</span>
<span class="n">male_gt</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;M&#39;</span><span class="p">)</span>

<span class="c1"># The &quot;score&quot; our classifier uses. In this case, it&#39;s just the height itself.</span>
<span class="c1"># A higher score = more likely to be &#39;Male&#39;.</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">]</span>

<span class="c1"># 1. Calculate the ROC curve</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>

<span class="c1"># 2. Calculate the Area Under the Curve (AUC)</span>
<span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>

<span class="c1"># 3. Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;ROC curve (AUC = </span><span class="si">{</span><span class="n">roc_auc</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Random Guess&#39;</span><span class="p">)</span> <span class="c1"># The &quot;worst&quot; curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ideal Curve&#39;</span><span class="p">)</span> <span class="c1"># The &quot;best&quot; curve</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate (FPR)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate (TPR)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ROC Curve for Height-Based Classifier&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2b5ae539f94f8c0e81b0d5342e351e17c1afba2da6c289ddbcccaeec241fb650.png" src="../_images/2b5ae539f94f8c0e81b0d5342e351e17c1afba2da6c289ddbcccaeec241fb650.png" />
</div>
</div>
<section id="interpreting-the-roc-curve-auc">
<h3>Interpreting the ROC Curve &amp; AUC<a class="headerlink" href="#interpreting-the-roc-curve-auc" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>The Curve (Orange):</strong> This shows our classifier. Each point on this line is a different threshold.</p></li>
<li><p><strong>The “Random Guess” Line (Blue Dashed):</strong> This is the performance of a coin flip (AUC = 0.5). Any model below this line is useless.</p></li>
<li><p><strong>The “Ideal” Curve (Green Dotted):</strong> A perfect classifier would go straight to the top-left corner (100% TPR, 0% FPR).</p></li>
</ul>
<p>The <strong>Area Under the Curve (AUC)</strong> summarizes this entire plot into a single number from 0 to 1.</p>
<ul class="simple">
<li><p><strong>AUC = 1.0:</strong> A perfect classifier.</p></li>
<li><p><strong>AUC = 0.5:</strong> A useless (random) classifier.</p></li>
<li><p><strong>AUC &lt; 0.5:</strong> A classifier that is actively wrong (it’s better if you reverse its predictions).</p></li>
</ul>
<p>Our model, based <em>only on height</em>, has an <strong>AUC of 0.92</strong>, which is extremely good!</p>
<p>We can even use this method to compare classifiers. For example, is <code class="docutils literal notranslate"><span class="pre">height</span></code> or <code class="docutils literal notranslate"><span class="pre">weight</span></code> a better predictor of <code class="docutils literal notranslate"><span class="pre">sex</span></code>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate ROC for both features</span>
<span class="n">fpr_h</span><span class="p">,</span> <span class="n">tpr_h</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">])</span>
<span class="n">auc_h</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr_h</span><span class="p">,</span> <span class="n">tpr_h</span><span class="p">)</span>

<span class="n">fpr_w</span><span class="p">,</span> <span class="n">tpr_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">])</span>
<span class="n">auc_w</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr_w</span><span class="p">,</span> <span class="n">tpr_w</span><span class="p">)</span>

<span class="c1"># Plot both</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_h</span><span class="p">,</span> <span class="n">tpr_h</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Height (AUC = </span><span class="si">{</span><span class="n">auc_h</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_w</span><span class="p">,</span> <span class="n">tpr_w</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Weight (AUC = </span><span class="si">{</span><span class="n">auc_w</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Random Guess&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate (FPR)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate (TPR)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Classifier Comparison: Height vs. Weight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/744a9795912a0e8477313c2bee50896bb0e79530df8452085639e4ea9fab6cb1.png" src="../_images/744a9795912a0e8477313c2bee50896bb0e79530df8452085639e4ea9fab6cb1.png" />
</div>
</div>
<p>The plot clearly shows that <code class="docutils literal notranslate"><span class="pre">height</span></code> (AUC = 0.92) is a significantly better predictor than <code class="docutils literal notranslate"><span class="pre">weight</span></code> (AUC = 0.88).</p>
<p>This entire analysis (Confusion Matrix, TPR, FPR, ROC, AUC) is our toolkit for evaluating <em>any</em> classifier.</p>
</section>
<section id="how-to-choose-an-optimal-threshold">
<h3>How to Choose an Optimal Threshold<a class="headerlink" href="#how-to-choose-an-optimal-threshold" title="Permalink to this heading">#</a></h3>
<p>The ROC curve shows us <em>all</em> possible tradeoffs, but in the end, we must choose <strong>one single threshold</strong> for our model.</p>
<p>The “best” threshold depends on your goal:</p>
<ul class="simple">
<li><p><strong>High-Sensitivity (e.g., medical screening):</strong> You might choose a threshold that gives a <strong>high TPR (e.g., 0.95)</strong>, even if it means a higher FPR.</p></li>
<li><p><strong>High-Specificity (e.g., spam filter):</strong> You might choose a threshold that gives a <strong>very low FPR (e.g., 0.01)</strong>, even if you miss some positives.</p></li>
</ul>
<p>If you don’t have a specific preference and want a “balanced” model, a common method is to find the threshold that is “closest” to the top-left corner (0 FPR, 1 TPR).</p>
<p>A simple way to do this is to find the threshold that maximizes the sum of <strong>TPR</strong> and <strong>Specificity (1 - FPR)</strong>. This is also known as <strong>Youden’s J statistic</strong>.</p>
<div class="math notranslate nohighlight">
\[J = TPR + (1 - FPR)\]</div>
<p>Let’s find the threshold for our <code class="docutils literal notranslate"><span class="pre">height</span></code> classifier that maximizes this balanced score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We already have fpr_h, tpr_h, and thresholds_h from the &#39;height&#39; model</span>

<span class="c1"># 1. Calculate the balanced score (Youden&#39;s J) for each threshold</span>
<span class="n">balanced_score</span> <span class="o">=</span> <span class="n">tpr_h</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">fpr_h</span><span class="p">)</span>

<span class="c1"># 2. Find the index of the threshold that gives the maximum score</span>
<span class="n">best_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">balanced_score</span><span class="p">)</span>
<span class="n">best_threshold</span> <span class="o">=</span> <span class="n">thresholds</span><span class="p">[</span><span class="n">best_index</span><span class="p">]</span>
<span class="n">best_tpr</span> <span class="o">=</span> <span class="n">tpr_h</span><span class="p">[</span><span class="n">best_index</span><span class="p">]</span>
<span class="n">best_fpr</span> <span class="o">=</span> <span class="n">fpr_h</span><span class="p">[</span><span class="n">best_index</span><span class="p">]</span>

<span class="c1"># 3. Plot the score vs. the thresholds</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="c1"># We slice [1:] to avoid a plotting artifact from the &#39;inf&#39; threshold</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thresholds</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">balanced_score</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">best_threshold</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Optimal Threshold = </span><span class="si">{</span><span class="n">best_threshold</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> cm&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Finding the Optimal Threshold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Threshold (cm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Balanced Score (TPR + 1-FPR)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal threshold was found at </span><span class="si">{</span><span class="n">best_threshold</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> cm&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dd5273cc92096199518b7333a765c17a89428fbdc3f6e4423a7675aaffa9831f.png" src="../_images/dd5273cc92096199518b7333a765c17a89428fbdc3f6e4423a7675aaffa9831f.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal threshold was found at 172.72 cm
</pre></div>
</div>
</div>
</div>
</section>
<section id="final-evaluation-at-the-optimal-threshold">
<h3>Final Evaluation at the Optimal Threshold<a class="headerlink" href="#final-evaluation-at-the-optimal-threshold" title="Permalink to this heading">#</a></h3>
<p>The plot shows us exactly how the performance changes. The peak of the curve (our optimal threshold) is at <strong>169.57 cm</strong>.</p>
<p>Let’s see the final TPR and FPR if we use this single, balanced threshold for our classifier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We already found the best TPR/FPR from the argmax</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--- Final Model at Optimal Threshold (</span><span class="si">{</span><span class="n">best_threshold</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> cm) ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True Positive Rate (TPR):  </span><span class="si">{</span><span class="n">best_tpr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (We find </span><span class="si">{</span><span class="n">best_tpr</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% of all Males)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;False Positive Rate (FPR): </span><span class="si">{</span><span class="n">best_fpr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (We mislabel only </span><span class="si">{</span><span class="n">best_fpr</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% of Females)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">25</span><span class="p">)</span>

<span class="c1"># You can also re-calculate it manually to double-check</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Verifying with confusion_matrix:&quot;</span><span class="p">)</span>
<span class="n">male_pred_optimal</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">best_threshold</span><span class="p">)</span>
<span class="n">male_gt</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;M&#39;</span><span class="p">)</span>

<span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">male_pred_optimal</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">tpr_check</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">/</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fn</span><span class="p">)</span>
<span class="n">fpr_check</span> <span class="o">=</span> <span class="n">fp</span> <span class="o">/</span> <span class="p">(</span><span class="n">fp</span> <span class="o">+</span> <span class="n">tn</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TPR Check: </span><span class="si">{</span><span class="n">tpr_check</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;FPR Check: </span><span class="si">{</span><span class="n">fpr_check</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report at Optimal Threshold:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">male_pred_optimal</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Final Model at Optimal Threshold (172.72 cm) ---
True Positive Rate (TPR):  0.78 (We find 78% of all Males)
False Positive Rate (FPR): 0.12 (We mislabel only 12% of Females)
-------------------------

Verifying with confusion_matrix:
TPR Check: 0.78
FPR Check: 0.12

Classification Report at Optimal Threshold:
              precision    recall  f1-score   support

       False       0.83      0.88      0.85      2285
        True       0.85      0.78      0.82      1946

    accuracy                           0.84      4231
   macro avg       0.84      0.83      0.84      4231
weighted avg       0.84      0.84      0.84      4231
</pre></div>
</div>
</div>
</div>
<p>This method gives us a principled, balanced way to select a single threshold from our ROC curve. We find that a threshold of ~172.2 cm gives us a 78% TPR while only mislabeling 12% of females, which is a strong, balanced result.</p>
</section>
</section>
<section id="k-nearest-neighbor-classification">
<h2>K-Nearest Neighbor Classification<a class="headerlink" href="#k-nearest-neighbor-classification" title="Permalink to this heading">#</a></h2>
<p>Threshold-based classification is very limited as it works when we only have one feature. We will now see the K-Nearest Neighbor classifier, an intuitive, non-parametric algorithm based on the concept of <strong>similarity</strong> between data points. We will first start by introducing the simple 1-NN algorithm (or nearest neighbor).</p>
<section id="the-nearest-neighbor-or-1-nn-classification-algorithm">
<h3>The Nearest Neighbor (or 1-NN) Classification Algorithm<a class="headerlink" href="#the-nearest-neighbor-or-1-nn-classification-algorithm" title="Permalink to this heading">#</a></h3>
<p>Given an observation <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, the basic principle of Nearest Neighbor classification is to look at the nearest example in the training set and assign the same class. The idea is that nearby elements in the representation space will likely be similar. Consider the following example in which each data point is an email:</p>
<p><img alt="" src="../_images/knn1.png" /></p>
<p>We could think of the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> features as characteristics useful to determine if an e-mail is spam. For instance <span class="math notranslate nohighlight">\(x\)</span> may count the number of orthographic errors, while <span class="math notranslate nohighlight">\(y\)</span> may count the number of occurrences of given keywords such as “buy” or “viagra”.</p>
<p>Looking at the plot above, what would be the class of the new example (the red cross)?</p>
<p>We notice that, while the test example is not exactly equal to any training example, it is still
reasonably similar to <strong>some training examples</strong> (i.e., there are points
nearby in the Euclidean space).</p>
<p>Since the training and test sets have been collected in similar ways
(e.g., both contain spam and legitimate e-mails), we can hypothesize
that the <em>test example will be of the same class of similar examples
belonging to the training set</em>.</p>
<p>For instance, if two documents have similar word frequencies, they are
probably of the same class (e.g., if both contain the words “viagra” and
“sales” many times, they probably belong to “spam” class).</p>
<p>We can measure how “similar” two examples <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>’
are using a suitable distance function <span class="math notranslate nohighlight">\(d\)</span>:</p>
<div class="math notranslate nohighlight">
\[d(\mathbf{x},\mathbf{x}^{\mathbf{'}})\]</div>
<p>We expect similar examples to have a small distance. <em>A very common
choice for</em> <span class="math notranslate nohighlight">\(d\)</span> <em>is the Euclidean distance</em>:</p>
<div class="math notranslate nohighlight">
\[d\left( \mathbf{x,}\mathbf{x}^{\mathbf{'}} \right) = \left\| \mathbf{x -}\mathbf{x}^{\mathbf{'}} \right\|_{2} = \sqrt{\sum_{i = 1}^{N}\left( x_{i} - x_{i}^{'} \right)^{2}}\]</div>
<p>We can hence define a classifier which leverages our intuition
<strong>assigning to a test example</strong> <span class="math notranslate nohighlight">\(\mathbf{x}^{\mathbf{'}}\)</span><strong>the class of
the closest example in the training set</strong>:</p>
<div class="math notranslate nohighlight">
\[h\left( \mathbf{x}^{\mathbf{'}} \right) = \arg_{y}{\min{\{ d\left( \mathbf{x}^{\mathbf{'}}\mathbf{,\ x} \right)|\left( \mathbf{x},y \right) \in TR\}}}\]</div>
<p>This algorithm is referred to as the <strong>nearest neighbor algorithm (or
1-NN as we will see in its generalization in a moment)</strong>. In practice,
given a test example <span class="math notranslate nohighlight">\(\mathbf{x}^{\mathbf{'}}\)</span>, the nearest neighbor
algorithm works in two steps:</p>
<ul class="simple">
<li><p>Find the element <span class="math notranslate nohighlight">\(\overline{\mathbf{x}}\)</span> of the training set with
the smallest distance with <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span> (i.e., such that
<span class="math notranslate nohighlight">\(d(\overline{\mathbf{x}},\mathbf{x}^{\mathbf{'}})\)</span> is minimum). The
element <span class="math notranslate nohighlight">\(\overline{\mathbf{x}}\)</span> is called the nearest neighbor of
<span class="math notranslate nohighlight">\(\mathbf{x}^{'}\)</span>.</p></li>
<li><p>Return the ground truth label associated to <span class="math notranslate nohighlight">\(\overline{\mathbf{x}}\)</span>,
i.e., <span class="math notranslate nohighlight">\(y\ |\ \left( \overline{\mathbf{x}},y \right) \in TR\)</span>.</p></li>
</ul>
<p>In the example above, the test observation will be assigned the “spam”
class, as shown in the following figure:</p>
<p><img alt="" src="../_images/knn2.png" /></p>
</section>
<section id="the-k-nearest-neighbour-classification-algorithm">
<h3>The K-Nearest Neighbour Classification Algorithm<a class="headerlink" href="#the-k-nearest-neighbour-classification-algorithm" title="Permalink to this heading">#</a></h3>
<p>The Nearest Neighbor (or 1-NN) algorithm assumes that data points of the
same class are close to each other in the representation space. This can
be reasonably true when the representation space is ideal for the
classification task and the data is <em>clean and simple enough</em>. For
instance, we expect similar documents to have similar word frequencies.</p>
<p>However, it is often common to have <strong>‘outliers’</strong> in the training data,
i.e., data points which do not closely follow the distribution of the
other data points. This can be due to different factors:</p>
<ul class="simple">
<li><p><em>The data may not be clean</em>: maybe an email has been wrongly
classified as “spam” when it’s actually not spam;</p></li>
<li><p><em>The data representation may not be ideal</em>: there could be
legitimate email in which the word “viagra” is used and there are
many orthographical errors. <em>Think of a legitimate email forwarding
a spam email</em>. Our simple representation does not account for that,
which leads to outliers.</p></li>
</ul>
<p>Let us consider as an outlier a <strong>legitimate e-mail containing the word
‘viagra’</strong>. This example can be seen graphically as follows:</p>
<p><img alt="" src="../_images/knn3.png" /></p>
<p>Let us now assume that we are presented with a test example which is
shown as a red cross in the following figure:</p>
<p><img alt="" src="../_images/knn4.png" /></p>
<p>In the example above, the nearest neighbor algorithm would classify the
test example (the red cross) as “non spam” since the closest point is
the green outlier, while it is clear that the example is most probably a
“spam” e-mail. Indeed, while the closest example is “non spam”, all
other examples nearby belong to the “spam” class.</p>
<p>Reasonably, in cases like this, <strong>we should not look just at the closest
point in space</strong>, but instead, we should look at a <strong>neighborhood</strong> of
the data point. Consider the following example:</p>
<p><img alt="" src="../_images/knn5.png" /></p>
<p>If we look at a sufficiently large neighborhood, we find that most of
the points in the neighborhood are actually spam! Hence, it is wiser to
classify the data point as belonging to the spam class, rather than to
the non-spam one.</p>
<p>In practice, setting an appropriate radius for the neighborhood is not
easy. For instance, if the space is not uniformly dense (and usually it
is not – as in the example above!), a given radius could lead to
neighborhoods containing different numbers of elements. Indeed, in some
cases, they may even include just zero elements. Hence, rather than
considering a neighborhood of a given radius, we consider
neighborhoods of the point containing <strong>at most <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> elements</strong>, where <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> <strong>is a <em>hyper-parameter</em> of the
algorithm</strong>.</p>
<p>Similarly to what we have defined in the case of density estimation, given a point <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span>, we will define the neighborhood of
training points of size <span class="math notranslate nohighlight">\(K\)</span> centered at <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[N_K(\mathbf{x'}) = N(\mathbf{x'},R_K(\mathbf{x'}))\]</div>
<p>where <span class="math notranslate nohighlight">\(N(x,r)\)</span> denotes a neighborhood centered at <span class="math notranslate nohighlight">\(x\)</span> and with radius <span class="math notranslate nohighlight">\(r\)</span>, and:</p>
<div class="math notranslate nohighlight">
\[R_K(\mathbf{x'}) = \sup \{r : |N(\mathbf{x'},r) \setminus \{\mathbf{x'}\}| \leq K\}\]</div>
<p>Finally, we define the K-Nearest Neighbor Classification Algorithm (also
called K-NN) as follows:</p>
<div class="math notranslate nohighlight">
\[h\left( \mathbf{x'} \right) = mode\{ y|\left( \mathbf{x},y \right) \in N\left( \mathbf{x'};TR,\ K \right)\}\]</div>
<p>Where <span class="math notranslate nohighlight">\(mode\)</span> is the “statistical mode” function, which returns the
<strong>most frequent element of a set</strong>.</p>
<p><span class="math notranslate nohighlight">\(\mathbf{K}\)</span> <strong>is in practice an hyperparameter of the algorithm</strong>. It
can be set to some arbitrary value or optimized using a <strong>validation set
or cross-validation</strong>.</p>
<p>We should note that this definition <strong>generalizes the nearest neighbor
algorithm defined before</strong>. Indeed, a 1-NN is exactly the nearest
neighbor classifier seen above.</p>
</section>
</section>
<section id="the-curse-of-dimensionality">
<h2>The Curse of Dimensionality<a class="headerlink" href="#the-curse-of-dimensionality" title="Permalink to this heading">#</a></h2>
<p>The K-Nearest Neighbors algorithm is simple, intuitive, and powerful. However, it suffers from a famous and critical problem known as the <strong>“Curse of Dimensionality.”</strong></p>
<p>This “curse” refers to a set of problems that arise when your data has a very large number of features (or “dimensions”).</p>
<p>The core idea is: <strong>As dimensions increase, your data becomes incredibly sparse, and the concept of “distance” becomes meaningless.</strong></p>
<section id="the-empty-space-problem-2d-vs-3d">
<h3>The “Empty Space” Problem (2D vs. 3D)<a class="headerlink" href="#the-empty-space-problem-2d-vs-3d" title="Permalink to this heading">#</a></h3>
<p>Let’s visualize how the same number of data points become increasingly sparse as we add dimensions. We’ll generate 50 random data points and plot them in 1, 2, and 3 dimensions. Imagine these points are uniformly distributed within a unit hypercube (from 0 to 1 on each axis).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span> <span class="c1"># Required for 3D plots</span>

<span class="n">n_points</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># Let&#39;s use 50 points for better visibility</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># --- 1 Dimension ---</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">data_1d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># 50 points, 1 feature</span>
<span class="n">y_1d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_points</span><span class="p">)</span> <span class="c1"># For plotting on a line</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_1d</span><span class="p">,</span> <span class="n">y_1d</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;1D Space with </span><span class="si">{</span><span class="n">n_points</span><span class="si">}</span><span class="s1"> Points&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span> <span class="c1"># Hide y-axis ticks for a line plot</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>


<span class="c1"># --- 2 Dimensions ---</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">data_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 50 points, 2 features</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_2d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_2d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;2D Space with </span><span class="si">{</span><span class="n">n_points</span><span class="si">}</span><span class="s1"> Points&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>


<span class="c1"># --- 3 Dimensions ---</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">data_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 50 points, 3 features</span>

<span class="n">ax3</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_3d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_3d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">data_3d</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;3D Space with </span><span class="si">{</span><span class="n">n_points</span><span class="si">}</span><span class="s1"> Points&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Feature 3&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a41fdf1d13bec63a73a4d70d036509adaf89a45772c8080458669d051857bbc2.png" src="../_images/a41fdf1d13bec63a73a4d70d036509adaf89a45772c8080458669d051857bbc2.png" />
</div>
</div>
<p>Interpretation of the Plots:</p>
<ul class="simple">
<li><p>1D: The 50 points are quite crowded on the line. You can easily find very close neighbors.</p></li>
<li><p>2D: The same 50 points are now spread out over a square. The space feels much emptier, and points are farther apart.</p></li>
<li><p>3D: In a cube, the 50 points are truly sparse. It’s hard to even see individual points, and the space between them is vast.</p></li>
</ul>
<p>Now, imagine doing this for dozens, hundreds, or even thousands of dimensions. The space becomes astronomically large, and your few data points are like individual dust particles lost in an empty galaxy.</p>
</section>
<section id="the-impact-on-knn">
<h3>The Impact on KNN<a class="headerlink" href="#the-impact-on-knn" title="Permalink to this heading">#</a></h3>
<p>This creates two massive problems for KNN, which relies <em>entirely</em> on the idea of “nearness”:</p>
<ol class="arabic simple">
<li><p><strong>Sparsity:</strong> In high dimensions, your “nearest neighbor” might still be incredibly far away. Is it <em>really</em> “similar” to you if it’s so far? Its “vote” is probably not reliable.</p></li>
<li><p><strong>Distance Becomes Meaningless:</strong> This is the more critical point. In high-dimensional space, a strange mathematical property emerges: the distance to your <em>nearest</em> point and the distance to your <em>farthest</em> point become almost the same.</p></li>
</ol>
<p>When all points are “equally far away” from each other, the concept of a “nearest neighbor” becomes random and meaningless. The algorithm’s foundation breaks down, and its predictions become unstable.</p>
<p>The plot below illustrates this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">pdist</span>

<span class="c1"># 1. Define our parameters</span>
<span class="n">n_points</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1"># We will test a range of dimensions, from 2 up to 1000</span>
<span class="n">dimensions_to_test</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5001</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> 

<span class="c1"># Store the results</span>
<span class="n">min_distances</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">max_distances</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">relative_spread</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 2. Run the simulation</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dimensions_to_test</span><span class="p">:</span>
    <span class="c1"># Generate random points in a d-dimensional unit hypercube</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    
    <span class="c1"># Calculate all pairwise distances</span>
    <span class="n">all_distances</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
    
    <span class="c1"># Find the min and max</span>
    <span class="n">min_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">all_distances</span><span class="p">)</span>
    <span class="n">max_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">all_distances</span><span class="p">)</span>
    
    <span class="c1"># Store the results</span>
    <span class="n">min_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">min_d</span><span class="p">)</span>
    <span class="n">max_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">max_d</span><span class="p">)</span>
    <span class="c1"># This is the key metric: how &quot;different&quot; is max from min, relative to max?</span>
    <span class="n">relative_spread</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">max_d</span> <span class="o">-</span> <span class="n">min_d</span><span class="p">)</span> <span class="o">/</span> <span class="n">max_d</span><span class="p">)</span>

<span class="c1"># --- 3. Plot the Results ---</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Illustration of the Curse of Dimensionality&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="c1"># Plot 1: The actual min and max distances</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dimensions_to_test</span><span class="p">,</span> <span class="n">min_distances</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Minimum Distance&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dimensions_to_test</span><span class="p">,</span> <span class="n">max_distances</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Maximum Distance&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Dimensions (d)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Euclidean Distance&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distance between 100 Random Points&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Plot 2: The relative spread</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dimensions_to_test</span><span class="p">,</span> <span class="n">relative_spread</span><span class="p">,</span> <span class="s1">&#39;g-&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Dimensions (d)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Relative Spread: (Max - Min) / Max&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Collapse of &quot;Neighborhood&quot;&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Force y-axis to be 0 to 1 (or 100%)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;curse_of_dimensionality_collapse.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/13d0ee467f809b5923cd6a88d5c37867b5ec3b158fceb7c71cccc7a0cdb7d54d.png" src="../_images/13d0ee467f809b5923cd6a88d5c37867b5ec3b158fceb7c71cccc7a0cdb7d54d.png" />
</div>
</div>
<p>The plot on the left shows how both the minimum and maximum distance increase with the number of dimensions. The plot on the right computes the average relative spread between the maximum and minimum distance for each point. The realtive spread is computed as:</p>
<div class="math notranslate nohighlight">
\[\text{Relative Spread} = \frac{\text{max} - \text{min}}{\text{max}}\]</div>
<p>The plot shows that relative spread collapses to almost zero in high dimensions.</p>
</section>
<section id="takeways">
<h3>Takeways<a class="headerlink" href="#takeways" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>KNN works extremely well for <strong>low-dimensional data</strong> (e.g., our 2-feature or 4-feature Iris example).</p></li>
<li><p>It is almost always a <strong>poor choice</strong> for <strong>high-dimensional data</strong> (like images, which can have 10,000+ features, or text data with 5,000+ features) unless you use dimensionality reduction first (like PCA, as we will see later).</p></li>
</ul>
</section>
</section>
<section id="a-deeper-look-knn-as-a-discriminative-classifier">
<h2>A Deeper Look: KNN as a Discriminative Classifier<a class="headerlink" href="#a-deeper-look-knn-as-a-discriminative-classifier" title="Permalink to this heading">#</a></h2>
<p>This is a good time to introduce a fundamental concept in classification. Classifiers are generally split into two main types: <strong>Generative</strong> and <strong>Discriminative</strong>.</p>
<section id="generative-classifiers">
<h3>Generative Classifiers<a class="headerlink" href="#generative-classifiers" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What they do:</strong> A generative model learns the underlying probability distribution of <em>each class</em>. It builds a “story” for what each class looks like (e.g., “What’s the average and standard deviation of <code class="docutils literal notranslate"><span class="pre">sepal</span> <span class="pre">length</span></code> for a <code class="docutils literal notranslate"><span class="pre">setosa</span></code>?”).</p></li>
<li><p><strong>The Question it Answers:</strong> “What does <em>Class A</em> look like?”</p></li>
<li><p><strong>How it Classifies:</strong> It uses this knowledge and Bayes’ theorem to calculate <span class="math notranslate nohighlight">\(P(y|X)\)</span>—the probability of a class <span class="math notranslate nohighlight">\(y\)</span> given a new <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><strong>Example:</strong> Naive Bayes is the classic generative model.</p></li>
</ul>
</section>
<section id="discriminative-classifiers">
<h3>Discriminative Classifiers<a class="headerlink" href="#discriminative-classifiers" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What they do:</strong> A discriminative model <em>ignores</em> the underlying distribution of each class. It doesn’t care what “Class A” looks like. It <em>only</em> cares about finding the <strong>decision boundary</strong> that separates Class A from Class B.</p></li>
<li><p><strong>The Question it Answers:</strong> “What is the <em>difference</em> between Class A and Class B?”</p></li>
<li><p><strong>How it Classifies:</strong> It directly learns the function <span class="math notranslate nohighlight">\(h(x)\)</span> or the boundary <span class="math notranslate nohighlight">\(P(y|X)\)</span>.</p></li>
<li><p><strong>Example:</strong> <strong>K-Nearest Neighbors is a classic discriminative model.</strong></p></li>
</ul>
<p>The decision boundary plots we saw earlier are the <em>perfect</em> illustration of this. KNN is literally <em>all about</em> the boundary. It carves up the feature space into regions based on the training data, and it doesn’t spend <em>any</em> time modeling the probability distribution of the <code class="docutils literal notranslate"><span class="pre">setosa</span></code> class itself.</p>
<p>Logistic Regression, which we will see later, is another famous discriminative classifier.</p>
</section>
<section id="knn-example-the-fisher-iris-dataset">
<h3>KNN Example: The Fisher Iris Dataset<a class="headerlink" href="#knn-example-the-fisher-iris-dataset" title="Permalink to this heading">#</a></h3>
<p>To understand K-Nearest Neighbors, we will use a classic, famous dataset: the <strong>Fisher Iris Dataset</strong>. This is the “hello, world” of machine learning classification.</p>
<section id="fisher-s-iris-dataset">
<h4>Fisher’s Iris Dataset<a class="headerlink" href="#fisher-s-iris-dataset" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Origin:</strong> Introduced by the statistician and biologist Ronald Fisher in 1936.</p></li>
<li><p><strong>Goal:</strong> To classify Iris flowers into one of three species (the target variable) based on four physical measurements (the features).</p></li>
<li><p><strong>Data Structure:</strong></p>
<ul>
<li><p><strong>Target (<span class="math notranslate nohighlight">\(Y\)</span>):</strong> 3 classes (species)</p>
<ol class="arabic simple">
<li><p><em>Iris setosa</em></p></li>
<li><p><em>Iris versicolor</em></p></li>
<li><p><em>Iris virginica</em></p></li>
</ol>
</li>
<li><p><strong>Features (<span class="math notranslate nohighlight">\(X\)</span>):</strong> 4 continuous measurements</p>
<ol class="arabic simple">
<li><p>Sepal Length (in cm)</p></li>
<li><p>Sepal Width (in cm)</p></li>
<li><p>Petal Length (in cm)</p></li>
<li><p>Petal Width (in cm)</p></li>
</ol>
</li>
<li><p><strong>Data Size:</strong> 150 total samples, perfectly balanced with 50 samples for each of the three species.</p></li>
</ul>
</li>
</ul>
<p>The figure below depicts the three categories of flowers:</p>
<p><img alt="" src="../_images/iris.png" /></p>
<p>The following one illustrates the features:</p>
<p><img alt="" src="../_images/sepal_petal.png" /></p>
<p>The Iris dataset is famous because it perfectly illustrates the main challenge of classification.</p>
<ul class="simple">
<li><p>One species, <em>Iris setosa</em>, is <strong>linearly separable</strong> from the other two (it’s very easy to classify).</p></li>
<li><p>The other two species, <em>Iris versicolor</em> and <em>Iris virginica</em>, <strong>are <em>not</em> linearly separable</strong>. They overlap, meaning there is no simple, straight line that can perfectly separate them.</p></li>
</ul>
<p>This overlap is what makes the problem interesting and is a perfect scenario to test a flexible, non-linear classifier like K-Nearest Neighbors (KNN). Our goal will be to train a KNN model that can learn the “boundaries” between these species based on their petal and sepal measurements.</p>
</section>
<section id="loading-and-exploring-the-data">
<h4>Loading and Exploring the Data<a class="headerlink" href="#loading-and-exploring-the-data" title="Permalink to this heading">#</a></h4>
<p>First, we’ll load the dataset. It’s included directly in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. We will load it and convert it into a <code class="docutils literal notranslate"><span class="pre">pandas</span></code> DataFrame to make it easier to work with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="c1"># Load the dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Create a DataFrame</span>
<span class="c1"># We use iris.feature_names as the column headers</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>

<span class="c1"># Add the target (species) to the DataFrame</span>
<span class="c1"># iris.target is numeric (0, 1, 2)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Add a column for the actual species names for easier plotting</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;setosa&#39;</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;versicolor&#39;</span><span class="p">,</span>
    <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;virginica&#39;</span>
<span class="p">})</span>

<span class="c1"># Display the first 5 rows</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- Data Head ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Check the data types</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Data Info ---&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>

<span class="c1"># Check the class balance</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Class Balance ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Data Head ---
   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \
0                5.1               3.5                1.4               0.2   
1                4.9               3.0                1.4               0.2   
2                4.7               3.2                1.3               0.2   
3                4.6               3.1                1.5               0.2   
4                5.0               3.6                1.4               0.2   

   target species  
0       0  setosa  
1       0  setosa  
2       0  setosa  
3       0  setosa  
4       0  setosa  

--- Data Info ---
&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 150 entries, 0 to 149
Data columns (total 6 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   sepal length (cm)  150 non-null    float64
 1   sepal width (cm)   150 non-null    float64
 2   petal length (cm)  150 non-null    float64
 3   petal width (cm)   150 non-null    float64
 4   target             150 non-null    int64  
 5   species            150 non-null    object 
dtypes: float64(4), int64(1), object(1)
memory usage: 7.2+ KB

--- Class Balance ---
species
setosa        50
versicolor    50
virginica     50
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
</section>
<section id="preparing-for-modeling">
<h4>Preparing for Modeling<a class="headerlink" href="#preparing-for-modeling" title="Permalink to this heading">#</a></h4>
<p>Before we can train any model, we must perform two critical steps:</p>
<ol class="arabic simple">
<li><p><strong>Train/Test Split:</strong> This is the <strong>Golden Rule of Machine Learning</strong>. We will split our 150 samples into two groups: a <code class="docutils literal notranslate"><span class="pre">training</span> <span class="pre">set</span></code> (which the model learns from) and a <code class="docutils literal notranslate"><span class="pre">test</span> <span class="pre">set</span></code> (which we hold back to get an honest, unbiased score).</p></li>
<li><p><strong>Feature Scaling:</strong> KNN is a <strong>distance-based algorithm</strong>. It finds the “nearest” neighbors by calculating Euclidean distance. A feature like <code class="docutils literal notranslate"><span class="pre">sepal</span> <span class="pre">length</span></code> (range 4.3-7.9) would have a much larger influence on this distance than <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">width</span></code> (range 0.1-2.5). To prevent this, we must <strong>standardize</strong> our features (e.g., scale them to have a mean of 0 and a standard deviation of 1) so they are all on a level playing field.</p></li>
</ol>
<p>For this example, we will use only <strong>two features</strong>—<code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">length</span> <span class="pre">(cm)</span></code> and <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">width</span> <span class="pre">(cm)</span></code>—because they are highly predictive and allow us to easily visualize our model’s decisions in 2D later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># 1. Define our features (X) and target (y)</span>
<span class="c1"># We select only the two petal features for this example</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;petal length (cm)&#39;</span><span class="p">,</span> <span class="s1">&#39;petal width (cm)&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>

<span class="c1"># 2. Create the Train/Test Split</span>
<span class="c1"># test_size=0.3 means 30% of the data is held back for testing.</span>
<span class="c1"># random_state=42 ensures our split is reproducible.</span>
<span class="c1"># stratify=y ensures that the proportion of each class (setosa, etc.)</span>
<span class="c1"># is the same in both the train and test sets. This is a best practice.</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training samples: </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test samples:     </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 3. Scale the Features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># We .fit() the scaler ONLY on the training data</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># We .transform() the test data using the *same* scaler</span>
<span class="c1"># (We don&#39;t fit on the test data, as that would be &quot;cheating&quot;)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training samples: 105
Test samples:     45
</pre></div>
</div>
</div>
</div>
<p>Let’s visualize the data, both training and test:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;target_name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>

<span class="c1"># Select features and target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;petal length (cm)&#39;</span><span class="p">,</span> <span class="s1">&#39;petal width (cm)&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;target_name&#39;</span><span class="p">]</span>

<span class="c1"># Train/test split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Scale features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Prepare dataframes for plotting</span>
<span class="n">data_training</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="s1">&#39;Y&#39;</span><span class="p">])</span>
<span class="n">data_training</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span>

<span class="n">data_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="s1">&#39;Y&#39;</span><span class="p">])</span>
<span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Plotting function</span>
<span class="k">def</span> <span class="nf">plot2d</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">):</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
        <span class="n">subset</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">c</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subset</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">subset</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">marker</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="n">c</span> <span class="o">+</span> <span class="n">label_suffix</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Create plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plot2d</span><span class="p">(</span><span class="n">data_training</span><span class="p">,</span> <span class="s1">&#39; training&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># Reset color cycle</span>
<span class="n">plot2d</span><span class="p">(</span><span class="n">data_test</span><span class="p">,</span> <span class="s1">&#39; test&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;2D Scatter Plot of Iris Data (Petal Features)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Scaled Petal Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Scaled Petal Width&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/52664a30bdfdf986eb1e5e93ebe13260b730258c23fab71c7ce216969dd25635.png" src="../_images/52664a30bdfdf986eb1e5e93ebe13260b730258c23fab71c7ce216969dd25635.png" />
</div>
</div>
</section>
<section id="training-the-knn-model">
<h4>Training the KNN Model<a class="headerlink" href="#training-the-knn-model" title="Permalink to this heading">#</a></h4>
<p>Now we can train our model. The KNN algorithm is very simple:</p>
<ul class="simple">
<li><p><strong>Training:</strong> The “training” step is just storing the (scaled) <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> data. That’s it.</p></li>
<li><p><strong>Predicting:</strong> When we get a new, unseen flower, the model calculates the distance to <em>all</em> points in its “memory” (the training data) and finds the <span class="math notranslate nohighlight">\(k\)</span> nearest ones. It then takes a “majority vote” of those <span class="math notranslate nohighlight">\(k\)</span> neighbors to decide the species.</p></li>
</ul>
<p>The most important choice we have to make is the <strong>hyperparameter <span class="math notranslate nohighlight">\(k\)</span></strong> (the number of neighbors).</p>
<ul class="simple">
<li><p>A <strong>small <span class="math notranslate nohighlight">\(k\)</span></strong> (e.g., <span class="math notranslate nohighlight">\(k=1\)</span>) creates a “nervous” model that is very sensitive to noise. It has <strong>low bias</strong> but <strong>high variance</strong> (it overfits).</p></li>
<li><p>A <strong>large <span class="math notranslate nohighlight">\(k\)</span></strong> (e.g., <span class="math notranslate nohighlight">\(k=50\)</span>) creates a “smooth” model that ignores local patterns. It has <strong>high bias</strong> but <strong>low variance</strong> (it underfits).</p></li>
</ul>
<p>We will start with a common, balanced choice: <strong><span class="math notranslate nohighlight">\(k=5\)</span></strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Choose k</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># 2. Create the classifier</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

<span class="c1"># 3. &quot;Train&quot; the model (i.e., let it memorize the scaled training data)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KNN model trained with k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KNN model trained with k=5
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluating-the-model">
<h4>Evaluating the Model<a class="headerlink" href="#evaluating-the-model" title="Permalink to this heading">#</a></h4>
<p>Now for the moment of truth. We will use our trained model to make predictions on the <code class="docutils literal notranslate"><span class="pre">X_test_scaled</span></code> data, which it has <em>never seen before</em>. We will then compare its predictions (<code class="docutils literal notranslate"><span class="pre">y_pred</span></code>) to the <em>true</em> labels (<code class="docutils literal notranslate"><span class="pre">y_test</span></code>).</p>
<p>We will use three key metrics:</p>
<ol class="arabic simple">
<li><p><strong>Accuracy:</strong> The simplest metric. What percentage of predictions did the model get right?</p></li>
<li><p><strong>Confusion Matrix:</strong> The most important diagnostic. It shows us <em>where</em> the model got confused.</p></li>
<li><p><strong>Classification Report:</strong> A full summary of Precision, Recall, and F1-Score for each class.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Make predictions on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># 2. Calculate Accuracy</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model Accuracy (k=5): </span><span class="si">{</span><span class="n">acc</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="c1"># 3. Calculate and print the Confusion Matrix</span>
<span class="c1"># Rows are the &quot;True&quot; class, Columns are the &quot;Predicted&quot; class</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Confusion Matrix ---&quot;</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span>

<span class="c1"># 4. Print the Classification Report</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Classification Report ---&quot;</span><span class="p">)</span>
<span class="n">report</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model Accuracy (k=5): 91.11%

--- Confusion Matrix ---
[[15  0  0]
 [ 0 14  1]
 [ 0  3 12]]

--- Classification Report ---
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        15
  versicolor       0.82      0.93      0.88        15
   virginica       0.92      0.80      0.86        15

    accuracy                           0.91        45
   macro avg       0.92      0.91      0.91        45
weighted avg       0.92      0.91      0.91        45
</pre></div>
</div>
</div>
</div>
</section>
<section id="interpreting-the-results">
<h4>Interpreting the Results<a class="headerlink" href="#interpreting-the-results" title="Permalink to this heading">#</a></h4>
<p>Let’s analyze the output:</p>
<ul>
<li><p><strong>Accuracy:</strong> We achieved a very high accuracy! This is common on the Iris dataset, especially when using the petal features.</p></li>
<li><p><strong>Confusion Matrix:</strong> The matrix is the best way to see this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">15</span>  <span class="mi">0</span>  <span class="mi">0</span><span class="p">]</span>   <span class="o">&lt;-</span> <span class="p">(</span><span class="kc">True</span><span class="p">:</span> <span class="n">setosa</span><span class="p">)</span>
 <span class="p">[</span> <span class="mi">0</span> <span class="mi">14</span>  <span class="mi">1</span><span class="p">]</span>   <span class="o">&lt;-</span> <span class="p">(</span><span class="kc">True</span><span class="p">:</span> <span class="n">versicolor</span><span class="p">)</span>
 <span class="p">[</span> <span class="mi">0</span>  <span class="mi">3</span> <span class="mi">12</span><span class="p">]]</span>  <span class="o">&lt;-</span> <span class="p">(</span><span class="kc">True</span><span class="p">:</span> <span class="n">virginica</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The numbers on the diagonal (15, 14, 12) are our <strong>correct</strong> predictions.</p></li>
<li><p>The numbers <em>off</em> the diagonal are our <strong>errors</strong>. <span class="math notranslate nohighlight">\(1\)</span> versicolor has been classified as virginica, while <span class="math notranslate nohighlight">\(2\)</span> virginica have been classified as versicolor.</p></li>
<li><p>It correctly identified all 15 <code class="docutils literal notranslate"><span class="pre">setosa</span></code>, 14 <code class="docutils literal notranslate"><span class="pre">versicolor</span></code>, and 12 <code class="docutils literal notranslate"><span class="pre">virginica</span></code> samples in the test set.</p></li>
</ul>
</li>
<li><p><strong>Classification Report:</strong></p>
<ul class="simple">
<li><p>This confirms the perfect score. The <strong>precision</strong> (of the predictions) and <strong>recall</strong> (of the true classes) are very high for the three species.</p></li>
</ul>
</li>
</ul>
<p>This shows that for this dataset, a simple KNN (k=5) model using only two features is a good classifier.</p>
</section>
</section>
<section id="macro-vs-micro-averaging">
<h3>Macro vs. Micro Averaging<a class="headerlink" href="#macro-vs-micro-averaging" title="Permalink to this heading">#</a></h3>
<p>The report gives us scores for <code class="docutils literal notranslate"><span class="pre">setosa</span></code>, <code class="docutils literal notranslate"><span class="pre">versicolor</span></code>, and <code class="docutils literal notranslate"><span class="pre">virginica</span></code> individually. But how do we get a single, overall score for “Precision” or “Recall”? We have to average the per-class scores. There are three main ways to do this:</p>
<ol class="arabic simple">
<li><p><strong>Macro Averaging</strong></p>
<ul class="simple">
<li><p><strong>How:</strong> Calculates the metric (e.g., Precision) for each class <em>independently</em>, then computes the simple, unweighted average.</p></li>
<li><p><strong>Formula:</strong> <code class="docutils literal notranslate"><span class="pre">Macro</span> <span class="pre">Precision</span> <span class="pre">=</span> <span class="pre">(Precision_setosa</span> <span class="pre">+</span> <span class="pre">Precision_versicolor</span> <span class="pre">+</span> <span class="pre">Precision_virginica)</span> <span class="pre">/</span> <span class="pre">3</span></code></p></li>
<li><p><strong>What it means:</strong> Every <strong>class</strong> gets an equal vote, regardless of its size. Use this if you care about performance on rare classes.</p></li>
</ul>
</li>
<li><p><strong>Weighted Averaging</strong></p>
<ul class="simple">
<li><p><strong>How:</strong> Calculates the metric for each class, then computes a weighted average based on the number of true samples for each class (its “support”).</p></li>
<li><p><strong>Formula:</strong> <code class="docutils literal notranslate"><span class="pre">Weighted</span> <span class="pre">Precision</span> <span class="pre">=</span> <span class="pre">(Prec_setosa</span> <span class="pre">*</span> <span class="pre">15</span> <span class="pre">+</span> <span class="pre">Prec_versicolor</span> <span class="pre">*</span> <span class="pre">15</span> <span class="pre">+</span> <span class="pre">Prec_virginica</span> <span class="pre">*</span> <span class="pre">15)</span> <span class="pre">/</span> <span class="pre">(15</span> <span class="pre">+</span> <span class="pre">15</span> <span class="pre">+</span> <span class="pre">15)</span></code></p></li>
<li><p><strong>What it means:</strong> Every class’s score is weighted by its <em>size</em>. This is a good, “fair” average for imbalanced datasets.</p></li>
</ul>
</li>
<li><p><strong>Micro Averaging</strong></p>
<ul class="simple">
<li><p><strong>How:</strong> Sums up <em>all individual</em> True Positives, False Positives, and False Negatives from <em>all classes</em> first, then computes one global score.</p></li>
<li><p><strong>Formula:</strong> <code class="docutils literal notranslate"><span class="pre">Micro</span> <span class="pre">Precision</span> <span class="pre">=</span> <span class="pre">(TP_set</span> <span class="pre">+</span> <span class="pre">TP_ver</span> <span class="pre">+</span> <span class="pre">TP_vir)</span> <span class="pre">/</span> <span class="pre">(TP_set</span> <span class="pre">+</span> <span class="pre">TP_ver</span> <span class="pre">+</span> <span class="pre">TP_vir</span> <span class="pre">+</span> <span class="pre">FP_set</span> <span class="pre">+</span> <span class="pre">FP_ver</span> <span class="pre">+</span> <span class="pre">FP_vir)</span></code></p></li>
<li><p><strong>What it means:</strong> Every <strong>sample</strong> gets an equal vote. In multi-class classification, this number will always be <strong>exactly the same as the overall Accuracy</strong>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="classification-map-decision-boundary-of-k-nn-and-importance-of-parameter-k">
<h3>Classification Map/Decision Boundary Of K-NN and Importance of Parameter K<a class="headerlink" href="#classification-map-decision-boundary-of-k-nn-and-importance-of-parameter-k" title="Permalink to this heading">#</a></h3>
<p>A classifier <span class="math notranslate nohighlight">\(f\)</span> assigns a class to each input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Since
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> can be seen as a geometrical point in the <span class="math notranslate nohighlight">\(n\)</span>-dimensional
space, it is generally interesting to see how the classification
function <span class="math notranslate nohighlight">\(f\)</span> works on a portion of the representation space. This is
done by generating a <strong>classification map</strong> or <strong>decision boundary</strong>,
which is obtained computing the label that the classifier would assign
to a dense grid of data points.</p>
<p>This is the decision boundary with <span class="math notranslate nohighlight">\(K=1\)</span> in our case:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="c1"># Load Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Use petal length and petal width (features 2 and 3)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Train KNN classifier</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Create meshgrid for decision boundary</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">500</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Define color maps</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FFAAAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAFFAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAAAFF&#39;</span><span class="p">])</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00DD00&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">])</span>

<span class="c1"># Plot decision boundaries</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>

<span class="c1"># Plot training points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#00DD00&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#0000FF&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="c1"># Label axes and finalize plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Petal length (cm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Petal width (cm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;KNN Decision Boundaries (Petal Features)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d59a14f187907d497d29e15dc23ae3cb46c781d2d538ace107788a614aa59a41.png" src="../_images/d59a14f187907d497d29e15dc23ae3cb46c781d2d538ace107788a614aa59a41.png" />
</div>
</div>
<p>In the plot above, points of different colors represent the data points
belonging to the three classes of the dataset, while regions of different colors represent how that point in space would be classified by the model.</p>
<p><span class="math notranslate nohighlight">\(K\)</span> controls how the space is partitioned (bias-variance tradeoff).</p>
<p>Examples of classification maps for a 1-NN, a 5-NN, a 10-NN, and a 20-NN classifiers are shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="c1"># Load Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>  <span class="c1"># Petal length and petal width</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Define color maps</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FFAAAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAFFAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAAAFF&#39;</span><span class="p">])</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00DD00&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">])</span>

<span class="c1"># Create meshgrid</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">500</span><span class="p">))</span>

<span class="c1"># Loop through different k values</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]:</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">shading</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>

    <span class="c1"># Plot training points</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#00DD00&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#0000FF&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Petal length (cm)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Petal width (cm)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;K=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/homebrew/anaconda3/lib/python3.13/site-packages/IPython/core/pylabtools.py:170: UserWarning: Creating legend with loc=&quot;best&quot; can be slow with large amounts of data.
  fig.canvas.print_figure(bytes_io, **kw)
</pre></div>
</div>
<img alt="../_images/273113a70a0f3770557f1ae91c7e952d90b39c46108124f40f34fd821b2cef8b.png" src="../_images/273113a70a0f3770557f1ae91c7e952d90b39c46108124f40f34fd821b2cef8b.png" />
<img alt="../_images/ea7e021b246bd93715687734f3415b002867ebd474c70c890ea2ff25e3413740.png" src="../_images/ea7e021b246bd93715687734f3415b002867ebd474c70c890ea2ff25e3413740.png" />
<img alt="../_images/025ecaae7cecc1721f8c3b5635ea950cfefe6180d2d77d8ce3826445978786e6.png" src="../_images/025ecaae7cecc1721f8c3b5635ea950cfefe6180d2d77d8ce3826445978786e6.png" />
<img alt="../_images/ab82b202a9fea660a6637a7ada157aa3c18d982d54a61555b6e4034636f963ab.png" src="../_images/ab82b202a9fea660a6637a7ada157aa3c18d982d54a61555b6e4034636f963ab.png" />
</div>
</div>
<p>The number of neighbors changes the decision map:</p>
<ul class="simple">
<li><p>For small values, the algorithm tends to over-segment the space and
creates very small regions for isolated training data-points.</p></li>
<li><p>For larger values, the regions tend to be smoother and isolated data
points are ignored.</p></li>
<li><p>Choosing <strong>a larger K</strong> <strong>can</strong> reduce overfitting (indeed the
isolated data-points can be seen as outliers).</p></li>
<li><p>However, choosing a <strong>too large K</strong> can encourage underfitting, by
completing ignoring some of the decision regions.</p></li>
<li><p>In particular, setting <strong>K to the size of the training set</strong>, any
data point is classified with the most numerous class.</p></li>
</ul>
</section>
<section id="finding-the-best-k">
<h3>Finding the best <span class="math notranslate nohighlight">\(K\)</span><a class="headerlink" href="#finding-the-best-k" title="Permalink to this heading">#</a></h3>
<p>We saw that <span class="math notranslate nohighlight">\(K\)</span> affects the quality of our classifier, but how to choose the appropriate one? We will see how to do it with cross-validation.</p>
<section id="step-1-prepare-data">
<h4>Step 1: Prepare Data<a class="headerlink" href="#step-1-prepare-data" title="Permalink to this heading">#</a></h4>
<p>For this, we’ll use all 4 features to make the problem more robust. We’ll do a single 70/30 split and then lock the test set away.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># 1. Load data</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># 2. Create the one-and-only Train/Test Split</span>
<span class="c1"># We&#39;ll call it X_train_full, y_train_full to be clear</span>
<span class="n">X_train_full</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># 3. Scale the data</span>
<span class="c1"># We fit the scaler ONLY on the full training set</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_full</span><span class="p">)</span>

<span class="c1"># We transform the test set using the *same* scaler</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-run-cross-validation-to-find-best-k">
<h4>Step 2: Run Cross-Validation to Find Best K<a class="headerlink" href="#step-2-run-cross-validation-to-find-best-k" title="Permalink to this heading">#</a></h4>
<p>Now we loop through our <span class="math notranslate nohighlight">\(K\)</span> values (e.g., 1 to 20). For each <span class="math notranslate nohighlight">\(K\)</span>, we will use <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> to automatically perform 5-fold cross-validation <em>only on the training data</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Define a range of K values to test</span>
<span class="n">k_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># This will store the *mean* accuracy for each k</span>

<span class="c1"># 2. Loop through each K</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="c1"># Create a new KNN model for this k</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Perform 5-fold cross-validation</span>
    <span class="c1"># We use X_train_scaled and y_train_full</span>
    <span class="c1"># &#39;cv=5&#39; means 5 folds</span>
    <span class="c1"># &#39;scoring=&#39;accuracy&#39;&#39; is the metric we want</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
    
    <span class="c1"># Store the average accuracy from the 5 folds</span>
    <span class="n">cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># 3. Plot the Validation Curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">cv_scores</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;KNN Validation Curve (5-Fold Cross-Validation)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;K (Number of Neighbors)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean CV Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">k_range</span><span class="p">)</span>

<span class="c1"># Find and mark the best K</span>
<span class="n">best_k_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span>
<span class="n">best_k</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">best_k_index</span><span class="p">]</span>
<span class="n">best_acc</span> <span class="o">=</span> <span class="n">cv_scores</span><span class="p">[</span><span class="n">best_k_index</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">best_k</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Best K = </span><span class="si">{</span><span class="n">best_k</span><span class="si">}</span><span class="s1"> (Avg. Acc = </span><span class="si">{</span><span class="n">best_acc</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2d8ca77008908962e260e05abf130773b9eae9025df34047413893a93e8d1d8c.png" src="../_images/2d8ca77008908962e260e05abf130773b9eae9025df34047413893a93e8d1d8c.png" />
</div>
</div>
</section>
</section>
<section id="step-3-final-training-and-evaluation">
<h3>Step 3: Final Training and Evaluation<a class="headerlink" href="#step-3-final-training-and-evaluation" title="Permalink to this heading">#</a></h3>
<p>The plot shows us the bias-variance tradeoff.</p>
<ul class="simple">
<li><p><strong>Low <span class="math notranslate nohighlight">\(K\)</span> (left):</strong> The accuracy is jumpy and lower. The model is overfitting to the <em>folds</em> (High Variance).</p></li>
<li><p><strong>High <span class="math notranslate nohighlight">\(K\)</span> (right):</strong> The accuracy starts to drop as the model becomes too “simple” (High Bias).</p></li>
<li><p><strong>The Peak:</strong> Our plot shows the “sweet spot” is around <span class="math notranslate nohighlight">\(K=10\)</span> or <span class="math notranslate nohighlight">\(K=14\)</span>. This is our chosen hyperparameter.</p></li>
</ul>
<p>Now, we perform our final two steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Get the best K we found from our CV</span>
<span class="n">best_k</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cross-validation complete. The best K is </span><span class="si">{</span><span class="n">best_k</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

<span class="c1"># 2. Train a *new* model on the *ENTIRE* training set</span>
<span class="n">final_knn_model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">best_k</span><span class="p">)</span>
<span class="n">final_knn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">)</span>

<span class="c1"># 3. Finally, use the &quot;locked away&quot; Test Set</span>
<span class="c1"># This is our one and only time.</span>
<span class="n">final_predictions</span> <span class="o">=</span> <span class="n">final_knn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># 4. Report the final, unbiased score</span>
<span class="n">final_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">final_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Final Model Evaluation ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final Test Set Accuracy (with K=</span><span class="si">{</span><span class="n">best_k</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">final_accuracy</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Final Classification Report ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">final_predictions</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross-validation complete. The best K is 14.

--- Final Model Evaluation ---
Final Test Set Accuracy (with K=14): 95.56%

--- Final Classification Report ---
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        15
  versicolor       0.88      1.00      0.94        15
   virginica       1.00      0.87      0.93        15

    accuracy                           0.96        45
   macro avg       0.96      0.96      0.96        45
weighted avg       0.96      0.96      0.96        45
</pre></div>
</div>
</div>
</div>
</section>
<section id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h3>
<p>This is the principled, robust machine learning workflow.</p>
<ol class="arabic simple">
<li><p>We split our data once.</p></li>
<li><p>We used the <strong>training data</strong> and <strong>cross-validation</strong> to find the best hyperparameter (<span class="math notranslate nohighlight">\(K\)</span>).</p></li>
<li><p>We trained a single, final model on <em>all</em> the training data.</p></li>
<li><p>We reported a final, honest accuracy by testing on the <strong>test data</strong>.</p></li>
</ol>
<p>The final classification report gives us the unbiased performance of our <em>tuned</em> model.</p>
</section>
</section>
<section id="k-nearest-neighbors-knn-for-regression">
<h2>K-Nearest Neighbors (KNN) for Regression<a class="headerlink" href="#k-nearest-neighbors-knn-for-regression" title="Permalink to this heading">#</a></h2>
<p>We’ve seen how KNN works for <strong>Classification</strong> (predicting a label) by taking a “majority vote” of the nearest neighbors.</p>
<p>The algorithm is extremely flexible and can also be used for <strong>Regression</strong> (predicting a continuous value). The logic is identical, with one small change in the final step:</p>
<ul class="simple">
<li><p><strong>KNN Classifier (Iris):</strong> Find the <span class="math notranslate nohighlight">\(k\)</span> nearest flowers <span class="math notranslate nohighlight">\(\to\)</span> Predict the <strong>most common species</strong> (the <em>mode</em>) of the neighbors.</p></li>
<li><p><strong>KNN Regressor (MPG):</strong> Find the <span class="math notranslate nohighlight">\(k\)</span> nearest cars <span class="math notranslate nohighlight">\(\to\)</span> Predict the <strong>average <code class="docutils literal notranslate"><span class="pre">mpg</span></code></strong> (the <em>mean</em>) of the neighbors.</p></li>
</ul>
<p>Let’s use our <code class="docutils literal notranslate"><span class="pre">mpg</span></code> dataset to build a <code class="docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code> that predicts <code class="docutils literal notranslate"><span class="pre">mpg</span></code> using <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Import the Regressor and other tools</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Load and clean the data</span>
<span class="n">mpg</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;mpg&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># 1. Define our features (X) and target (y)</span>
<span class="c1"># We use .values.reshape(-1, 1) to turn the pandas Series into a 2D numpy array</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mpg</span><span class="p">[[</span><span class="s1">&#39;horsepower&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;mpg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># 2. Create the Train/Test Split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 3. Scale the Features (CRITICAL for KNN)</span>
<span class="c1"># KNN is distance-based, so scaling is not optional.</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="visualizing-the-bias-variance-tradeoff-overfit-vs-underfit">
<h3>Visualizing the Bias-Variance Tradeoff (Overfit vs. Underfit)<a class="headerlink" href="#visualizing-the-bias-variance-tradeoff-overfit-vs-underfit" title="Permalink to this heading">#</a></h3>
<p>Just like in classification, the choice of <span class="math notranslate nohighlight">\(k\)</span> controls the bias-variance tradeoff. Let’s build two models to see this:</p>
<ol class="arabic simple">
<li><p><strong>A <span class="math notranslate nohighlight">\(k=3\)</span> model (Low Bias, High Variance):</strong> This model will be “nervous” and “wiggly,” as it only looks at its 3 closest neighbors. It will overfit to the training data.</p></li>
<li><p><strong>A <span class="math notranslate nohighlight">\(k=50\)</span> model (High Bias, Low Variance):</strong> This model will be very “smooth” and “simple,” as it averages the opinions of 50 neighbors. It will be stable but will underfit, missing the local patterns.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create the two models (using &#39;uniform&#39; weights)</span>
<span class="n">knn_overfit</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
<span class="n">knn_underfit</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">)</span>

<span class="c1"># 2. Fit both models on the scaled training data</span>
<span class="n">knn_overfit</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">knn_underfit</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 3. Create a smooth line of x-values for plotting</span>
<span class="n">X_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X_train_scaled</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">200</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># 4. Get the predictions from both models</span>
<span class="n">y_pred_overfit</span> <span class="o">=</span> <span class="n">knn_overfit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plot</span><span class="p">)</span>
<span class="n">y_pred_underfit</span> <span class="o">=</span> <span class="n">knn_underfit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plot</span><span class="p">)</span>

<span class="c1"># 5. Plot the results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test Data&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">y_pred_overfit</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prediction (k=3, High Variance)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">y_pred_underfit</span><span class="p">,</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prediction (k=50, High Bias)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;KNN Regression: The Effect of K&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Scaled Horsepower&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MPG&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8dfd4bd87abbc8b1ba3853a248e1d5d7bd7262b10fe085436c3b3cf404080439.png" src="../_images/8dfd4bd87abbc8b1ba3853a248e1d5d7bd7262b10fe085436c3b3cf404080439.png" />
</div>
</div>
</section>
<section id="interpretation-of-the-plot">
<h3>Interpretation of the Plot<a class="headerlink" href="#interpretation-of-the-plot" title="Permalink to this heading">#</a></h3>
<p>You can clearly see the bias-variance tradeoff in action:</p>
<ul class="simple">
<li><p>The <strong>red line (<span class="math notranslate nohighlight">\(k=3\)</span>)</strong> is “spiky” and “nervous.” It tries to follow the training data (blue dots) too closely. This is <strong>overfitting (High Variance)</strong>.</p></li>
<li><p>The <strong>green line (<span class="math notranslate nohighlight">\(k=50\)</span>)</strong> is very smooth, acting almost like a simple average. It misses the clear non-linear curve in the data. This is <strong>underfitting (High Bias)</strong>.</p></li>
</ul>
<p>Our goal is to find the “sweet spot” <span class="math notranslate nohighlight">\(k\)</span> in the middle.</p>
</section>
<section id="finding-the-optimal-k-with-cross-validation">
<h3>Finding the Optimal <span class="math notranslate nohighlight">\(K\)</span> with Cross-Validation<a class="headerlink" href="#finding-the-optimal-k-with-cross-validation" title="Permalink to this heading">#</a></h3>
<p>Just like in our classification example, we can use K-Fold Cross-Validation on the <strong>training set</strong> to find the best <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>The only difference is our <code class="docutils literal notranslate"><span class="pre">scoring</span></code> metric. Instead of <code class="docutils literal notranslate"><span class="pre">accuracy</span></code>, we use <code class="docutils literal notranslate"><span class="pre">neg_mean_squared_error</span></code> (scikit-learn uses <em>negative</em> MSE because its internal optimizer always tries to <em>maximize</em> a score).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Define a range of K values to test</span>
<span class="n">k_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">41</span><span class="p">)</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># This will store the *mean* RMSE for each k</span>

<span class="c1"># 2. Loop through each K</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
    
    <span class="c1"># Perform 5-fold cross-validation on the training data</span>
    <span class="c1"># We ask for &#39;neg_mean_squared_error&#39;</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
    
    <span class="c1"># We store the *positive* *root* of the MSE</span>
    <span class="n">cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

<span class="c1"># 3. Plot the Validation &quot;Elbow&quot; Curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">cv_scores</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;KNN Regression: Validation Curve (Elbow Plot)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;K (Number of Neighbors)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Average CV RMSE&#39;</span><span class="p">)</span> <span class="c1"># Lower is better!</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Find and mark the best K (lowest RMSE)</span>
<span class="n">best_k_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span>
<span class="n">best_k</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">best_k_index</span><span class="p">]</span>
<span class="n">best_rmse</span> <span class="o">=</span> <span class="n">cv_scores</span><span class="p">[</span><span class="n">best_k_index</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">best_k</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Best K = </span><span class="si">{</span><span class="n">best_k</span><span class="si">}</span><span class="s1"> (Avg. RMSE = </span><span class="si">{</span><span class="n">best_rmse</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/eff8f88d88a6288a9d8fa2a183ff2e3bd635832accdee91097cff0124b072dbc.png" src="../_images/eff8f88d88a6288a9d8fa2a183ff2e3bd635832accdee91097cff0124b072dbc.png" />
</div>
</div>
</section>
<section id="final-evaluation">
<h3>Final Evaluation<a class="headerlink" href="#final-evaluation" title="Permalink to this heading">#</a></h3>
<p>The validation plot shows us that the “sweet spot” (lowest error) is at <strong><span class="math notranslate nohighlight">\(k=25\)</span></strong>.</p>
<p>Now we can train our final, optimized model on the <em>entire</em> training set and get our final, unbiased score from the <strong>test set</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Train the *final* model with the best K</span>
<span class="n">final_knn_regressor</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">best_k</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
<span class="n">final_knn_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 2. Get final predictions on the (held-out) test set</span>
<span class="n">final_predictions</span> <span class="o">=</span> <span class="n">final_knn_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># 3. Calculate the final, honest RMSE</span>
<span class="n">final_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">final_predictions</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--- Final Model Evaluation ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The best K found by CV: </span><span class="si">{</span><span class="n">best_k</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final Test Set RMSE: </span><span class="si">{</span><span class="n">final_rmse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (mpg)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Final Model Evaluation ---
The best K found by CV: 25
Final Test Set RMSE: 4.2943 (mpg)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Evaluation Measures for Classifcation:
<a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">https://en.wikipedia.org/wiki/Precision_and_recall</a></p></li>
<li><p>Nearest Neighbor: Section 2.5.2 of [1]</p></li>
</ul>
<p>[1] Bishop, Christopher M. <em>Pattern recognition and machine learning</em>.
springer, 2006.
<a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10_beyond_linear_regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Beyond Linear Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="12_logistic_regression.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Logistic Regression - Statistical View</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-definition">Task Definition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-measures">Evaluation Measures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy">Accuracy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-rate">Error Rate</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-imbalanced-dataset">Example - Imbalanced Dataset</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-types">Error Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion Matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-spam-detector">Example 1 – Spam Detector</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-imbalanced-dataset">Example 2 – Imbalanced Dataset</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-and-recall">Precision and Recall</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-precision-vs-high-recall">High Precision vs High Recall</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-spam-detector">Example – Spam Detector</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#f-1-score"><span class="math notranslate nohighlight">\(F_{1}\)</span> Score</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Example - Spam Detector</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix-for-multi-class-classification">Confusion Matrix for Multi-Class Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-receiver-operating-characteristic-curve-and-area-under-the-curve-auc-measures">ROC (Receiver Operating Characteristic) Curve and Area Under the Curve (AUC) Measures</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-classifier-the-threshold-model">A Simple Classifier: The Threshold Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-roc-curve-auc">Interpreting the ROC Curve &amp; AUC</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-choose-an-optimal-threshold">How to Choose an Optimal Threshold</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#final-evaluation-at-the-optimal-threshold">Final Evaluation at the Optimal Threshold</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbor-classification">K-Nearest Neighbor Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-nearest-neighbor-or-1-nn-classification-algorithm">The Nearest Neighbor (or 1-NN) Classification Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-k-nearest-neighbour-classification-algorithm">The K-Nearest Neighbour Classification Algorithm</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-curse-of-dimensionality">The Curse of Dimensionality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-empty-space-problem-2d-vs-3d">The “Empty Space” Problem (2D vs. 3D)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-impact-on-knn">The Impact on KNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#takeways">Takeways</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deeper-look-knn-as-a-discriminative-classifier">A Deeper Look: KNN as a Discriminative Classifier</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-classifiers">Generative Classifiers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discriminative-classifiers">Discriminative Classifiers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#knn-example-the-fisher-iris-dataset">KNN Example: The Fisher Iris Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fisher-s-iris-dataset">Fisher’s Iris Dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-and-exploring-the-data">Loading and Exploring the Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-for-modeling">Preparing for Modeling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-knn-model">Training the KNN Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-model">Evaluating the Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-results">Interpreting the Results</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#macro-vs-micro-averaging">Macro vs. Micro Averaging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-map-decision-boundary-of-k-nn-and-importance-of-parameter-k">Classification Map/Decision Boundary Of K-NN and Importance of Parameter K</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-best-k">Finding the best <span class="math notranslate nohighlight">\(K\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-prepare-data">Step 1: Prepare Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-run-cross-validation-to-find-best-k">Step 2: Run Cross-Validation to Find Best K</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-final-training-and-evaluation">Step 3: Final Training and Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-knn-for-regression">K-Nearest Neighbors (KNN) for Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-bias-variance-tradeoff-overfit-vs-underfit">Visualizing the Bias-Variance Tradeoff (Overfit vs. Underfit)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-plot">Interpretation of the Plot</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-optimal-k-with-cross-validation">Finding the Optimal <span class="math notranslate nohighlight">\(K\)</span> with Cross-Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#final-evaluation">Final Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>