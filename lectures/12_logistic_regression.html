

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Logistic Regression - Statistical View &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/12_logistic_regression';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Multiclass Logistic Regression and Predictive View" href="13_multiclass_logistic_regression.html" />
    <link rel="prev" title="Classification Task, Evaluation Measures, and K-Nearest Neighbor" href="11_classification_knn.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamentals of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">Introduction to the Labs and Work Environment Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_python_crash_course.html">Python Crash Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_python_data_science_crash_course.html">Python for Data Science Crash Course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro_data_analysis.html">Data Analysis Key Concepts, Loading and Inspecting the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="02_describing_and_visualizing_the_data.html">Describing and Visualizing the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_probability_for_data_analysis.html">Probability for Data Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_association_between_variables.html">Association between variables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05_data_distributions.html">Data Distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_statistical_inference.html">Statistical Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="07_storytelling_with_data.html">Storytelling with Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_predictive_modeling.html">Introduction to Predictive Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_linear_regression.html">Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 11</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_beyond_linear_regression.html">Beyond Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 12</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_classification_knn.html">Classification Task, Evaluation Measures, and K-Nearest Neighbor</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 13</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Logistic Regression - Statistical View</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 14</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_multiclass_logistic_regression.html">Multiclass Logistic Regression and Predictive View</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 15</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="14_map_naive_bayes.html">Naive Bayes</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes2526/blob/master/lecturenotes/lectures/12_logistic_regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526/issues/new?title=Issue%20on%20page%20%2Flectures/12_logistic_regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/12_logistic_regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Logistic Regression - Statistical View</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-data">Example Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limits-of-linear-regression">Limits of Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-binary-values-to-probabilities">From Binary Values to Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-function">The Logistic Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-regression-model">The Logistic Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-logistic-regressor">Estimation of the Parameters of a Logistic Regressor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-interpretation-of-the-coefficients-of-a-linear-regressor">Statistical Interpretation of the Coefficients of a Linear Regressor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-intercept-beta-0">Interpretation of the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-variable-coefficients-beta-i">Interpretation of variable coefficients <span class="math notranslate nohighlight">\(\beta_i\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-logistic-regression">Example of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-logistic-regression-model">Evaluating the Logistic Regression Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-we-can-t-use-r-2">The Problem: We Can’t Use <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-pseudo-r-2">The Solution: “Pseudo <span class="math notranslate nohighlight">\(R^2\)</span>”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-evaluation">Predictive Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-in-python">Logistic Regression in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="logistic-regression-statistical-view">
<h1>Logistic Regression - Statistical View<a class="headerlink" href="#logistic-regression-statistical-view" title="Permalink to this heading">#</a></h1>
<p>We have seen how K-NN allows to perform classification in a <strong>non-parametric way</strong>. However, it has some short-comings:</p>
<ul class="simple">
<li><p>It doesn’t work with many features;</p></li>
<li><p>It requires memorization of the training set;</p></li>
<li><p>Each classification requires a search of the K-nearest neighbors in the training set.</p></li>
</ul>
<p>Compare this to a parametric predictive model such as the linear regression:</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{x}) = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n\]</div>
<p>Parametric models are fast and compact - once we train them, we don’t need to save the training data and they can be used to classify new examples in a very natural way.</p>
<p>While linear regression is a powerful parametric models, it allows to model relationships between <strong>continuous independent and dependent variables</strong> and <strong>between qualitative independent variables and continuous variables</strong>. However, it does not allow to model relationships between continuous or qualitative independent variables and <strong>qualitative dependent variables</strong>.</p>
<section id="example-data">
<h2>Example Data<a class="headerlink" href="#example-data" title="Permalink to this heading">#</a></h2>
<p>Establishing such relationships is useful in different contexts. For instance, let us consider the <a class="reference external" href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic">Breast Cancer Wisconsin</a> dataset:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>radius1</th>
      <th>texture1</th>
      <th>perimeter1</th>
      <th>area1</th>
      <th>smoothness1</th>
      <th>compactness1</th>
      <th>concavity1</th>
      <th>concave_points1</th>
      <th>symmetry1</th>
      <th>fractal_dimension1</th>
      <th>...</th>
      <th>texture3</th>
      <th>perimeter3</th>
      <th>area3</th>
      <th>smoothness3</th>
      <th>compactness3</th>
      <th>concavity3</th>
      <th>concave_points3</th>
      <th>symmetry3</th>
      <th>fractal_dimension3</th>
      <th>Diagnosis</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.30010</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.16220</td>
      <td>0.66560</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
      <td>M</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.08690</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.12380</td>
      <td>0.18660</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
      <td>M</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.19740</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.14440</td>
      <td>0.42450</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
      <td>M</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.24140</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>0.09744</td>
      <td>...</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.20980</td>
      <td>0.86630</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
      <td>M</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.19800</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>0.05883</td>
      <td>...</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.13740</td>
      <td>0.20500</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
      <td>M</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>21.56</td>
      <td>22.39</td>
      <td>142.00</td>
      <td>1479.0</td>
      <td>0.11100</td>
      <td>0.11590</td>
      <td>0.24390</td>
      <td>0.13890</td>
      <td>0.1726</td>
      <td>0.05623</td>
      <td>...</td>
      <td>26.40</td>
      <td>166.10</td>
      <td>2027.0</td>
      <td>0.14100</td>
      <td>0.21130</td>
      <td>0.4107</td>
      <td>0.2216</td>
      <td>0.2060</td>
      <td>0.07115</td>
      <td>M</td>
    </tr>
    <tr>
      <th>565</th>
      <td>20.13</td>
      <td>28.25</td>
      <td>131.20</td>
      <td>1261.0</td>
      <td>0.09780</td>
      <td>0.10340</td>
      <td>0.14400</td>
      <td>0.09791</td>
      <td>0.1752</td>
      <td>0.05533</td>
      <td>...</td>
      <td>38.25</td>
      <td>155.00</td>
      <td>1731.0</td>
      <td>0.11660</td>
      <td>0.19220</td>
      <td>0.3215</td>
      <td>0.1628</td>
      <td>0.2572</td>
      <td>0.06637</td>
      <td>M</td>
    </tr>
    <tr>
      <th>566</th>
      <td>16.60</td>
      <td>28.08</td>
      <td>108.30</td>
      <td>858.1</td>
      <td>0.08455</td>
      <td>0.10230</td>
      <td>0.09251</td>
      <td>0.05302</td>
      <td>0.1590</td>
      <td>0.05648</td>
      <td>...</td>
      <td>34.12</td>
      <td>126.70</td>
      <td>1124.0</td>
      <td>0.11390</td>
      <td>0.30940</td>
      <td>0.3403</td>
      <td>0.1418</td>
      <td>0.2218</td>
      <td>0.07820</td>
      <td>M</td>
    </tr>
    <tr>
      <th>567</th>
      <td>20.60</td>
      <td>29.33</td>
      <td>140.10</td>
      <td>1265.0</td>
      <td>0.11780</td>
      <td>0.27700</td>
      <td>0.35140</td>
      <td>0.15200</td>
      <td>0.2397</td>
      <td>0.07016</td>
      <td>...</td>
      <td>39.42</td>
      <td>184.60</td>
      <td>1821.0</td>
      <td>0.16500</td>
      <td>0.86810</td>
      <td>0.9387</td>
      <td>0.2650</td>
      <td>0.4087</td>
      <td>0.12400</td>
      <td>M</td>
    </tr>
    <tr>
      <th>568</th>
      <td>7.76</td>
      <td>24.54</td>
      <td>47.92</td>
      <td>181.0</td>
      <td>0.05263</td>
      <td>0.04362</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.1587</td>
      <td>0.05884</td>
      <td>...</td>
      <td>30.37</td>
      <td>59.16</td>
      <td>268.6</td>
      <td>0.08996</td>
      <td>0.06444</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.2871</td>
      <td>0.07039</td>
      <td>B</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 31 columns</p>
</div></div></div>
</div>
<p>The dataset contains several measurements of given quantities measured from digitized image of a fine needle aspirate (FNA) of a breast mass, together with a categorical variable <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code> with two levels: <code class="docutils literal notranslate"><span class="pre">M</span></code> (malignant) and <code class="docutils literal notranslate"><span class="pre">B</span></code> (benign).</p>
<p>In this case, it would be good to be able to study whether a relationship exists between some of the considered independent variables and the dependent variable or simply to classify an example in one of the two classes from new observation.</p>
<p>We will consider the <code class="docutils literal notranslate"><span class="pre">radius1</span></code> variable for the moment. Let us plot this variable with respect to <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/2a5f4cbe55a097d315dd6ded3fbff13749c889188b93817874108b720bb2c443.png" src="../_images/2a5f4cbe55a097d315dd6ded3fbff13749c889188b93817874108b720bb2c443.png" />
</div>
</div>
<p>From the plot above, we can note that there is some form of relationship between the two variables. Indeed:</p>
<ul class="simple">
<li><p>For low values of <code class="docutils literal notranslate"><span class="pre">radius1</span></code>, we tend to have more benign cases;</p></li>
<li><p>For large values of <code class="docutils literal notranslate"><span class="pre">radius1</span></code>, we tend to have more malignant cases.</p></li>
</ul>
</section>
<section id="limits-of-linear-regression">
<h2>Limits of Linear Regression<a class="headerlink" href="#limits-of-linear-regression" title="Permalink to this heading">#</a></h2>
<p>Of course, we would like to quantify this relationship in a more formal way.
<strong>As in the case of a linear regressor, we want to define a model which can predict the independent variable <span class="math notranslate nohighlight">\(y\)</span> from the dependent variables <span class="math notranslate nohighlight">\(x_i\)</span>. If such model gives good predictions, than we can trust its interpretation as a means of studying the relationship between the variables.</strong></p>
<p>We can think of converting <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">=&gt;</span> <span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">M</span> <span class="pre">=&gt;</span> <span class="pre">0</span></code>, and then compute a linear regressor:</p>
<div class="math notranslate nohighlight">
\[Diagnosis = \beta_0 + \beta_1 radius1\]</div>
<p>This would be the result:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/cs/p62_d78d49n3ddj0xlfh1h7r0000gn/T/ipykernel_63863/3386663047.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option(&#39;future.no_silent_downcasting&#39;, True)`
  data[&#39;Diagnosis&#39;]=data[&#39;Diagnosis&#39;].replace({&#39;B&#39;:0,&#39;M&#39;:1})
</pre></div>
</div>
<img alt="../_images/aa620a78e91dc1a944bdcbd297b08be386636e3f467ea8e2d9a5723473f8d871.png" src="../_images/aa620a78e91dc1a944bdcbd297b08be386636e3f467ea8e2d9a5723473f8d871.png" />
</div>
</div>
<p>We can immediately see that this function does not model the relationship between the two variables very well. While we obtain a statistically relevant regressor with <span class="math notranslate nohighlight">\(R^2=0.533\)</span> and statistically relevant coefficients, the residual plot will look like this:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/da8e7f1760328d04d3f38e53526441875fa031ae01ccc4a507e7d4fe773f26db.png" src="../_images/da8e7f1760328d04d3f38e53526441875fa031ae01ccc4a507e7d4fe773f26db.png" />
</div>
</div>
<p>The correlation between the residuals and the independent variable is a strong indication that the true relationship between the two variables is not correctly modeled. After all, from a purely predictive point of view, we are using a linear regressor which takes the form:</p>
<div class="math notranslate nohighlight">
\[f:\mathbb{R} \to \mathbb{R}\]</div>
<p>while the values <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code> variable belong to the set <span class="math notranslate nohighlight">\(\{0,1\}\)</span> and we would need instead a function with the following form:</p>
<div class="math notranslate nohighlight">
\[f:\mathbb{R} \to \{0,1\}\]</div>
<p>However, the linear regressor cannot directly predict <strong>discrete values</strong>.</p>
<p><strong>In practice, while with a linear regressor we wanted to predict continuous values, now we want to assign observations <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to discrete bins (in this case only two possible ones). As we will better study later in the course, this problem is known as classification.</strong></p>
</section>
<section id="from-binary-values-to-probabilities">
<h2>From Binary Values to Probabilities<a class="headerlink" href="#from-binary-values-to-probabilities" title="Permalink to this heading">#</a></h2>
<p>If we want to model some form of continuous value, we could think to transition from <span class="math notranslate nohighlight">\(\{0,1\}\)</span> to <span class="math notranslate nohighlight">\([0,1]\)</span> using probabilities, which is a way to turn discretized values to “soft” values indicating our belief in the fact that <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code> will take either a <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span> value. We could hence think to model the following probability, rather than modeling <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code> directly:</p>
<div class="math notranslate nohighlight">
\[P(Diagnosis=1| radius1)\]</div>
<p>Note that modeling that probability directly is what **discriminative models do.</p>
<p>However, even in this case, a model of the form:</p>
<div class="math notranslate nohighlight">
\[P(Diagnosis=1|radius1) = \beta_0 + \beta_1 radius1\]</div>
<p>Would not be appropriate. Indeed, while <span class="math notranslate nohighlight">\(P(Diagnosis=1| radius1)\)</span> needs to be in the <span class="math notranslate nohighlight">\([0,1]\)</span> range, the linear combination <span class="math notranslate nohighlight">\(\beta_0 + \beta_1 radius1\)</span> will naturally output values <strong>smaller than <span class="math notranslate nohighlight">\(0\)</span></strong> and <strong>larger than <span class="math notranslate nohighlight">\(1\)</span></strong>. How should we interpret such values?</p>
<p>Intuitively, we would expect to <span class="math notranslate nohighlight">\(P(Diagnosis=1| radius1)\)</span> to assume values in the <span class="math notranslate nohighlight">\([0,1]\)</span> range for intermediate values (say <code class="docutils literal notranslate"><span class="pre">radius</span></code> <span class="math notranslate nohighlight">\(\in [10,20]\)</span>), while for extremely low values of (say <code class="docutils literal notranslate"><span class="pre">radius</span></code> <span class="math notranslate nohighlight">\(&lt;10\)</span>) the probability <strong>should saturate to <span class="math notranslate nohighlight">\(0\)</span></strong> and for extremely large values (say <code class="docutils literal notranslate"><span class="pre">radius</span></code> <span class="math notranslate nohighlight">\(&gt;20\)</span>) the probability should saturate to 1.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">radius1</span></code> takes large values (say larger than <span class="math notranslate nohighlight">\(20\)</span>), we expect <strong>probability to saturate to <span class="math notranslate nohighlight">\(1\)</span></strong>.</p>
<p>In practice, we would expect a result similar to the following:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/0373d3dceb147b6c51686acef750fb4160e7342d19d635ed36dce788cb2d6c38.png" src="../_images/0373d3dceb147b6c51686acef750fb4160e7342d19d635ed36dce788cb2d6c38.png" />
</div>
</div>
<p>As can be noted, the function above is not linear, and hence it cannot be fit with a linear regressor. However, we have seen that a linear regressor can be tweaked to also represent nonlinear functions.</p>
</section>
<section id="the-logistic-function">
<h2>The Logistic Function<a class="headerlink" href="#the-logistic-function" title="Permalink to this heading">#</a></h2>
<p>We need to find a <strong>transformation of the formulation of the linear regressor to transform its output into a nonlinear function of the independent variables</strong>. Of course, we do not want <em>any</em> transformation, but one that has the previously highlighted properties.</p>
<p>In practice the <strong>logistic function has some nice properties that, as we will se in a moment, allow to easily interpret the resulting model in a probabilistic way</strong>. The logistic function is defined as:</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{1+e^{-x}}\]</div>
<p>and has the following shape:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/f05b06c8f36418118bb5c3b6f488a8986fa75608f9632edba3ef507a90911442.png" src="../_images/f05b06c8f36418118bb5c3b6f488a8986fa75608f9632edba3ef507a90911442.png" />
</div>
</div>
<p>As we can see, the function has the properties we need:</p>
<ul class="simple">
<li><p>Its values are comprised between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>;</p></li>
<li><p>It saturates to <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> for extreme values of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
<p>Additionally, it is differentiable, which will be useful for optimization later.</p>
<p>To obtain the size of population <span class="math notranslate nohighlight">\(P\)</span> for varying <span class="math notranslate nohighlight">\(t\)</span>, we can solve the ODE above, obtaining the following expression:</p>
<div class="math notranslate nohighlight">
\[P = \frac{K}{1+(\frac{K-P_0}{P_0})e^{-rt}}\]</div>
<p>From here, it can be shown that we can obtain the standard logistic function:</p>
<div class="math notranslate nohighlight">
\[f(t) = \frac{1}{1+e^{-t}}\]</div>
</section>
<section id="the-logistic-regression-model">
<h2>The Logistic Regression Model<a class="headerlink" href="#the-logistic-regression-model" title="Permalink to this heading">#</a></h2>
<p>In practice, we define our model, <strong>the logistic regressor model</strong> as follows (<strong>simple logistic regression</strong>):</p>
<div class="math notranslate nohighlight">
\[P(Diagnosis=1|X) = f(\beta_0 + \beta_1 X) = \frac{1}{1+e^{-(\beta_0 + \beta_1 X)}}\]</div>
<p>Or, more in general (<strong>multiple logistic regression</strong>):</p>
<div class="math notranslate nohighlight">
\[P(y=1|\mathbf{x}) = \frac{1}{1+e^{-(\beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n)}}\]</div>
<p>It is easy to see that:</p>
<div class="math notranslate nohighlight">
\[p=\frac{1}{1+e^{-x}} \Rightarrow p+pe^{-x} = 1 \Rightarrow pe^{-x} = 1-p \Rightarrow e^{-x} = \frac{1-p}{p} \Rightarrow e^{x} = \frac{p}{1-p}\]</div>
<p>Hence:</p>
<div class="math notranslate nohighlight">
\[e^{\beta_0+\beta_1x_1 + \ldots + \beta_nx_n} = \frac{P(y=1|\mathbf{x})}{1-P(y=1|\mathbf{x})}\]</div>
<p>The term on the right is called the odd of <span class="math notranslate nohighlight">\(P(y=1|\mathbf{x})\)</span>.</p>
<p>The odd of <span class="math notranslate nohighlight">\(P(y=1|\mathbf{x})\)</span> is the number of times we believe the example will be positive (observed <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>) over the number of times we believe the example will be negative.</p>
<p>For instance, if we believe that the example will be positive <span class="math notranslate nohighlight">\(3\)</span> times out of <span class="math notranslate nohighlight">\(10\)</span>, then the odd will be <span class="math notranslate nohighlight">\(\frac{3}{7}\)</span>.</p>
<p>By taking the logarithm of both terms, we obtain:</p>
<div class="math notranslate nohighlight">
\[\log \left(\frac{P(y=1|\mathbf{x})}{1-P(y=1|\mathbf{x})}\right) = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n\]</div>
<p>The expression:</p>
<div class="math notranslate nohighlight">
\[\log \left(\frac{P(y=1|\mathbf{x})}{1-P(y=1|\mathbf{x})}\right)\]</div>
<p>Is the logarithm of the odd (log odd), and it is called <strong>logit</strong>, hence the <strong>logistic regression</strong> is sometimes called <strong>logit regression</strong>.</p>
<p>The expression above shows how a logistic regressor can be seen as <strong>a linear regressor (the expression on the right side of the equation) on the logit (the log odd)</strong>. This paves the way to useful interpretations of the model, as shown in the next section.</p>
</section>
<section id="estimation-of-the-parameters-of-a-logistic-regressor">
<h2>Estimation of the Parameters of a Logistic Regressor<a class="headerlink" href="#estimation-of-the-parameters-of-a-logistic-regressor" title="Permalink to this heading">#</a></h2>
<p>To fit the model and find suitable values for the <span class="math notranslate nohighlight">\(\mathbf{\beta_i}\)</span>
parameters, we will define a <strong>cost function</strong>, similarly to what we have
done in the case of linear regression.</p>
<p>Even if we can see the logistic
regression problem as the linear regression problem of fitting the
<span class="math notranslate nohighlight">\(logit(p) = \mathbf{\beta}^{T}\mathbf{x}\)</span> function, differently from
linear regression, <strong>we should note that we do not have the ground truth probabilities p</strong>.
Indeed, our observations only provide input examples
<span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> and the corresponding labels <span class="math notranslate nohighlight">\(y^{(i)}\)</span>.</p>
<p>Starting from the definition:</p>
<div class="math notranslate nohighlight">
\[P\left( y = 1 \middle| \mathbf{x}; \mathbf{\beta} \right) = f_{\mathbf{\beta}}\left( \mathbf{x} \right) = \frac{1}{1 + e^{- \mathbf{\beta}^{T}\mathbf{x}}} = \sigma(\mathbf{\beta}^{T}\mathbf{x})\]</div>
<p>We can write:</p>
<div class="math notranslate nohighlight">
\[P\left( y = 1 \middle| \mathbf{x};\mathbf{\beta} \right) = f_{\mathbf{\beta}}(\mathbf{x})\]</div>
<div class="math notranslate nohighlight">
\[P\left( y = 0 \middle| \mathbf{x};\mathbf{\beta} \right) = 1 - f_{\mathbf{\beta}}(\mathbf{x})\]</div>
<p>Since <span class="math notranslate nohighlight">\(y\)</span> can only take values <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, this can also be written as follows in a more compact form:</p>
<div class="math notranslate nohighlight">
\[P\left( y \middle| \mathbf{x};\mathbf{\beta} \right) = \left( f_{\mathbf{\beta}}\left( \mathbf{x} \right) \right)^{y}\left( 1 - f_{\mathbf{\beta}}\left( \mathbf{x} \right) \right)^{1 - y}\]</div>
<p>Indeed, when <span class="math notranslate nohighlight">\(y = 1\)</span>, the second factor is equal to <span class="math notranslate nohighlight">\(1\)</span> and the
expression reduces to
<span class="math notranslate nohighlight">\(P\left( y = 1 \middle| \mathbf{x};\mathbf{\beta} \right) = f_{\mathbf{\beta}}(\mathbf{x})\)</span>.
Similarly, if <span class="math notranslate nohighlight">\(y = 0\)</span>, the first factor is equal to <span class="math notranslate nohighlight">\(1\)</span> and the
expression reduces to <span class="math notranslate nohighlight">\(1 - f_{\mathbf{\beta}}(x)\)</span>.</p>
<p>We can estimate the parameters by maximum likelihood, i.e., choosing the
values of the parameters which maximize the probability of the data
under the model identified by the parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[L\left( \mathbf{\beta} \right) = P(Y|X;\mathbf{\beta})\]</div>
<p>If we assume that the training examples are all independent, the
likelihood can be expressed as:</p>
<div class="math notranslate nohighlight">
\[L\left( \mathbf{\beta} \right) = \prod_{i = 1}^{N}{P(y^{(i)}|\mathbf{x}^{(i)};\mathbf{\beta})} = \prod_{i = 1}^{N}{f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)^{y^{(i)}}\left( 1 - f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right) \right)^{{1 - y}^{(i)}}}\]</div>
<p>Maximizing this expression is equivalent to minimizing the negative
logarithm of <span class="math notranslate nohighlight">\(L(\mathbf{\beta})\)</span> (negative log-likelihood - nll):</p>
<div class="math notranslate nohighlight">
\[nll\left( \mathbf{\beta} \right) = - \log{L\left( \mathbf{\beta} \right)} = - \sum_{i = 1}^{N}{\log\left\lbrack f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)^{y^{(i)}}\left( 1 - f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right) \right)^{{1 - y}^{(i)}} \right\rbrack} =\]</div>
<div class="math notranslate nohighlight">
\[= - \sum_{i = 1}^{N}{\lbrack y^{(i)}\log{f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)}} + \left( 1 - y^{(i)} \right)\log{(1 - f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)})\rbrack\]</div>
<p>Hence, we will define our cost function as:</p>
<div class="math notranslate nohighlight">
\[J\left( \mathbf{\beta} \right) = - \sum_{i = 1}^{N}{\lbrack y^{(i)}\log{f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)}} + \left( 1 - y^{(i)} \right)\log{(1 - f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)})\rbrack\]</div>
<p>This can be rewritten more explicitly in terms of the <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>
parameters as follows:</p>
<div class="math notranslate nohighlight">
\[J\left( \mathbf{\beta} \right) = - \sum_{i = 1}^{N}{\lbrack y^{(i)}\log{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}} + \left( 1 - y^{(i)} \right)\log{(1 - \sigma\left( \mathbf{\beta}^{T}\mathbf{x}^{(i)} \right)})\rbrack\]</div>
<p>Similarly to linear regression, we now have a cost function to minimize in order to find the values of the <span class="math notranslate nohighlight">\(\beta_i\)</span> parameters. Unfortunately, in this case, <span class="math notranslate nohighlight">\(J(\mathbf{\beta})\)</span> assumes a nonlinear form <strong>which prevents us to use the least square principles</strong> and <strong>there is no closed form solution for the parameter estimation</strong>. In these cases, parameters can be estimated using some form of <strong>iterative solver</strong>, which begins with an initial guess for the parameters and iteratively refine them to find the final solution. Luckily, the logistic regression cost function <strong>is convex, and hence only a single solution is admitted, independently from the initial guess</strong>.</p>
<p>Different iterative solvers can be used in practice. The most commonly used is the <strong>gradient descent algorithm</strong>, which requires the cost function to be differentiable.</p>
</section>
<section id="statistical-interpretation-of-the-coefficients-of-a-linear-regressor">
<h2>Statistical Interpretation of the Coefficients of a Linear Regressor<a class="headerlink" href="#statistical-interpretation-of-the-coefficients-of-a-linear-regressor" title="Permalink to this heading">#</a></h2>
<p>Let’s now see how to interpret the coefficients of a logistic regressor.</p>
<section id="interpretation-of-the-intercept-beta-0">
<h3>Interpretation of the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span><a class="headerlink" href="#interpretation-of-the-intercept-beta-0" title="Permalink to this heading">#</a></h3>
<p>Remember that the regression model (in the case of simple logistic regression) is as follows:</p>
<div class="math notranslate nohighlight">
\[
\log(\frac{p}{1-p})=\beta_0 + \beta_1 x
\]</div>
<p>Applying what we know about logistic regressors, we can write:</p>
<div class="math notranslate nohighlight">
\[
x=0 \Rightarrow \log \frac{p}{1-p}={\beta}_0
\]</div>
<p>To have a clearer picture, we can exponentiate both sides of the equation and write:</p>
<div class="math notranslate nohighlight">
\[x=0 \Rightarrow \frac{p}{1-p}=e^{\beta_0}\]</div>
<p>Remember that <span class="math notranslate nohighlight">\(\frac{p}{1-p}\)</span> is the odd that the dependent variable is equal to 1 when observing <span class="math notranslate nohighlight">\(x\)</span> and, as such, it has a clear interpretation. For example, if the odds of an event are <span class="math notranslate nohighlight">\(\frac{3}{1}\)</span>, then it is <span class="math notranslate nohighlight">\(3\)</span> times more likely to occur than not to occur. So, <strong>for <span class="math notranslate nohighlight">\(x=0\)</span>, it is <span class="math notranslate nohighlight">\(e^{\beta_0}\)</span> times more likely that the dependent variable is equal to 1, rather than being equal to 0</strong>.</p>
</section>
<section id="interpretation-of-variable-coefficients-beta-i">
<h3>Interpretation of variable coefficients <span class="math notranslate nohighlight">\(\beta_i\)</span><a class="headerlink" href="#interpretation-of-variable-coefficients-beta-i" title="Permalink to this heading">#</a></h3>
<p>We know that:</p>
<div class="math notranslate nohighlight">
\[odds(p|x) = \frac{P(y=1|x)}{1-P(y=1|x)}\]</div>
<p>We can write:</p>
<div class="math notranslate nohighlight">
\[
\log odds(p|x) = \beta_0 + \beta_1 x
\]</div>
<p>Hence:</p>
<div class="math notranslate nohighlight">
\[\log odds(p|x+1) - \log odds(p|x) = \beta_0 + \beta_1 (x+1) - \beta_0 - \beta_1 x = \beta_1 (x+1) - \beta_1 x = \beta_1\]</div>
<p>Exponentiating both sides, we get:</p>
<div class="math notranslate nohighlight">
\[e^{\log odds(p|x+1) - \log odds(p|x)} = e^{\beta_1} \Rightarrow \frac{e^{\log odds(p|x+1)}}{e^{\log odds(p|x)}} = e^{\beta_1} \Rightarrow \frac{odds(p|x+1)}{odds(p|x)} = e^{\beta_1} \Rightarrow odds(p|x+1) = e^{\beta_1}odds(p|x)\]</div>
<p>We can thus say that <strong>increasing the variable <span class="math notranslate nohighlight">\(x\)</span> by one unit corresponds to a multiplicative increase in odds by <span class="math notranslate nohighlight">\(e^{\beta_1}\)</span></strong>.</p>
<p>This analysis can be easily extended to the case of a multiple logistic regressor. Hence in general, given the model:</p>
<div class="math notranslate nohighlight">
\[P(y=1|\mathbf{x}) = \sigma(\beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n)\]</div>
<p>We can say that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(e^{\beta_0}\)</span> is the odd of <span class="math notranslate nohighlight">\(y\)</span> being equal to <span class="math notranslate nohighlight">\(1\)</span> rather than <span class="math notranslate nohighlight">\(0\)</span> when <span class="math notranslate nohighlight">\(x_i=0, \ \forall i\)</span>;</p></li>
<li><p>An increment of one unit in the independent variable <span class="math notranslate nohighlight">\(x_i\)</span> corresponds to a multiplicative increment of <span class="math notranslate nohighlight">\(e^{\beta_i}\)</span> in the odds of <span class="math notranslate nohighlight">\(y=1\)</span>. So if <span class="math notranslate nohighlight">\(e^{\beta_i}=0.05\)</span>, then <span class="math notranslate nohighlight">\(y=1\)</span> is <span class="math notranslate nohighlight">\(5\%\)</span> more likely for a one-unit increment of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
</section>
</section>
<section id="example-of-logistic-regression">
<h2>Example of Logistic Regression<a class="headerlink" href="#example-of-logistic-regression" title="Permalink to this heading">#</a></h2>
<p>Let us now apply logistic regression to a larger set of variables in our regression problem. We will consider the following independent variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">radius1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">texture1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">perimeter1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">area1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">smoothness1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">compactness1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">concavity1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">symmetry1</span></code></p></li>
</ul>
<p>The dependent variable is again <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code>.</p>
<p>Once fit to the data, we will obtain the following parameters:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(R^2\)</span></p></th>
<th class="head"><p>Adj. <span class="math notranslate nohighlight">\(R^2\)</span></p></th>
<th class="head"><p>F-statistic</p></th>
<th class="head"><p>Prob(F-statistic)</p></th>
<th class="head"><p>Log-Likelihood</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.670</p></td>
<td><p>0.666</p></td>
<td><p>142.4</p></td>
<td><p>1.16e-129</p></td>
<td><p>-78.055</p></td>
</tr>
</tbody>
</table>
<p>All values have interpretations similar to the ones obtained in the case of linear regression. The Log-Likelihood reports the value of the logarithm of the likelihood which was used to train the data.</p>
<p>The estimates for the coefficients are as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;Diagnosis ~ radius1 + texture1 + perimeter1 + area1 + smoothness1 + compactness1 + concavity1 + symmetry1&quot;</span><span class="p">,</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>   -2.6591</td> <td>    0.224</td> <td>  -11.896</td> <td> 0.000</td> <td>   -3.098</td> <td>   -2.220</td>
</tr>
<tr>
  <th>radius1</th>      <td>    0.4688</td> <td>    0.133</td> <td>    3.532</td> <td> 0.000</td> <td>    0.208</td> <td>    0.730</td>
</tr>
<tr>
  <th>texture1</th>     <td>    0.0219</td> <td>    0.003</td> <td>    7.376</td> <td> 0.000</td> <td>    0.016</td> <td>    0.028</td>
</tr>
<tr>
  <th>perimeter1</th>   <td>   -0.0473</td> <td>    0.021</td> <td>   -2.272</td> <td> 0.023</td> <td>   -0.088</td> <td>   -0.006</td>
</tr>
<tr>
  <th>area1</th>        <td>   -0.0009</td> <td>    0.000</td> <td>   -3.985</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.000</td>
</tr>
<tr>
  <th>smoothness1</th>  <td>    5.1389</td> <td>    1.221</td> <td>    4.208</td> <td> 0.000</td> <td>    2.740</td> <td>    7.538</td>
</tr>
<tr>
  <th>compactness1</th> <td>    0.3080</td> <td>    0.854</td> <td>    0.360</td> <td> 0.719</td> <td>   -1.370</td> <td>    1.986</td>
</tr>
<tr>
  <th>concavity1</th>   <td>    2.0973</td> <td>    0.414</td> <td>    5.065</td> <td> 0.000</td> <td>    1.284</td> <td>    2.911</td>
</tr>
<tr>
  <th>symmetry1</th>    <td>    1.2739</td> <td>    0.568</td> <td>    2.244</td> <td> 0.025</td> <td>    0.159</td> <td>    2.389</td>
</tr>
</table></div></div>
</div>
<p>We notice that not all variables have a statistically relevant relationship with the dependent variable. Applying backward elimination, we remove <code class="docutils literal notranslate"><span class="pre">compactness1</span></code> and obtain the following estimates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;Diagnosis ~ radius1 + texture1 + perimeter1 + area1 + smoothness1 + concavity1 + symmetry1&quot;</span><span class="p">,</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>   <td>   -2.6708</td> <td>    0.221</td> <td>  -12.086</td> <td> 0.000</td> <td>   -3.105</td> <td>   -2.237</td>
</tr>
<tr>
  <th>radius1</th>     <td>    0.4360</td> <td>    0.097</td> <td>    4.517</td> <td> 0.000</td> <td>    0.246</td> <td>    0.626</td>
</tr>
<tr>
  <th>texture1</th>    <td>    0.0219</td> <td>    0.003</td> <td>    7.405</td> <td> 0.000</td> <td>    0.016</td> <td>    0.028</td>
</tr>
<tr>
  <th>perimeter1</th>  <td>   -0.0419</td> <td>    0.014</td> <td>   -2.915</td> <td> 0.004</td> <td>   -0.070</td> <td>   -0.014</td>
</tr>
<tr>
  <th>area1</th>       <td>   -0.0010</td> <td>    0.000</td> <td>   -4.477</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>
</tr>
<tr>
  <th>smoothness1</th> <td>    5.3093</td> <td>    1.125</td> <td>    4.719</td> <td> 0.000</td> <td>    3.099</td> <td>    7.519</td>
</tr>
<tr>
  <th>concavity1</th>  <td>    2.1479</td> <td>    0.389</td> <td>    5.517</td> <td> 0.000</td> <td>    1.383</td> <td>    2.913</td>
</tr>
<tr>
  <th>symmetry1</th>   <td>    1.3132</td> <td>    0.557</td> <td>    2.359</td> <td> 0.019</td> <td>    0.220</td> <td>    2.407</td>
</tr>
</table></div></div>
</div>
<p>These are now all statistically relevant. For instance, we can see that:</p>
<ul class="simple">
<li><p>When all variables are set to zero, the odds of the benign tumor are <span class="math notranslate nohighlight">\(e^{-2.6708} \approx 0.07\)</span>, or <span class="math notranslate nohighlight">\(\frac{7}{100}\)</span>. This is a base value.</p></li>
<li><p>An increment in one unit of <code class="docutils literal notranslate"><span class="pre">texture1</span></code> increments the odds of a benign tumor multiplicatively by a factor of <span class="math notranslate nohighlight">\(e^{0.0219} \approx 1.02\)</span> (a +<span class="math notranslate nohighlight">\(2\%\)</span>).</p></li>
<li><p>An increment of one unit of <code class="docutils literal notranslate"><span class="pre">perimeter1</span></code> decrements the odds of benign tumor multiplicatively by a factor of <span class="math notranslate nohighlight">\(e^{-0.0419} \approx 0.96\)</span> (a -<span class="math notranslate nohighlight">\(4\%\)</span>).</p></li>
</ul>
</section>
<section id="evaluating-the-logistic-regression-model">
<h2>Evaluating the Logistic Regression Model<a class="headerlink" href="#evaluating-the-logistic-regression-model" title="Permalink to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> summary table gives us a lot of information, including a value for <code class="docutils literal notranslate"><span class="pre">R-squared</span></code>. It’s critical to understand that this is <strong>NOT</strong> the same <span class="math notranslate nohighlight">\(R^2\)</span> we used in linear regression.</p>
<section id="the-problem-we-can-t-use-r-2">
<h3>The Problem: We Can’t Use <span class="math notranslate nohighlight">\(R^2\)</span><a class="headerlink" href="#the-problem-we-can-t-use-r-2" title="Permalink to this heading">#</a></h3>
<p>In OLS (Linear Regression), <span class="math notranslate nohighlight">\(R^2 = 1 - \frac{RSS}{TSS}\)</span>. This metric is based on minimizing the <em>sum of squared residuals</em> (RSS).</p>
<p>In Logistic Regression, our model does not predict a continuous value. It predicts a <em>probability</em>. We don’t minimize squared residuals; we minimize the <strong>negative log-likelihood</strong> (also called <strong>Cross-Entropy Loss</strong>).</p>
<p>Because the underlying cost function is different, the <span class="math notranslate nohighlight">\(R^2\)</span> metric is not mathematically valid. A model that predicts a probability of <code class="docutils literal notranslate"><span class="pre">0.9</span></code> for an event that happens (<code class="docutils literal notranslate"><span class="pre">y=1</span></code>) is a <em>fantastic</em> prediction, but it would have a “residual” of <span class="math notranslate nohighlight">\((1 - 0.9) = 0.1\)</span>, which OLS would want to square and “punish.” This doesn’t make sense.</p>
</section>
<section id="the-solution-pseudo-r-2">
<h3>The Solution: “Pseudo <span class="math notranslate nohighlight">\(R^2\)</span>”<a class="headerlink" href="#the-solution-pseudo-r-2" title="Permalink to this heading">#</a></h3>
<p>To get a similar “0-to-1” metric for <em>goodness-of-fit</em>, statisticians have developed several <strong>“Pseudo <span class="math notranslate nohighlight">\(R^2\)</span>”</strong> measures. The one <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> reports by default is <strong>McFadden’s <span class="math notranslate nohighlight">\(R^2\)</span></strong>.</p>
<p>The logic is the same as OLS <span class="math notranslate nohighlight">\(R^2\)</span>: “How much better is our model than a ‘dumb’ baseline?”</p>
<ul class="simple">
<li><p><strong>In OLS:</strong> We compared our model’s error (<span class="math notranslate nohighlight">\(RSS\)</span>) to the baseline’s error (<span class="math notranslate nohighlight">\(TSS\)</span>).</p></li>
<li><p><strong>In Logistic Regression:</strong> We compare our model’s <strong>Log-Likelihood</strong> to a baseline model’s Log-Likelihood.</p></li>
</ul>
<ol class="arabic simple">
<li><p><strong>Baseline Model (Log-Likelihood Null, <span class="math notranslate nohighlight">\(LL_{Null}\)</span>):</strong>
A “dumb” model that only knows the <em>prior</em> (e.g., <span class="math notranslate nohighlight">\(P(y=1) = 0.35\)</span> for the <code class="docutils literal notranslate"><span class="pre">biopsy</span></code> set). This is an “intercept-only” model. <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> calculates this for you.</p></li>
<li><p><strong>Our “Full” Model (Log-Likelihood, <span class="math notranslate nohighlight">\(LL_{Model}\)</span>):</strong>
This is the log-likelihood of our <em>fitted</em> model. It will be a “less negative” (better) number than <span class="math notranslate nohighlight">\(LL_{Null}\)</span>.</p></li>
</ol>
<p><strong>McFadden’s <span class="math notranslate nohighlight">\(R^2\)</span></strong> is then defined as:</p>
<div class="math notranslate nohighlight">
\[
R^2_{McFadden} = 1 - \frac{LL_{Model}}{LL_{Null}}
\]</div>
<p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(LL_{Model}\)</span></strong> is the value reported as “Log-Likelihood” in your summary table.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(LL_{Null}\)</span></strong> is reported as “LL-Null”.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Pseudo</span> <span class="pre">R-sq.</span></code>:</strong> This is the <span class="math notranslate nohighlight">\(R^2_{McFadden}\)</span> score.</p></li>
</ul>
<p>Let’s look at your <code class="docutils literal notranslate"><span class="pre">biopsy</span></code> model summary:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Log-Likelihood:</span> <span class="pre">-105.19</span></code> (<span class="math notranslate nohighlight">\(LL_{Model}\)</span>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LL-Null:</span> <span class="pre">-452.39</span></code> (<span class="math notranslate nohighlight">\(LL_{Null}\)</span>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Pseudo</span> <span class="pre">R-sq.:</span> <span class="pre">0.7675</span></code></p></li>
</ul>
<p>This <code class="docutils literal notranslate"><span class="pre">0.7675</span></code> was calculated as <span class="math notranslate nohighlight">\(1 - (\frac{-105.19}{-452.39}) \approx 0.767\)</span>.</p>
<p>It is interpreted similarly to <span class="math notranslate nohighlight">\(R^2\)</span>: “Our model’s features explain about <strong>76.7%</strong> of the ‘deviance’ (uncertainty) in the outcome, compared to a model that just guesses the average.”</p>
<p><strong>Important:</strong> You cannot compare a <code class="docutils literal notranslate"><span class="pre">Pseudo</span> <span class="pre">R-sq.</span></code> from logistic regression to an <span class="math notranslate nohighlight">\(R^2\)</span> from OLS. They are not equivalent. It is only useful for comparing <em>nested</em> logistic regression models (like we do in backward elimination).</p>
</section>
<section id="predictive-evaluation">
<h3>Predictive Evaluation<a class="headerlink" href="#predictive-evaluation" title="Permalink to this heading">#</a></h3>
<p>When we use the model in a purely predictive way, we can also use all evaluation measures we have seen for predictive analysis, such as accuracy, precision, recall, F1, confusion matrix etc.</p>
</section>
</section>
<section id="logistic-regression-in-python">
<h2>Logistic Regression in Python<a class="headerlink" href="#logistic-regression-in-python" title="Permalink to this heading">#</a></h2>
<p>Let’s look at an example of logistic regression in Python. We will use the R dataset <code class="docutils literal notranslate"><span class="pre">biopsy</span></code>. We can load it using <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.datasets</span> <span class="kn">import</span> <span class="n">get_rdataset</span>
<span class="n">biopsy</span> <span class="o">=</span> <span class="n">get_rdataset</span><span class="p">(</span><span class="s1">&#39;biopsy&#39;</span><span class="p">,</span><span class="n">package</span><span class="o">=</span><span class="s1">&#39;MASS&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">biopsy</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. container::

   .. container::

      ====== ===============
      biopsy R Documentation
      ====== ===============

      .. rubric:: Biopsy Data on Breast Cancer Patients
         :name: biopsy-data-on-breast-cancer-patients

      .. rubric:: Description
         :name: description

      This breast cancer database was obtained from the University of
      Wisconsin Hospitals, Madison from Dr. William H. Wolberg. He
      assessed biopsies of breast tumours for 699 patients up to 15 July
      1992; each of nine attributes has been scored on a scale of 1 to
      10, and the outcome is also known. There are 699 rows and 11
      columns.

      .. rubric:: Usage
         :name: usage

      .. code:: R

         biopsy

      .. rubric:: Format
         :name: format

      This data frame contains the following columns:

      ``ID``
         sample code number (not unique).

      ``V1``
         clump thickness.

      ``V2``
         uniformity of cell size.

      ``V3``
         uniformity of cell shape.

      ``V4``
         marginal adhesion.

      ``V5``
         single epithelial cell size.

      ``V6``
         bare nuclei (16 values are missing).

      ``V7``
         bland chromatin.

      ``V8``
         normal nucleoli.

      ``V9``
         mitoses.

      ``class``
         ``&quot;benign&quot;`` or ``&quot;malignant&quot;``.

      .. rubric:: Source
         :name: source

      P. M. Murphy and D. W. Aha (1992). UCI Repository of machine
      learning databases. [Machine-readable data repository]. Irvine,
      CA: University of California, Department of Information and
      Computer Science.

      O. L. Mangasarian and W. H. Wolberg (1990) Cancer diagnosis via
      linear programming. *SIAM News* **23**, pp 1 &amp; 18.

      William H. Wolberg and O.L. Mangasarian (1990) Multisurface method
      of pattern separation for medical diagnosis applied to breast
      cytology. *Proceedings of the National Academy of Sciences,
      U.S.A.* **87**, pp. 9193–9196.

      O. L. Mangasarian, R. Setiono and W.H. Wolberg (1990) Pattern
      recognition via linear programming: Theory and application to
      medical diagnosis. In *Large-scale Numerical Optimization* eds
      Thomas F. Coleman and Yuying Li, SIAM Publications, Philadelphia,
      pp 22–30.

      K. P. Bennett and O. L. Mangasarian (1992) Robust linear
      programming discrimination of two linearly inseparable sets.
      *Optimization Methods and Software* **1**, pp. 23–34 (Gordon &amp;
      Breach Science Publishers).

      .. rubric:: References
         :name: references

      Venables, W. N. and Ripley, B. D. (2002) *Modern Applied
      Statistics with S-PLUS.* Fourth Edition. Springer.
</pre></div>
</div>
</div>
</div>
<p>The dataset contains <span class="math notranslate nohighlight">\(699\)</span> observations and <span class="math notranslate nohighlight">\(11\)</span> columns. Each observation contains measurements of <span class="math notranslate nohighlight">\(9\)</span> quantities relating to tissue samples that can be “benign” or “malignant” tumors. Let’s start by manipulating the data a bit. We visualize the values of the <code class="docutils literal notranslate"><span class="pre">class</span></code> variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;benign&#39;, &#39;malignant&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>To calculate the logistic regression model using statsmodels, it is necessary to convert these values into integers (<span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>). Furthermore, it is advisable to avoid calling the column <code class="docutils literal notranslate"><span class="pre">class</span></code> as this is a reserved word for statsmodels. We build a new column <code class="docutils literal notranslate"><span class="pre">cl</span></code> that contains the modified <code class="docutils literal notranslate"><span class="pre">class</span></code> values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;cl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="s1">&#39;benign&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;malignant&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/cs/p62_d78d49n3ddj0xlfh1h7r0000gn/T/ipykernel_63863/663777289.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option(&#39;future.no_silent_downcasting&#39;, True)`
  biopsy.data[&#39;cl&#39;] = biopsy.data[&#39;class&#39;].replace({&#39;benign&#39;:0, &#39;malignant&#39;:1})
</pre></div>
</div>
</div>
</div>
<p>We will use the <code class="docutils literal notranslate"><span class="pre">logit</span></code> object form statsmodels:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">import</span> <span class="n">logit</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="s1">&#39;cl ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9&#39;</span><span class="p">,</span><span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.075321
         Iterations 10
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>cl</td>        <th>  No. Observations:  </th>   <td>   683</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   673</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     9</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Sat, 15 Nov 2025</td> <th>  Pseudo R-squ.:     </th>   <td>0.8837</td>  
</tr>
<tr>
  <th>Time:</th>                <td>16:22:35</td>     <th>  Log-Likelihood:    </th>  <td> -51.444</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -442.18</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.077e-162</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>  -10.1039</td> <td>    1.175</td> <td>   -8.600</td> <td> 0.000</td> <td>  -12.407</td> <td>   -7.801</td>
</tr>
<tr>
  <th>V1</th>        <td>    0.5350</td> <td>    0.142</td> <td>    3.767</td> <td> 0.000</td> <td>    0.257</td> <td>    0.813</td>
</tr>
<tr>
  <th>V2</th>        <td>   -0.0063</td> <td>    0.209</td> <td>   -0.030</td> <td> 0.976</td> <td>   -0.416</td> <td>    0.404</td>
</tr>
<tr>
  <th>V3</th>        <td>    0.3227</td> <td>    0.231</td> <td>    1.399</td> <td> 0.162</td> <td>   -0.129</td> <td>    0.775</td>
</tr>
<tr>
  <th>V4</th>        <td>    0.3306</td> <td>    0.123</td> <td>    2.678</td> <td> 0.007</td> <td>    0.089</td> <td>    0.573</td>
</tr>
<tr>
  <th>V5</th>        <td>    0.0966</td> <td>    0.157</td> <td>    0.617</td> <td> 0.537</td> <td>   -0.210</td> <td>    0.404</td>
</tr>
<tr>
  <th>V6</th>        <td>    0.3830</td> <td>    0.094</td> <td>    4.082</td> <td> 0.000</td> <td>    0.199</td> <td>    0.567</td>
</tr>
<tr>
  <th>V7</th>        <td>    0.4472</td> <td>    0.171</td> <td>    2.609</td> <td> 0.009</td> <td>    0.111</td> <td>    0.783</td>
</tr>
<tr>
  <th>V8</th>        <td>    0.2130</td> <td>    0.113</td> <td>    1.887</td> <td> 0.059</td> <td>   -0.008</td> <td>    0.434</td>
</tr>
<tr>
  <th>V9</th>        <td>    0.5348</td> <td>    0.329</td> <td>    1.627</td> <td> 0.104</td> <td>   -0.110</td> <td>    1.179</td>
</tr>
</table></div></div>
</div>
<p>The logistic regressor explains the relationship between variables well (<span class="math notranslate nohighlight">\(R^2\)</span> high) and is significant (p-value almost zero). Some coefficients have a high p-value. Let’s start by eliminating the variable <code class="docutils literal notranslate"><span class="pre">V2</span></code>, which has the highest p-value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="s1">&#39;cl ~ V1 + V3 + V4 + V5 + V6 + V7 + V8 + V9&#39;</span><span class="p">,</span><span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.075321
         Iterations 10
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>cl</td>        <th>  No. Observations:  </th>   <td>   683</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   674</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     8</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Sat, 15 Nov 2025</td> <th>  Pseudo R-squ.:     </th>   <td>0.8837</td>  
</tr>
<tr>
  <th>Time:</th>                <td>16:22:54</td>     <th>  Log-Likelihood:    </th>  <td> -51.445</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -442.18</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.036e-163</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>  -10.0976</td> <td>    1.155</td> <td>   -8.739</td> <td> 0.000</td> <td>  -12.362</td> <td>   -7.833</td>
</tr>
<tr>
  <th>V1</th>        <td>    0.5346</td> <td>    0.141</td> <td>    3.784</td> <td> 0.000</td> <td>    0.258</td> <td>    0.811</td>
</tr>
<tr>
  <th>V3</th>        <td>    0.3182</td> <td>    0.174</td> <td>    1.826</td> <td> 0.068</td> <td>   -0.023</td> <td>    0.660</td>
</tr>
<tr>
  <th>V4</th>        <td>    0.3299</td> <td>    0.121</td> <td>    2.723</td> <td> 0.006</td> <td>    0.092</td> <td>    0.567</td>
</tr>
<tr>
  <th>V5</th>        <td>    0.0961</td> <td>    0.156</td> <td>    0.618</td> <td> 0.537</td> <td>   -0.209</td> <td>    0.401</td>
</tr>
<tr>
  <th>V6</th>        <td>    0.3831</td> <td>    0.094</td> <td>    4.082</td> <td> 0.000</td> <td>    0.199</td> <td>    0.567</td>
</tr>
<tr>
  <th>V7</th>        <td>    0.4465</td> <td>    0.170</td> <td>    2.628</td> <td> 0.009</td> <td>    0.114</td> <td>    0.779</td>
</tr>
<tr>
  <th>V8</th>        <td>    0.2125</td> <td>    0.112</td> <td>    1.902</td> <td> 0.057</td> <td>   -0.006</td> <td>    0.432</td>
</tr>
<tr>
  <th>V9</th>        <td>    0.5341</td> <td>    0.328</td> <td>    1.630</td> <td> 0.103</td> <td>   -0.108</td> <td>    1.176</td>
</tr>
</table></div></div>
</div>
<p>We proceed by removing <code class="docutils literal notranslate"><span class="pre">V5</span></code>, which has a p-value of <span class="math notranslate nohighlight">\(0.537\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="s1">&#39;cl ~ V1 + V3 + V4 + V6 + V7 + V8 + V9&#39;</span><span class="p">,</span><span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.075598
         Iterations 10
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>cl</td>        <th>  No. Observations:  </th>   <td>   683</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   675</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     7</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Sat, 15 Nov 2025</td> <th>  Pseudo R-squ.:     </th>   <td>0.8832</td>  
</tr>
<tr>
  <th>Time:</th>                <td>16:22:56</td>     <th>  Log-Likelihood:    </th>  <td> -51.633</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -442.18</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.240e-164</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -9.9828</td> <td>    1.126</td> <td>   -8.865</td> <td> 0.000</td> <td>  -12.190</td> <td>   -7.776</td>
</tr>
<tr>
  <th>V1</th>        <td>    0.5340</td> <td>    0.141</td> <td>    3.793</td> <td> 0.000</td> <td>    0.258</td> <td>    0.810</td>
</tr>
<tr>
  <th>V3</th>        <td>    0.3453</td> <td>    0.172</td> <td>    2.012</td> <td> 0.044</td> <td>    0.009</td> <td>    0.682</td>
</tr>
<tr>
  <th>V4</th>        <td>    0.3425</td> <td>    0.119</td> <td>    2.873</td> <td> 0.004</td> <td>    0.109</td> <td>    0.576</td>
</tr>
<tr>
  <th>V6</th>        <td>    0.3883</td> <td>    0.094</td> <td>    4.150</td> <td> 0.000</td> <td>    0.205</td> <td>    0.572</td>
</tr>
<tr>
  <th>V7</th>        <td>    0.4619</td> <td>    0.168</td> <td>    2.746</td> <td> 0.006</td> <td>    0.132</td> <td>    0.792</td>
</tr>
<tr>
  <th>V8</th>        <td>    0.2261</td> <td>    0.111</td> <td>    2.037</td> <td> 0.042</td> <td>    0.009</td> <td>    0.444</td>
</tr>
<tr>
  <th>V9</th>        <td>    0.5312</td> <td>    0.324</td> <td>    1.637</td> <td> 0.102</td> <td>   -0.105</td> <td>    1.167</td>
</tr>
</table></div></div>
</div>
<p>We remove <code class="docutils literal notranslate"><span class="pre">V9</span></code>, which has a p-value greater than <span class="math notranslate nohighlight">\(0.05\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="s1">&#39;cl ~ V1 + V3 + V4 + V6 + V7 + V8&#39;</span><span class="p">,</span><span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.078436
         Iterations 9
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>cl</td>        <th>  No. Observations:  </th>   <td>   683</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   676</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     6</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Sat, 15 Nov 2025</td> <th>  Pseudo R-squ.:     </th>   <td>0.8788</td>  
</tr>
<tr>
  <th>Time:</th>                <td>16:22:57</td>     <th>  Log-Likelihood:    </th>  <td> -53.572</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -442.18</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.294e-164</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -9.7671</td> <td>    1.085</td> <td>   -9.001</td> <td> 0.000</td> <td>  -11.894</td> <td>   -7.640</td>
</tr>
<tr>
  <th>V1</th>        <td>    0.6225</td> <td>    0.137</td> <td>    4.540</td> <td> 0.000</td> <td>    0.354</td> <td>    0.891</td>
</tr>
<tr>
  <th>V3</th>        <td>    0.3495</td> <td>    0.165</td> <td>    2.118</td> <td> 0.034</td> <td>    0.026</td> <td>    0.673</td>
</tr>
<tr>
  <th>V4</th>        <td>    0.3375</td> <td>    0.116</td> <td>    2.920</td> <td> 0.004</td> <td>    0.111</td> <td>    0.564</td>
</tr>
<tr>
  <th>V6</th>        <td>    0.3786</td> <td>    0.094</td> <td>    4.035</td> <td> 0.000</td> <td>    0.195</td> <td>    0.562</td>
</tr>
<tr>
  <th>V7</th>        <td>    0.4713</td> <td>    0.166</td> <td>    2.837</td> <td> 0.005</td> <td>    0.146</td> <td>    0.797</td>
</tr>
<tr>
  <th>V8</th>        <td>    0.2432</td> <td>    0.109</td> <td>    2.240</td> <td> 0.025</td> <td>    0.030</td> <td>    0.456</td>
</tr>
</table></div></div>
</div>
<p>All coefficients now have an acceptable p-value. Let’s proceed to the analysis of the coefficients. We calculate the exponentials:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept    0.000057
V1           1.863641
V3           1.418374
V4           1.401487
V6           1.460166
V7           1.602133
V8           1.275287
dtype: float64
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The near-zero value of the exponential of the intercept indicates that, when all variables take zero values, the odds are very low. This suggests that <span class="math notranslate nohighlight">\(p\)</span> is low, while <span class="math notranslate nohighlight">\(1-p\)</span> is very high. The probability of having a malignant tumor is therefore very low if all variables take zero values;</p></li>
<li><p>An increase of one unit in the value of <code class="docutils literal notranslate"><span class="pre">V1</span></code> corresponds to an increase of approximately <span class="math notranslate nohighlight">\(86\%\)</span> in the odds, making the possibility of a malignant tumor higher;</p></li>
<li><p>An increase of one unit in the value of <code class="docutils literal notranslate"><span class="pre">V3</span></code> corresponds to an increase of approximately <span class="math notranslate nohighlight">\(41\%\)</span> in the odds;</p></li>
<li><p>An increase of one unit in the value of <code class="docutils literal notranslate"><span class="pre">V4</span></code> corresponds to an increase of approximately <span class="math notranslate nohighlight">\(40\%\)</span> in the odds;</p></li>
<li><p>An increase of one unit in the value of <code class="docutils literal notranslate"><span class="pre">V6</span></code> corresponds to an increase of approximately <span class="math notranslate nohighlight">\(46\%\)</span> in the odds;</p></li>
<li><p>An increase of one unit in the value of <code class="docutils literal notranslate"><span class="pre">V7</span></code> corresponds to an increase of approximately <span class="math notranslate nohighlight">\(60\%\)</span> in the odds;</p></li>
<li><p>An increase of one unit in the value of <code class="docutils literal notranslate"><span class="pre">V8</span></code> corresponds to an increase of approximately <span class="math notranslate nohighlight">\(27\%\)</span> in the odds;</p></li>
</ul>
<p>The increase of variables generally causes an increase in the odds. Therefore, we expect the values of the variables to be small in the presence of benign tumors.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Chapter <span class="math notranslate nohighlight">\(4\)</span> of [1]</p></li>
</ul>
<p>[1] James, Gareth Gareth Michael. An introduction to statistical learning: with applications in Python, 2023.<a class="reference external" href="https://www.statlearning.com">https://www.statlearning.com</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="11_classification_knn.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Classification Task, Evaluation Measures, and K-Nearest Neighbor</p>
      </div>
    </a>
    <a class="right-next"
       href="13_multiclass_logistic_regression.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Multiclass Logistic Regression and Predictive View</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-data">Example Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limits-of-linear-regression">Limits of Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-binary-values-to-probabilities">From Binary Values to Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-function">The Logistic Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-regression-model">The Logistic Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-logistic-regressor">Estimation of the Parameters of a Logistic Regressor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-interpretation-of-the-coefficients-of-a-linear-regressor">Statistical Interpretation of the Coefficients of a Linear Regressor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-intercept-beta-0">Interpretation of the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-variable-coefficients-beta-i">Interpretation of variable coefficients <span class="math notranslate nohighlight">\(\beta_i\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-logistic-regression">Example of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-logistic-regression-model">Evaluating the Logistic Regression Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-we-can-t-use-r-2">The Problem: We Can’t Use <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-pseudo-r-2">The Solution: “Pseudo <span class="math notranslate nohighlight">\(R^2\)</span>”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-evaluation">Predictive Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-in-python">Logistic Regression in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>