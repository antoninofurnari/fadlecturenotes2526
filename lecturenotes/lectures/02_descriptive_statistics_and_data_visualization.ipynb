{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics and Data Visualization\n",
    "\n",
    "In this comprehensive lecture, we will explore the fundamental concepts of descriptive statistics and data visualization. We'll cover everything from data collection methods to advanced visualization techniques, providing you with the essential tools for understanding and presenting data effectively.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Getting the Data](#getting-the-data)\n",
    "2. [Statistical Variables and Types of Data](#statistical-variables-and-types-of-data)\n",
    "3. [Datasets & Design Matrices](#datasets--design-matrices)\n",
    "4. [Descriptive Statistics](#descriptive-statistics)\n",
    "5. [Data Visualization](#data-visualization)\n",
    "6. [Data Cleaning](#data-cleaning)\n",
    "7. [Data Normalization](#data-normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the Data\n",
    "\n",
    "Data collection is the crucial first step in any data analysis process. There are several methods to obtain data, each with its own advantages and limitations.\n",
    "\n",
    "### 1.1 Surveys\n",
    "\n",
    "Surveys are structured questionnaires used to gather information from individuals or groups. They can be conducted through various means:\n",
    "- **Paper questionnaires**: Traditional method, good for controlled environments\n",
    "- **Online forms**: Cost-effective, wide reach, easy data collection\n",
    "- **Telephone interviews**: Personal interaction, higher response rates\n",
    "- **In-person interviews**: Most detailed responses, highest quality data\n",
    "\n",
    "**Key characteristics of surveys:**\n",
    "- Structured and standardized questions\n",
    "- Can be administered in various formats\n",
    "- Allow for quantifiable responses\n",
    "- Useful for collecting opinions, attitudes, and demographic information\n",
    "\n",
    "### 1.2 Experiments\n",
    "\n",
    "Experiments allow researchers to collect data in a **controlled** environment and are primarily used to establish cause-and-effect relationships between variables.\n",
    "\n",
    "**Randomized Controlled Experiments (RCTs)** are the gold standard:\n",
    "- **Controlled manipulation** of variables\n",
    "- **Random assignment** of participants to treatment groups\n",
    "- **Replicability** to test hypotheses\n",
    "- **Control groups** to isolate the effect of the treatment\n",
    "\n",
    "**Example**: Testing drug effectiveness\n",
    "- Population: People with a specific disease\n",
    "- Sample: Random selection of patients\n",
    "- Treatment: Half receive the drug, half receive placebo\n",
    "- Randomization: Assignment is random to avoid bias\n",
    "\n",
    "### 1.3 Observational Studies\n",
    "\n",
    "Observational data collection involves passive recording of information as it naturally occurs, without interference from the researcher.\n",
    "\n",
    "**Key characteristics:**\n",
    "- No direct manipulation of variables\n",
    "- Captures data as it naturally occurs\n",
    "- Useful for studying complex, uncontrolled environments\n",
    "- Often used when experiments are unethical or impractical\n",
    "\n",
    "**Example**: Studying the effects of smoking\n",
    "- Cannot ethically ask people to smoke\n",
    "- Observe existing smokers vs. non-smokers\n",
    "- Track health outcomes over time\n",
    "\n",
    "### 1.4 Data Collection Methods\n",
    "\n",
    "#### Scraping\n",
    "Web scraping involves automatically extracting data from websites:\n",
    "- **Advantages**: Large amounts of data, automated collection\n",
    "- **Challenges**: Legal considerations, changing website structures\n",
    "- **Tools**: BeautifulSoup, Scrapy, Selenium\n",
    "\n",
    "#### APIs (Application Programming Interfaces)\n",
    "APIs provide structured access to data from various services:\n",
    "- **Advantages**: Structured data, official access, real-time updates\n",
    "- **Examples**: Twitter API, Google Maps API, Weather APIs\n",
    "- **Considerations**: Rate limits, authentication requirements\n",
    "\n",
    "#### Public Datasets\n",
    "Many organizations provide free access to datasets:\n",
    "\n",
    "**Kaggle**: \n",
    "- Largest community of data scientists\n",
    "- Competitions with real-world datasets\n",
    "- Wide variety of domains (finance, healthcare, sports, etc.)\n",
    "\n",
    "**UCI Machine Learning Repository**:\n",
    "- Academic datasets for research\n",
    "- Well-documented and clean datasets\n",
    "- Classic datasets for learning and benchmarking\n",
    "\n",
    "**ISTAT (Italian National Institute of Statistics)**:\n",
    "- Official Italian government statistics\n",
    "- Demographic, economic, and social data\n",
    "- Reliable and authoritative source\n",
    "\n",
    "### 1.5 Sample vs Population\n",
    "\n",
    "Understanding the distinction between sample and population is crucial for proper statistical inference.\n",
    "\n",
    "**Population (Ω)**:\n",
    "- The complete set of all possible observations\n",
    "- Often theoretical or too large to study entirely\n",
    "- Examples: All people in the world, all possible coin tosses\n",
    "\n",
    "**Sample**:\n",
    "- A subset of the population: {ω⁽¹⁾, ω⁽²⁾, ..., ω⁽ⁿ⁾} ⊆ Ω\n",
    "- Practical and manageable size\n",
    "- Should be representative of the population\n",
    "\n",
    "**Key considerations**:\n",
    "- **Sampling bias**: When the sample doesn't represent the population\n",
    "- **Sample size**: Larger samples generally provide better estimates\n",
    "- **Sampling method**: Random sampling is preferred for generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading a dataset from different sources\n",
    "\n",
    "# 1. From a URL (simulating API or web data)\n",
    "titanic_url = 'https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv'\n",
    "titanic = pd.read_csv(titanic_url, index_col='PassengerId')\n",
    "\n",
    "print(\"Titanic Dataset Shape:\", titanic.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Variables and Types of Data\n",
    "\n",
    "Statistical variables are the building blocks of data analysis. Understanding different types of variables is essential for choosing appropriate analysis methods.\n",
    "\n",
    "### 2.1 Definition of Statistical Variables\n",
    "\n",
    "A statistical variable is a function that maps observations to values:\n",
    "\n",
    "**X : Ω → S**\n",
    "**ω ↦ x**\n",
    "\n",
    "Where:\n",
    "- Ω is the population\n",
    "- S is the set of possible values\n",
    "- ω is an observation\n",
    "- x is the value assigned to that observation\n",
    "\n",
    "### 2.2 Classification by Nature\n",
    "\n",
    "#### Quantitative Variables\n",
    "Represent properties that can be measured numerically:\n",
    "- **Examples**: Height, weight, age, income, temperature\n",
    "- **Operations**: Can perform arithmetic operations (addition, subtraction, etc.)\n",
    "- **Analysis**: Can calculate mean, standard deviation, correlation\n",
    "\n",
    "#### Qualitative (Categorical) Variables\n",
    "Represent properties that describe categories or qualities:\n",
    "- **Examples**: Gender, color, brand, nationality, education level\n",
    "- **Operations**: Cannot perform arithmetic operations\n",
    "- **Analysis**: Use frequencies, proportions, chi-square tests\n",
    "\n",
    "### 2.3 Classification by Continuity\n",
    "\n",
    "#### Discrete Variables\n",
    "- Can assume a **finite** or **countably infinite** number of values\n",
    "- Often result from counting\n",
    "- **Examples**: Number of children, number of cars, dice rolls\n",
    "- **Visualization**: Bar charts, frequency tables\n",
    "\n",
    "#### Continuous Variables\n",
    "- Can assume an **infinite** number of values within a range\n",
    "- Often result from measuring\n",
    "- **Examples**: Height, weight, time, temperature\n",
    "- **Visualization**: Histograms, density plots\n",
    "\n",
    "### 2.4 Scales of Measurement\n",
    "\n",
    "#### Nominal Scale\n",
    "- **No natural ordering** of categories\n",
    "- Categories are mutually exclusive\n",
    "- **Examples**: Gender (male, female), eye color (blue, brown, green)\n",
    "- **Operations**: Equality (=, ≠)\n",
    "- **Statistics**: Mode, frequencies\n",
    "\n",
    "#### Ordinal Scale\n",
    "- Categories have a **natural ordering**\n",
    "- Differences between categories are not necessarily equal\n",
    "- **Examples**: Education level (high school, bachelor's, master's), satisfaction rating (poor, fair, good, excellent)\n",
    "- **Operations**: Equality, ordering (<, >)\n",
    "- **Statistics**: Median, percentiles\n",
    "\n",
    "#### Interval Scale\n",
    "- **Equal intervals** between consecutive values\n",
    "- **No true zero** point\n",
    "- **Examples**: Temperature in Celsius, calendar years\n",
    "- **Operations**: Addition, subtraction\n",
    "- **Statistics**: Mean, standard deviation\n",
    "\n",
    "#### Ratio Scale\n",
    "- Equal intervals AND **true zero** point\n",
    "- **Examples**: Height, weight, age, income\n",
    "- **Operations**: All arithmetic operations\n",
    "- **Statistics**: All descriptive statistics, geometric mean\n",
    "\n",
    "### 2.5 Grouped Variables\n",
    "\n",
    "Sometimes continuous variables are **grouped** into categories for analysis:\n",
    "- **Age groups**: 0-18, 19-35, 36-65, 65+\n",
    "- **Income brackets**: Low, Medium, High\n",
    "- **Advantages**: Simplifies analysis, easier interpretation\n",
    "- **Disadvantages**: Loss of information, arbitrary boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Identifying variable types in the Titanic dataset\n",
    "\n",
    "print(\"Titanic Dataset Info:\")\n",
    "print(titanic.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Classify variables by type\n",
    "quantitative_vars = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "qualitative_vars = ['Survived', 'Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n",
    "\n",
    "print(\"\\nQuantitative Variables:\")\n",
    "for var in quantitative_vars:\n",
    "    if var in titanic.columns:\n",
    "        print(f\"- {var}: {titanic[var].dtype}\")\n",
    "\n",
    "print(\"\\nQualitative Variables:\")\n",
    "for var in qualitative_vars:\n",
    "    if var in titanic.columns:\n",
    "        print(f\"- {var}: {titanic[var].dtype}\")\n",
    "\n",
    "# Example of creating grouped variables\n",
    "titanic['Age_Group'] = pd.cut(titanic['Age'], \n",
    "                             bins=[0, 18, 35, 65, 100], \n",
    "                             labels=['Child', 'Young Adult', 'Adult', 'Senior'])\n",
    "\n",
    "print(\"\\nAge Groups Distribution:\")\n",
    "print(titanic['Age_Group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Datasets & Design Matrices\n",
    "\n",
    "### 3.1 Structure of Datasets\n",
    "\n",
    "A dataset is typically organized as a **design matrix** or **data matrix** where:\n",
    "- **Rows** represent observations (samples, instances)\n",
    "- **Columns** represent variables (features, attributes)\n",
    "- Each cell contains the value of a specific variable for a specific observation\n",
    "\n",
    "### 3.2 Long vs Wide Data\n",
    "\n",
    "Data can be organized in different formats depending on the analysis needs:\n",
    "\n",
    "#### Wide Format\n",
    "- Each variable has its own column\n",
    "- Each observation is a single row\n",
    "- **Advantages**: Easy to read, good for analysis\n",
    "- **Use cases**: Most statistical analyses, machine learning\n",
    "\n",
    "#### Long Format\n",
    "- Variables are stacked into fewer columns\n",
    "- Multiple rows per observation\n",
    "- **Advantages**: Efficient storage, good for certain visualizations\n",
    "- **Use cases**: Time series, repeated measures, some plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Wide vs Long format\n",
    "\n",
    "# Create sample data in wide format\n",
    "wide_data = pd.DataFrame({\n",
    "    'Student': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Math': [85, 92, 78],\n",
    "    'Science': [88, 85, 92],\n",
    "    'English': [92, 88, 85]\n",
    "})\n",
    "\n",
    "print(\"Wide Format:\")\n",
    "print(wide_data)\n",
    "\n",
    "# Convert to long format\n",
    "long_data = pd.melt(wide_data, \n",
    "                   id_vars=['Student'], \n",
    "                   var_name='Subject', \n",
    "                   value_name='Score')\n",
    "\n",
    "print(\"\\nLong Format:\")\n",
    "print(long_data)\n",
    "\n",
    "# Convert back to wide format\n",
    "wide_again = long_data.pivot(index='Student', columns='Subject', values='Score')\n",
    "print(\"\\nBack to Wide Format:\")\n",
    "print(wide_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Descriptive Statistics\n",
    "\n",
    "Descriptive statistics summarize and describe the main features of a dataset. They provide simple summaries about the sample and measures.\n",
    "\n",
    "### 4.1 Measures of Central Tendency\n",
    "\n",
    "Central tendency measures indicate where the center of a distribution lies.\n",
    "\n",
    "#### Mean (Arithmetic Average)\n",
    "The sum of all values divided by the number of values:\n",
    "\n",
    "**x̄ = (1/n) Σᵢ xᵢ**\n",
    "\n",
    "- **Advantages**: Uses all data points, mathematically tractable\n",
    "- **Disadvantages**: Sensitive to outliers\n",
    "- **Best for**: Symmetric distributions without extreme outliers\n",
    "\n",
    "#### Median\n",
    "The middle value when data is ordered from smallest to largest:\n",
    "\n",
    "**Median = x₍ₙ₊₁₎/₂** (if n is odd)\n",
    "**Median = (x₍ₙ/₂₎ + x₍ₙ/₂₊₁₎)/2** (if n is even)\n",
    "\n",
    "- **Advantages**: Robust to outliers, good for skewed distributions\n",
    "- **Disadvantages**: Doesn't use all information\n",
    "- **Best for**: Skewed distributions, ordinal data\n",
    "\n",
    "#### Mode\n",
    "The most frequently occurring value(s):\n",
    "\n",
    "- **Advantages**: Can be used with any type of data\n",
    "- **Disadvantages**: May not exist or may not be unique\n",
    "- **Best for**: Categorical data, finding most common value\n",
    "\n",
    "### 4.2 Measures of Spread (Dispersion)\n",
    "\n",
    "Dispersion measures indicate how spread out the data values are.\n",
    "\n",
    "#### Range\n",
    "The difference between the maximum and minimum values:\n",
    "\n",
    "**Range = max(x) - min(x)**\n",
    "\n",
    "- **Advantages**: Simple to calculate and understand\n",
    "- **Disadvantages**: Very sensitive to outliers\n",
    "\n",
    "#### Variance\n",
    "The average of squared deviations from the mean:\n",
    "\n",
    "**s² = Σᵢ(xᵢ - x̄)² / (n-1)** (sample variance)\n",
    "\n",
    "- **Advantages**: Uses all data points, mathematically useful\n",
    "- **Disadvantages**: Units are squared, sensitive to outliers\n",
    "\n",
    "#### Standard Deviation\n",
    "The square root of variance:\n",
    "\n",
    "**s = √(s²)**\n",
    "\n",
    "- **Advantages**: Same units as original data, interpretable\n",
    "- **Disadvantages**: Sensitive to outliers\n",
    "\n",
    "#### Interquartile Range (IQR)\n",
    "The difference between the 75th and 25th percentiles:\n",
    "\n",
    "**IQR = Q₃ - Q₁**\n",
    "\n",
    "- **Advantages**: Robust to outliers\n",
    "- **Disadvantages**: Doesn't use all data\n",
    "\n",
    "### 4.3 Quantiles, Quartiles, and Percentiles\n",
    "\n",
    "#### Quantiles\n",
    "Values that divide the dataset into equal-sized groups:\n",
    "- **q-quantile of order α**: A value that divides data so that α proportion is below it\n",
    "- **Interpretation**: If qₐ = x, then α×n observations have values ≤ x\n",
    "\n",
    "#### Percentiles\n",
    "Quantiles expressed as percentages (0-100%)\n",
    "\n",
    "#### Quartiles\n",
    "Specific quantiles that divide data into four equal parts:\n",
    "- **Q₀** (0th quartile): Minimum value\n",
    "- **Q₁** (1st quartile): 25th percentile\n",
    "- **Q₂** (2nd quartile): 50th percentile (median)\n",
    "- **Q₃** (3rd quartile): 75th percentile\n",
    "- **Q₄** (4th quartile): Maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculating descriptive statistics\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "sample_data = np.random.normal(100, 15, 1000)  # Normal distribution, mean=100, std=15\n",
    "\n",
    "# Add some outliers\n",
    "outliers = [150, 160, 40, 30]\n",
    "data_with_outliers = np.concatenate([sample_data, outliers])\n",
    "\n",
    "# Calculate measures of central tendency\n",
    "print(\"MEASURES OF CENTRAL TENDENCY\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Mean: {np.mean(data_with_outliers):.2f}\")\n",
    "print(f\"Median: {np.median(data_with_outliers):.2f}\")\n",
    "print(f\"Mode: {stats.mode(data_with_outliers.round()).mode:.2f}\")\n",
    "\n",
    "# Calculate measures of dispersion\n",
    "print(\"\\nMEASURES OF DISPERSION\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Range: {np.ptp(data_with_outliers):.2f}\")\n",
    "print(f\"Variance: {np.var(data_with_outliers, ddof=1):.2f}\")\n",
    "print(f\"Standard Deviation: {np.std(data_with_outliers, ddof=1):.2f}\")\n",
    "print(f\"IQR: {np.percentile(data_with_outliers, 75) - np.percentile(data_with_outliers, 25):.2f}\")\n",
    "\n",
    "# Calculate quartiles and percentiles\n",
    "print(\"\\nQUARTILES AND PERCENTILES\")\n",
    "print(\"=\"*40)\n",
    "quartiles = np.percentile(data_with_outliers, [0, 25, 50, 75, 100])\n",
    "print(f\"Q0 (Min): {quartiles[0]:.2f}\")\n",
    "print(f\"Q1 (25th): {quartiles[1]:.2f}\")\n",
    "print(f\"Q2 (Median): {quartiles[2]:.2f}\")\n",
    "print(f\"Q3 (75th): {quartiles[3]:.2f}\")\n",
    "print(f\"Q4 (Max): {quartiles[4]:.2f}\")\n",
    "\n",
    "# Compare with and without outliers\n",
    "print(\"\\nIMPACT OF OUTLIERS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Mean without outliers: {np.mean(sample_data):.2f}\")\n",
    "print(f\"Mean with outliers: {np.mean(data_with_outliers):.2f}\")\n",
    "print(f\"Median without outliers: {np.median(sample_data):.2f}\")\n",
    "print(f\"Median with outliers: {np.median(data_with_outliers):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for Titanic dataset\n",
    "print(\"TITANIC DATASET - DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Numerical variables\n",
    "numerical_stats = titanic[['Age', 'SibSp', 'Parch', 'Fare']].describe()\n",
    "print(\"\\nNumerical Variables:\")\n",
    "print(numerical_stats)\n",
    "\n",
    "# Categorical variables\n",
    "print(\"\\nCategorical Variables:\")\n",
    "print(\"\\nSurvival Rate:\")\n",
    "print(titanic['Survived'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nPassenger Class Distribution:\")\n",
    "print(titanic['Pclass'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nGender Distribution:\")\n",
    "print(titanic['Sex'].value_counts(normalize=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization\n",
    "\n",
    "Data visualization is the graphical representation of information and data. It provides an accessible way to see and understand trends, outliers, and patterns in data.\n",
    "\n",
    "### 5.1 Histograms\n",
    "\n",
    "Histograms show the distribution of a continuous variable by dividing the data into bins and displaying the frequency of observations in each bin.\n",
    "\n",
    "**When to use:**\n",
    "- Continuous variables\n",
    "- Understanding distribution shape\n",
    "- Identifying outliers and skewness\n",
    "\n",
    "**Key considerations:**\n",
    "- Number of bins affects interpretation\n",
    "- Bin width should be appropriate for the data\n",
    "- Can reveal multimodal distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms examples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Histogram Examples', fontsize=16)\n",
    "\n",
    "# Age distribution\n",
    "axes[0,0].hist(titanic['Age'].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].set_title('Age Distribution')\n",
    "axes[0,0].set_xlabel('Age')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fare distribution (with outliers)\n",
    "axes[0,1].hist(titanic['Fare'].dropna(), bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[0,1].set_title('Fare Distribution (with outliers)')\n",
    "axes[0,1].set_xlabel('Fare')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Fare distribution (without extreme outliers)\n",
    "fare_filtered = titanic[titanic['Fare'] < 200]['Fare']\n",
    "axes[1,0].hist(fare_filtered, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1,0].set_title('Fare Distribution (Fare < 200)')\n",
    "axes[1,0].set_xlabel('Fare')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Different bin sizes comparison\n",
    "axes[1,1].hist(titanic['Age'].dropna(), bins=10, alpha=0.5, color='purple', label='10 bins')\n",
    "axes[1,1].hist(titanic['Age'].dropna(), bins=50, alpha=0.5, color='orange', label='50 bins')\n",
    "axes[1,1].set_title('Effect of Bin Size on Age Distribution')\n",
    "axes[1,1].set_xlabel('Age')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bar Charts\n",
    "\n",
    "Bar charts display categorical data with rectangular bars whose heights correspond to the values they represent.\n",
    "\n",
    "**When to use:**\n",
    "- Categorical variables\n",
    "- Comparing frequencies or proportions\n",
    "- Showing absolute or relative frequencies\n",
    "\n",
    "**Types:**\n",
    "- **Vertical bars**: Standard bar chart\n",
    "- **Horizontal bars**: Good for long category names\n",
    "- **Grouped bars**: Comparing multiple categories\n",
    "- **Stacked bars**: Showing composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart examples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Bar Chart Examples', fontsize=16)\n",
    "\n",
    "# Simple bar chart - Passenger class\n",
    "class_counts = titanic['Pclass'].value_counts().sort_index()\n",
    "axes[0,0].bar(class_counts.index, class_counts.values, color=['gold', 'silver', 'bronze'])\n",
    "axes[0,0].set_title('Passengers by Class')\n",
    "axes[0,0].set_xlabel('Passenger Class')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Horizontal bar chart - Embarked port\n",
    "embarked_counts = titanic['Embarked'].value_counts()\n",
    "axes[0,1].barh(embarked_counts.index, embarked_counts.values, color='lightblue')\n",
    "axes[0,1].set_title('Passengers by Embarkation Port')\n",
    "axes[0,1].set_xlabel('Count')\n",
    "axes[0,1].set_ylabel('Port')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Grouped bar chart - Survival by gender\n",
    "survival_gender = pd.crosstab(titanic['Sex'], titanic['Survived'])\n",
    "survival_gender.plot(kind='bar', ax=axes[1,0], color=['red', 'green'])\n",
    "axes[1,0].set_title('Survival by Gender')\n",
    "axes[1,0].set_xlabel('Gender')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].legend(['Did not survive', 'Survived'])\n",
    "axes[1,0].tick_params(axis='x', rotation=0)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Stacked bar chart - Survival by class\n",
    "survival_class = pd.crosstab(titanic['Pclass'], titanic['Survived'])\n",
    "survival_class.plot(kind='bar', stacked=True, ax=axes[1,1], color=['red', 'green'])\n",
    "axes[1,1].set_title('Survival by Class (Stacked)')\n",
    "axes[1,1].set_xlabel('Passenger Class')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].legend(['Did not survive', 'Survived'])\n",
    "axes[1,1].tick_params(axis='x', rotation=0)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Box Plots\n",
    "\n",
    "Box plots (box-and-whisker plots) display the distribution of data through quartiles and identify outliers.\n",
    "\n",
    "**Components:**\n",
    "- **Box**: Represents the interquartile range (IQR)\n",
    "- **Median line**: Line inside the box\n",
    "- **Whiskers**: Extend to the furthest points within 1.5×IQR\n",
    "- **Outliers**: Points beyond the whiskers\n",
    "\n",
    "**When to use:**\n",
    "- Comparing distributions across groups\n",
    "- Identifying outliers\n",
    "- Understanding data spread and skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot examples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Box Plot Examples', fontsize=16)\n",
    "\n",
    "# Simple box plot - Age distribution\n",
    "axes[0,0].boxplot(titanic['Age'].dropna(), patch_artist=True, \n",
    "                  boxprops=dict(facecolor='lightblue'))\n",
    "axes[0,0].set_title('Age Distribution')\n",
    "axes[0,0].set_ylabel('Age')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot by group - Age by passenger class\n",
    "age_by_class = [titanic[titanic['Pclass']==i]['Age'].dropna() for i in [1,2,3]]\n",
    "bp1 = axes[0,1].boxplot(age_by_class, patch_artist=True, labels=['1st', '2nd', '3rd'])\n",
    "colors = ['gold', 'silver', 'bronze']\n",
    "for patch, color in zip(bp1['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "axes[0,1].set_title('Age Distribution by Passenger Class')\n",
    "axes[0,1].set_xlabel('Passenger Class')\n",
    "axes[0,1].set_ylabel('Age')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot - Fare by survival\n",
    "fare_by_survival = [titanic[titanic['Survived']==i]['Fare'].dropna() for i in [0,1]]\n",
    "bp2 = axes[1,0].boxplot(fare_by_survival, patch_artist=True, labels=['Did not survive', 'Survived'])\n",
    "colors = ['red', 'green']\n",
    "for patch, color in zip(bp2['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1,0].set_title('Fare Distribution by Survival')\n",
    "axes[1,0].set_xlabel('Survival Status')\n",
    "axes[1,0].set_ylabel('Fare')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Multiple box plots - Age by gender and survival\n",
    "groups = [(titanic[(titanic['Sex']=='male') & (titanic['Survived']==0)]['Age'].dropna(), 'Male, Died'),\n",
    "          (titanic[(titanic['Sex']=='male') & (titanic['Survived']==1)]['Age'].dropna(), 'Male, Survived'),\n",
    "          (titanic[(titanic['Sex']=='female') & (titanic['Survived']==0)]['Age'].dropna(), 'Female, Died'),\n",
    "          (titanic[(titanic['Sex']=='female') & (titanic['Survived']==1)]['Age'].dropna(), 'Female, Survived')]\n",
    "\n",
    "data_groups = [group[0] for group in groups]\n",
    "labels = [group[1] for group in groups]\n",
    "\n",
    "bp3 = axes[1,1].boxplot(data_groups, patch_artist=True, labels=labels)\n",
    "colors = ['darkred', 'lightcoral', 'darkblue', 'lightblue']\n",
    "for patch, color in zip(bp3['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1,1].set_title('Age by Gender and Survival')\n",
    "axes[1,1].set_ylabel('Age')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Scatter Plots & Scatter Matrices\n",
    "\n",
    "Scatter plots show the relationship between two continuous variables by plotting points in a coordinate system.\n",
    "\n",
    "**When to use:**\n",
    "- Exploring relationships between variables\n",
    "- Identifying correlations\n",
    "- Detecting outliers\n",
    "- Understanding data patterns\n",
    "\n",
    "**Scatter matrices** show pairwise relationships between multiple variables in a single visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot examples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Scatter Plot Examples', fontsize=16)\n",
    "\n",
    "# Simple scatter plot - Age vs Fare\n",
    "axes[0,0].scatter(titanic['Age'], titanic['Fare'], alpha=0.6, color='blue')\n",
    "axes[0,0].set_title('Age vs Fare')\n",
    "axes[0,0].set_xlabel('Age')\n",
    "axes[0,0].set_ylabel('Fare')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Colored scatter plot - Age vs Fare by survival\n",
    "survived = titanic[titanic['Survived'] == 1]\n",
    "not_survived = titanic[titanic['Survived'] == 0]\n",
    "\n",
    "axes[0,1].scatter(not_survived['Age'], not_survived['Fare'], \n",
    "                  alpha=0.6, color='red', label='Did not survive')\n",
    "axes[0,1].scatter(survived['Age'], survived['Fare'], \n",
    "                  alpha=0.6, color='green', label='Survived')\n",
    "axes[0,1].set_title('Age vs Fare by Survival')\n",
    "axes[0,1].set_xlabel('Age')\n",
    "axes[0,1].set_ylabel('Fare')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot with size - Age vs Fare, size by SibSp\n",
    "sizes = (titanic['SibSp'] + 1) * 20  # Add 1 to avoid zero size\n",
    "scatter = axes[1,0].scatter(titanic['Age'], titanic['Fare'], \n",
    "                           s=sizes, alpha=0.6, c=titanic['Pclass'], \n",
    "                           cmap='viridis')\n",
    "axes[1,0].set_title('Age vs Fare (size=SibSp, color=Class)')\n",
    "axes[1,0].set_xlabel('Age')\n",
    "axes[1,0].set_ylabel('Fare')\n",
    "plt.colorbar(scatter, ax=axes[1,0], label='Passenger Class')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Create a simple scatter matrix for numerical variables\n",
    "from pandas.plotting import scatter_matrix\n",
    "numerical_vars = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "scatter_data = titanic[numerical_vars].dropna()\n",
    "\n",
    "# Since we can't easily fit a full scatter matrix in one subplot, \n",
    "# let's show a correlation heatmap instead\n",
    "correlation_matrix = scatter_data.corr()\n",
    "im = axes[1,1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
    "axes[1,1].set_xticks(range(len(numerical_vars)))\n",
    "axes[1,1].set_yticks(range(len(numerical_vars)))\n",
    "axes[1,1].set_xticklabels(numerical_vars)\n",
    "axes[1,1].set_yticklabels(numerical_vars)\n",
    "axes[1,1].set_title('Correlation Matrix')\n",
    "\n",
    "# Add correlation values to the heatmap\n",
    "for i in range(len(numerical_vars)):\n",
    "    for j in range(len(numerical_vars)):\n",
    "        axes[1,1].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', \n",
    "                       ha='center', va='center', color='black')\n",
    "\n",
    "plt.colorbar(im, ax=axes[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full scatter matrix using pandas\n",
    "fig, axes = plt.subplots(figsize=(12, 10))\n",
    "scatter_matrix(scatter_data, alpha=0.6, figsize=(12, 10), diagonal='hist')\n",
    "plt.suptitle('Scatter Matrix of Numerical Variables', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Heatmaps\n",
    "\n",
    "Heatmaps use color to represent data values in a matrix format.\n",
    "\n",
    "**When to use:**\n",
    "- Correlation matrices\n",
    "- Confusion matrices\n",
    "- Time series data\n",
    "- Any matrix-like data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap examples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Heatmap Examples', fontsize=16)\n",
    "\n",
    "# Correlation heatmap\n",
    "correlation_matrix = titanic[['Age', 'SibSp', 'Parch', 'Fare', 'Survived']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[0])\n",
    "axes[0].set_title('Correlation Matrix')\n",
    "\n",
    "# Survival rate by class and gender\n",
    "survival_pivot = titanic.pivot_table(values='Survived', index='Pclass', \n",
    "                                    columns='Sex', aggfunc='mean')\n",
    "sns.heatmap(survival_pivot, annot=True, cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Survival Rate'}, ax=axes[1])\n",
    "axes[1].set_title('Survival Rate by Class and Gender')\n",
    "\n",
    "# Average fare by class and embarkation port\n",
    "fare_pivot = titanic.pivot_table(values='Fare', index='Pclass', \n",
    "                                columns='Embarked', aggfunc='mean')\n",
    "sns.heatmap(fare_pivot, annot=True, cmap='Blues', fmt='.1f',\n",
    "            cbar_kws={'label': 'Average Fare'}, ax=axes[2])\n",
    "axes[2].set_title('Average Fare by Class and Port')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Pie Charts\n",
    "\n",
    "Pie charts show the composition of a whole by displaying the relative sizes of parts.\n",
    "\n",
    "**When to use:**\n",
    "- Showing proportions of a whole\n",
    "- Limited number of categories (ideally ≤ 7)\n",
    "- When exact values are less important than proportions\n",
    "\n",
    "**Limitations:**\n",
    "- Difficult to compare similar-sized segments\n",
    "- Not suitable for many categories\n",
    "- Can be misleading with 3D effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart examples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Pie Chart Examples', fontsize=16)\n",
    "\n",
    "# Simple pie chart - Passenger class distribution\n",
    "class_counts = titanic['Pclass'].value_counts().sort_index()\n",
    "axes[0,0].pie(class_counts.values, labels=[f'Class {i}' for i in class_counts.index], \n",
    "              autopct='%1.1f%%', colors=['gold', 'silver', 'bronze'])\n",
    "axes[0,0].set_title('Passenger Class Distribution')\n",
    "\n",
    "# Pie chart with explosion - Survival distribution\n",
    "survival_counts = titanic['Survived'].value_counts()\n",
    "labels = ['Did not survive', 'Survived']\n",
    "colors = ['red', 'green']\n",
    "explode = (0.1, 0)  # explode the first slice\n",
    "axes[0,1].pie(survival_counts.values, labels=labels, autopct='%1.1f%%', \n",
    "              colors=colors, explode=explode, shadow=True)\n",
    "axes[0,1].set_title('Survival Distribution')\n",
    "\n",
    "# Gender distribution\n",
    "gender_counts = titanic['Sex'].value_counts()\n",
    "axes[1,0].pie(gender_counts.values, labels=gender_counts.index, \n",
    "              autopct='%1.1f%%', colors=['lightblue', 'pink'])\n",
    "axes[1,0].set_title('Gender Distribution')\n",
    "\n",
    "# Embarkation port distribution\n",
    "embarked_counts = titanic['Embarked'].value_counts()\n",
    "port_names = {'S': 'Southampton', 'C': 'Cherbourg', 'Q': 'Queenstown'}\n",
    "labels = [port_names.get(port, port) for port in embarked_counts.index]\n",
    "axes[1,1].pie(embarked_counts.values, labels=labels, autopct='%1.1f%%')\n",
    "axes[1,1].set_title('Embarkation Port Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Advanced Visualizations Gallery\n",
    "\n",
    "Here are some more advanced visualization techniques that can provide deeper insights into your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Advanced Visualization Gallery', fontsize=16)\n",
    "\n",
    "# 1. Violin plot - Age distribution by class\n",
    "age_by_class_data = [titanic[titanic['Pclass']==i]['Age'].dropna() for i in [1,2,3]]\n",
    "parts = axes[0,0].violinplot(age_by_class_data, positions=[1,2,3], showmeans=True)\n",
    "axes[0,0].set_xticks([1,2,3])\n",
    "axes[0,0].set_xticklabels(['1st', '2nd', '3rd'])\n",
    "axes[0,0].set_title('Age Distribution by Class (Violin Plot)')\n",
    "axes[0,0].set_xlabel('Passenger Class')\n",
    "axes[0,0].set_ylabel('Age')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Density plot - Age distribution\n",
    "titanic['Age'].dropna().plot.density(ax=axes[0,1], color='blue', alpha=0.7)\n",
    "axes[0,1].set_title('Age Density Plot')\n",
    "axes[0,1].set_xlabel('Age')\n",
    "axes[0,1].set_ylabel('Density')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Stacked area chart - Survival by age groups\n",
    "age_groups = pd.cut(titanic['Age'], bins=[0, 18, 35, 65, 100], \n",
    "                   labels=['Child', 'Young Adult', 'Adult', 'Senior'])\n",
    "survival_age = pd.crosstab(age_groups, titanic['Survived'])\n",
    "survival_age.plot.area(ax=axes[0,2], alpha=0.7, color=['red', 'green'])\n",
    "axes[0,2].set_title('Survival by Age Group (Stacked Area)')\n",
    "axes[0,2].set_xlabel('Age Group')\n",
    "axes[0,2].set_ylabel('Count')\n",
    "axes[0,2].legend(['Did not survive', 'Survived'])\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Hexbin plot - Age vs Fare\n",
    "clean_data = titanic[['Age', 'Fare']].dropna()\n",
    "hb = axes[1,0].hexbin(clean_data['Age'], clean_data['Fare'], gridsize=20, cmap='Blues')\n",
    "axes[1,0].set_title('Age vs Fare (Hexbin Plot)')\n",
    "axes[1,0].set_xlabel('Age')\n",
    "axes[1,0].set_ylabel('Fare')\n",
    "plt.colorbar(hb, ax=axes[1,0])\n",
    "\n",
    "# 5. Parallel coordinates plot (simplified)\n",
    "# Normalize the data for better visualization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "numeric_cols = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "plot_data = titanic[numeric_cols + ['Survived']].dropna()\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(plot_data[numeric_cols])\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=numeric_cols)\n",
    "scaled_df['Survived'] = plot_data['Survived'].values\n",
    "\n",
    "# Sample data for cleaner visualization\n",
    "sample_data = scaled_df.sample(100, random_state=42)\n",
    "\n",
    "for idx, row in sample_data.iterrows():\n",
    "    color = 'green' if row['Survived'] == 1 else 'red'\n",
    "    axes[1,1].plot(range(len(numeric_cols)), row[numeric_cols], \n",
    "                   color=color, alpha=0.3, linewidth=0.5)\n",
    "\n",
    "axes[1,1].set_xticks(range(len(numeric_cols)))\n",
    "axes[1,1].set_xticklabels(numeric_cols)\n",
    "axes[1,1].set_title('Parallel Coordinates (Sample)')\n",
    "axes[1,1].set_ylabel('Standardized Values')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Radar chart - Average values by survival\n",
    "from math import pi\n",
    "\n",
    "# Calculate means for survivors and non-survivors\n",
    "survivors_mean = titanic[titanic['Survived']==1][numeric_cols].mean()\n",
    "non_survivors_mean = titanic[titanic['Survived']==0][numeric_cols].mean()\n",
    "\n",
    "# Normalize for radar chart\n",
    "max_vals = titanic[numeric_cols].max()\n",
    "survivors_norm = survivors_mean / max_vals\n",
    "non_survivors_norm = non_survivors_mean / max_vals\n",
    "\n",
    "# Number of variables\n",
    "N = len(numeric_cols)\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "# Add values to complete the circle\n",
    "survivors_values = survivors_norm.tolist() + [survivors_norm.iloc[0]]\n",
    "non_survivors_values = non_survivors_norm.tolist() + [non_survivors_norm.iloc[0]]\n",
    "\n",
    "axes[1,2] = plt.subplot(2, 3, 6, projection='polar')\n",
    "axes[1,2].plot(angles, survivors_values, 'o-', linewidth=2, label='Survived', color='green')\n",
    "axes[1,2].fill(angles, survivors_values, alpha=0.25, color='green')\n",
    "axes[1,2].plot(angles, non_survivors_values, 'o-', linewidth=2, label='Did not survive', color='red')\n",
    "axes[1,2].fill(angles, non_survivors_values, alpha=0.25, color='red')\n",
    "\n",
    "axes[1,2].set_xticks(angles[:-1])\n",
    "axes[1,2].set_xticklabels(numeric_cols)\n",
    "axes[1,2].set_title('Radar Chart - Average Values by Survival')\n",
    "axes[1,2].legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  {
  
 "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning\n",
    "\n",
    "Data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset. Real-world data is often messy and requires preprocessing before analysis.\n",
    "\n",
    "### 6.1 Missing Values\n",
    "\n",
    "Missing values are a common problem in datasets and can occur for various reasons:\n",
    "- **Data collection errors**\n",
    "- **Equipment malfunction**\n",
    "- **Survey non-response**\n",
    "- **Data entry mistakes**\n",
    "\n",
    "#### Identifying Missing Values\n",
    "Missing values can be represented as:\n",
    "- `NaN` (Not a Number)\n",
    "- `None`\n",
    "- Empty strings\n",
    "- Special codes (e.g., -999, 'N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing missing values in the Titanic dataset\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Count missing values\n",
    "missing_counts = titanic.isnull().sum()\n",
    "missing_percentages = (missing_counts / len(titanic)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing Percentage': missing_percentages\n",
    "})\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))\n",
    "\n",
    "# Visualize missing values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Missing values heatmap\n",
    "sns.heatmap(titanic.isnull(), cbar=True, yticklabels=False, \n",
    "            cmap='viridis', ax=axes[0])\n",
    "axes[0].set_title('Missing Values Heatmap')\n",
    "\n",
    "# Missing values bar chart\n",
    "missing_counts[missing_counts > 0].plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Missing Values Count')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategies for Handling Missing Values\n",
    "\n",
    "##### 1. Drop Missing Values\n",
    "Remove rows or columns with missing values.\n",
    "\n",
    "**When to use:**\n",
    "- Small percentage of missing data\n",
    "- Missing completely at random\n",
    "- Large dataset where losing some data is acceptable\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and fast\n",
    "- No assumptions about missing data\n",
    "\n",
    "**Disadvantages:**\n",
    "- Loss of information\n",
    "- May introduce bias if data is not missing at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping missing values examples\n",
    "print(\"DROPPING MISSING VALUES\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(f\"Original dataset shape: {titanic.shape}\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "titanic_dropna_any = titanic.dropna()\n",
    "print(f\"After dropping rows with any NaN: {titanic_dropna_any.shape}\")\n",
    "\n",
    "# Drop rows with missing values in specific columns\n",
    "titanic_dropna_age = titanic.dropna(subset=['Age'])\n",
    "print(f\"After dropping rows with missing Age: {titanic_dropna_age.shape}\")\n",
    "\n",
    "# Drop columns with high percentage of missing values\n",
    "threshold = 0.5  # Drop columns with more than 50% missing\n",
    "titanic_drop_cols = titanic.dropna(axis=1, thresh=int(threshold * len(titanic)))\n",
    "print(f\"After dropping columns with >50% missing: {titanic_drop_cols.shape}\")\n",
    "print(f\"Dropped columns: {set(titanic.columns) - set(titanic_drop_cols.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Imputation Methods\n",
    "\n",
    "Replace missing values with estimated values.\n",
    "\n",
    "###### Mean/Median/Mode Imputation\n",
    "- **Mean**: For normally distributed numerical data\n",
    "- **Median**: For skewed numerical data or when outliers are present\n",
    "- **Mode**: For categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation examples\n",
    "print(\"IMPUTATION METHODS\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Create a copy for imputation\n",
    "titanic_imputed = titanic.copy()\n",
    "\n",
    "# Mean imputation for Age\n",
    "age_mean = titanic_imputed['Age'].mean()\n",
    "titanic_imputed['Age_mean_imputed'] = titanic_imputed['Age'].fillna(age_mean)\n",
    "\n",
    "# Median imputation for Age\n",
    "age_median = titanic_imputed['Age'].median()\n",
    "titanic_imputed['Age_median_imputed'] = titanic_imputed['Age'].fillna(age_median)\n",
    "\n",
    "# Mode imputation for Embarked\n",
    "embarked_mode = titanic_imputed['Embarked'].mode()[0]\n",
    "titanic_imputed['Embarked_mode_imputed'] = titanic_imputed['Embarked'].fillna(embarked_mode)\n",
    "\n",
    "print(f\"Age mean: {age_mean:.2f}\")\n",
    "print(f\"Age median: {age_median:.2f}\")\n",
    "print(f\"Embarked mode: {embarked_mode}\")\n",
    "\n",
    "# Compare distributions before and after imputation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Impact of Imputation on Distributions', fontsize=16)\n",
    "\n",
    "# Age distribution comparison\n",
    "axes[0,0].hist(titanic['Age'].dropna(), bins=30, alpha=0.7, label='Original', color='blue')\n",
    "axes[0,0].hist(titanic_imputed['Age_mean_imputed'], bins=30, alpha=0.7, label='Mean Imputed', color='red')\n",
    "axes[0,0].set_title('Age Distribution: Original vs Mean Imputed')\n",
    "axes[0,0].set_xlabel('Age')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0,1].hist(titanic['Age'].dropna(), bins=30, alpha=0.7, label='Original', color='blue')\n",
    "axes[0,1].hist(titanic_imputed['Age_median_imputed'], bins=30, alpha=0.7, label='Median Imputed', color='green')\n",
    "axes[0,1].set_title('Age Distribution: Original vs Median Imputed')\n",
    "axes[0,1].set_xlabel('Age')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plots to show the effect\n",
    "age_data = [titanic['Age'].dropna(), \n",
    "           titanic_imputed['Age_mean_imputed'], \n",
    "           titanic_imputed['Age_median_imputed']]\n",
    "bp = axes[1,0].boxplot(age_data, labels=['Original', 'Mean Imputed', 'Median Imputed'], patch_artist=True)\n",
    "colors = ['blue', 'red', 'green']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "axes[1,0].set_title('Age Distribution Comparison (Box Plots)')\n",
    "axes[1,0].set_ylabel('Age')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Embarked distribution\n",
    "embarked_original = titanic['Embarked'].value_counts()\n",
    "embarked_imputed = titanic_imputed['Embarked_mode_imputed'].value_counts()\n",
    "\n",
    "x = range(len(embarked_original))\n",
    "width = 0.35\n",
    "axes[1,1].bar([i - width/2 for i in x], embarked_original.values, width, \n",
    "              label='Original', alpha=0.7, color='blue')\n",
    "axes[1,1].bar([i + width/2 for i in x], embarked_imputed.values, width, \n",
    "              label='Mode Imputed', alpha=0.7, color='orange')\n",
    "axes[1,1].set_title('Embarked Distribution: Original vs Mode Imputed')\n",
    "axes[1,1].set_xlabel('Embarked Port')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].set_xticks(x)\n",
    "axes[1,1].set_xticklabels(embarked_original.index)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Duplicates\n",
    "\n",
    "Duplicate records can skew analysis results and should be identified and handled appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling duplicates\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Total rows: {len(titanic)}\")\n",
    "print(f\"Duplicate rows: {titanic.duplicated().sum()}\")\n",
    "\n",
    "# Check for duplicates based on specific columns\n",
    "name_duplicates = titanic.duplicated(subset=['Name']).sum()\n",
    "print(f\"Duplicate names: {name_duplicates}\")\n",
    "\n",
    "# Create some artificial duplicates for demonstration\n",
    "titanic_with_dups = pd.concat([titanic, titanic.head(10)], ignore_index=True)\n",
    "print(f\"\\nAfter adding duplicates:\")\n",
    "print(f\"Total rows: {len(titanic_with_dups)}\")\n",
    "print(f\"Duplicate rows: {titanic_with_dups.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates\n",
    "titanic_no_dups = titanic_with_dups.drop_duplicates()\n",
    "print(f\"\\nAfter removing duplicates:\")\n",
    "print(f\"Total rows: {len(titanic_no_dups)}\")\n",
    "\n",
    "# Keep first, last, or False (remove all duplicates)\n",
    "print(f\"\\nDifferent strategies:\")\n",
    "print(f\"Keep first: {len(titanic_with_dups.drop_duplicates(keep='first'))} rows\")\n",
    "print(f\"Keep last: {len(titanic_with_dups.drop_duplicates(keep='last'))} rows\")\n",
    "print(f\"Remove all: {len(titanic_with_dups.drop_duplicates(keep=False))} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Outliers\n",
    "\n",
    "Outliers are data points that are significantly different from other observations. They can be:\n",
    "- **Legitimate extreme values**\n",
    "- **Data entry errors**\n",
    "- **Measurement errors**\n",
    "\n",
    "#### Methods for Detecting Outliers\n",
    "\n",
    "##### 1. Statistical Methods\n",
    "- **Z-score**: Values with |z| > 3 are often considered outliers\n",
    "- **IQR method**: Values outside Q1 - 1.5×IQR or Q3 + 1.5×IQR\n",
    "\n",
    "##### 2. Visual Methods\n",
    "- **Box plots**: Show outliers as individual points\n",
    "- **Scatter plots**: Identify unusual combinations of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection and handling\n",
    "print(\"OUTLIER DETECTION\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Focus on Fare column which likely has outliers\n",
    "fare_data = titanic['Fare'].dropna()\n",
    "\n",
    "# Method 1: Z-score\n",
    "z_scores = np.abs(stats.zscore(fare_data))\n",
    "z_outliers = fare_data[z_scores > 3]\n",
    "print(f\"Outliers using Z-score (|z| > 3): {len(z_outliers)}\")\n",
    "print(f\"Z-score outlier values: {sorted(z_outliers.values)[:10]}...\")  # Show first 10\n",
    "\n",
    "# Method 2: IQR method\n",
    "Q1 = fare_data.quantile(0.25)\n",
    "Q3 = fare_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "iqr_outliers = fare_data[(fare_data < lower_bound) | (fare_data > upper_bound)]\n",
    "print(f\"\\nOutliers using IQR method: {len(iqr_outliers)}\")\n",
    "print(f\"IQR bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "print(f\"IQR outlier values: {sorted(iqr_outliers.values)[:10]}...\")  # Show first 10\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Outlier Detection Visualization', fontsize=16)\n",
    "\n",
    "# Box plot showing outliers\n",
    "axes[0,0].boxplot(fare_data, patch_artist=True, \n",
    "                  boxprops=dict(facecolor='lightblue'))\n",
    "axes[0,0].set_title('Fare Distribution (Box Plot)')\n",
    "axes[0,0].set_ylabel('Fare')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram with outliers highlighted\n",
    "axes[0,1].hist(fare_data, bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "axes[0,1].axvline(upper_bound, color='red', linestyle='--', label=f'Upper bound ({upper_bound:.1f})')\n",
    "axes[0,1].set_title('Fare Distribution with IQR Bounds')\n",
    "axes[0,1].set_xlabel('Fare')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Age vs Fare with outliers highlighted\n",
    "clean_data = titanic[['Age', 'Fare']].dropna()\n",
    "fare_outlier_mask = (clean_data['Fare'] < lower_bound) | (clean_data['Fare'] > upper_bound)\n",
    "\n",
    "axes[1,0].scatter(clean_data[~fare_outlier_mask]['Age'], \n",
    "                  clean_data[~fare_outlier_mask]['Fare'], \n",
    "                  alpha=0.6, color='blue', label='Normal')\n",
    "axes[1,0].scatter(clean_data[fare_outlier_mask]['Age'], \n",
    "                  clean_data[fare_outlier_mask]['Fare'], \n",
    "                  alpha=0.8, color='red', label='Outliers', s=50)\n",
    "axes[1,0].set_title('Age vs Fare (Outliers Highlighted)')\n",
    "axes[1,0].set_xlabel('Age')\n",
    "axes[1,0].set_ylabel('Fare')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution without outliers\n",
    "fare_no_outliers = fare_data[(fare_data >= lower_bound) & (fare_data <= upper_bound)]\n",
    "axes[1,1].hist(fare_no_outliers, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1,1].set_title('Fare Distribution (Outliers Removed)')\n",
    "axes[1,1].set_xlabel('Fare')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare statistics with and without outliers\n",
    "print(\"\\nSTATISTICS COMPARISON\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Original data:\")\n",
    "print(f\"  Mean: {fare_data.mean():.2f}\")\n",
    "print(f\"  Median: {fare_data.median():.2f}\")\n",
    "print(f\"  Std: {fare_data.std():.2f}\")\n",
    "\n",
    "print(f\"\\nWithout outliers:\")\n",
    "print(f\"  Mean: {fare_no_outliers.mean():.2f}\")\n",
    "print(f\"  Median: {fare_no_outliers.median():.2f}\")\n",
    "print(f\"  Std: {fare_no_outliers.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Normalization\n",
    "\n",
    "Data normalization (also called feature scaling) is the process of scaling individual features to have similar ranges. This is important because:\n",
    "\n",
    "### 7.1 Why Normalize?\n",
    "\n",
    "1. **Different Unit Measures**: Variables may have different units (age in years, income in dollars)\n",
    "2. **Algorithm Performance**: Many algorithms are sensitive to the scale of input features\n",
    "3. **Convergence Speed**: Optimization algorithms converge faster with normalized data\n",
    "4. **Equal Importance**: Prevents variables with larger scales from dominating\n",
    "\n",
    "### 7.2 Common Normalization Techniques\n",
    "\n",
    "#### Min-Max Scaling (Normalization)\n",
    "Scales features to a fixed range, typically [0, 1]:\n",
    "\n",
    "**X_scaled = (X - X_min) / (X_max - X_min)**\n",
    "\n",
    "- **Advantages**: Preserves relationships, bounded output\n",
    "- **Disadvantages**: Sensitive to outliers\n",
    "- **Use when**: You know the approximate upper and lower bounds\n",
    "\n",
    "#### Standard Scaling (Z-score Normalization)\n",
    "Centers data around mean with unit variance:\n",
    "\n",
    "**X_scaled = (X - μ) / σ**\n",
    "\n",
    "- **Advantages**: Not bounded, less sensitive to outliers than Min-Max\n",
    "- **Disadvantages**: Doesn't guarantee a specific range\n",
    "- **Use when**: Data follows normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization examples\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# Prepare data for normalization\n",
    "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "data_for_scaling = titanic[numeric_features].dropna()\n",
    "\n",
    "print(\"ORIGINAL DATA STATISTICS\")\n",
    "print(\"=\"*35)\n",
    "print(data_for_scaling.describe())\n",
    "\n",
    "# Apply different scaling methods\n",
    "# 1. Min-Max Scaling\n",
    "minmax_scaler = MinMaxScaler()\n",
    "data_minmax = pd.DataFrame(\n",
    "    minmax_scaler.fit_transform(data_for_scaling),\n",
    "    columns=[f'{col}_minmax' for col in numeric_features]\n",
    ")\n",
    "\n",
    "# 2. Standard Scaling\n",
    "standard_scaler = StandardScaler()\n",
    "data_standard = pd.DataFrame(\n",
    "    standard_scaler.fit_transform(data_for_scaling),\n",
    "    columns=[f'{col}_standard' for col in numeric_features]\n",
    ")\n",
    "\n",
    "# 3. Robust Scaling (uses median and IQR, less sensitive to outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "data_robust = pd.DataFrame(\n",
    "    robust_scaler.fit_transform(data_for_scaling),\n",
    "    columns=[f'{col}_robust' for col in numeric_features]\n",
    ")\n",
    "\n",
    "print(\"\\nMIN-MAX SCALED DATA STATISTICS\")\n",
    "print(\"=\"*40)\n",
    "print(data_minmax.describe())\n",
    "\n",
    "print(\"\\nSTANDARD SCALED DATA STATISTICS\")\n",
    "print(\"=\"*40)\n",
    "print(data_standard.describe())\n",
    "\n",
    "# Visualize the effect of different scaling methods\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "fig.suptitle('Comparison of Scaling Methods', fontsize=16)\n",
    "\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    # Original data\n",
    "    axes[0, i].hist(data_for_scaling[feature], bins=30, alpha=0.7, color='blue')\n",
    "    axes[0, i].set_title(f'Original {feature}')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Min-Max scaled\n",
    "    axes[1, i].hist(data_minmax[f'{feature}_minmax'], bins=30, alpha=0.7, color='green')\n",
    "    axes[1, i].set_title(f'Min-Max {feature}')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Standard scaled\n",
    "    axes[2, i].hist(data_standard[f'{feature}_standard'], bins=30, alpha=0.7, color='red')\n",
    "    axes[2, i].set_title(f'Standard {feature}')\n",
    "    axes[2, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare ranges before and after scaling\n",
    "print(\"\\nRANGE COMPARISON\")\n",
    "print(\"=\"*25)\n",
    "for feature in numeric_features:\n",
    "    original_range = data_for_scaling[feature].max() - data_for_scaling[feature].min()\n",
    "    minmax_range = data_minmax[f'{feature}_minmax'].max() - data_minmax[f'{feature}_minmax'].min()\n",
    "    standard_range = data_standard[f'{feature}_standard'].max() - data_standard[f'{feature}_standard'].min()\n",
    "    \n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Original range: {original_range:.2f}\")\n",
    "    print(f\"  Min-Max range: {minmax_range:.2f}\")\n",
    "    print(f\"  Standard range: {standard_range:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the impact of scaling on machine learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare data (using only complete cases)\n",
    "ml_data = titanic[['Age', 'Fare']].dropna()\n",
    "\n",
    "# Apply K-means clustering with and without scaling\n",
    "kmeans_original = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters_original = kmeans_original.fit_predict(ml_data)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "ml_data_scaled = scaler.fit_transform(ml_data)\n",
    "\n",
    "kmeans_scaled = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters_scaled = kmeans_scaled.fit_predict(ml_data_scaled)\n",
    "\n",
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Impact of Scaling on K-Means Clustering', fontsize=16)\n",
    "\n",
    "# Original data clustering\n",
    "scatter1 = axes[0].scatter(ml_data['Age'], ml_data['Fare'], \n",
    "                          c=clusters_original, cmap='viridis', alpha=0.6)\n",
    "axes[0].set_title('Clustering on Original Data')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Fare')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scaled data clustering (transform back for visualization)\n",
    "ml_data_scaled_df = pd.DataFrame(ml_data_scaled, columns=['Age_scaled', 'Fare_scaled'])\n",
    "scatter2 = axes[1].scatter(ml_data['Age'], ml_data['Fare'], \n",
    "                          c=clusters_scaled, cmap='viridis', alpha=0.6)\n",
    "axes[1].set_title('Clustering on Scaled Data')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Fare')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCLUSTERING COMPARISON\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Inertia (original data): {kmeans_original.inertia_:.2f}\")\n",
    "print(f\"Inertia (scaled data): {kmeans_scaled.inertia_:.2f}\")\n",
    "print(\"\\nNote: Lower inertia generally indicates better clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive lecture, we have covered the essential aspects of descriptive statistics and data visualization:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Data Collection**: Understanding different methods (surveys, experiments, observational studies) and sources (APIs, scraping, public datasets)\n",
    "\n",
    "2. **Variable Types**: Distinguishing between quantitative/qualitative, discrete/continuous, and different scales of measurement\n",
    "\n",
    "3. **Data Structure**: Understanding wide vs long formats and design matrices\n",
    "\n",
    "4. **Descriptive Statistics**: \n",
    "   - Central tendency: mean, median, mode\n",
    "   - Dispersion: range, variance, standard deviation, IQR\n",
    "   - Position: quantiles, quartiles, percentiles\n",
    "\n",
    "5. **Data Visualization**:\n",
    "   - Histograms for distributions\n",
    "   - Bar charts for categorical data\n",
    "   - Box plots for comparing groups\n",
    "   - Scatter plots for relationships\n",
    "   - Heatmaps for matrices\n",
    "   - Pie charts for proportions\n",
    "\n",
    "6. **Data Cleaning**:\n",
    "   - Handling missing values (dropping vs imputation)\n",
    "   - Identifying and removing duplicates\n",
    "   - Detecting and handling outliers\n",
    "\n",
    "7. **Data Normalization**:\n",
    "   - Why normalization is important\n",
    "   - Min-Max scaling vs Standard scaling\n",
    "   - Impact on machine learning algorithms\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Always explore your data before analysis\n",
    "- Choose appropriate visualizations for your data types\n",
    "- Handle missing values thoughtfully\n",
    "- Consider the impact of outliers on your analysis\n",
    "- Scale your data when necessary for machine learning\n",
    "- Document your data cleaning and preprocessing steps\n",
    "\n",
    "This foundation in descriptive statistics and visualization will serve as the basis for more advanced statistical analyses and machine learning techniques in subsequent lectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}