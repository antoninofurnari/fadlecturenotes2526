

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Data Distributions &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/05_data_distributions';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Statistical Inference" href="06_statistical_inference.html" />
    <link rel="prev" title="Association between variables" href="04_association_between_variables.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamentals of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">Introduction to the Labs and Work Environment Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_python_crash_course.html">Python Crash Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_python_data_science_crash_course.html">Python for Data Science Crash Course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro_data_analysis.html">Data Analysis Key Concepts, Loading and Inspecting the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="02_describing_and_visualizing_the_data.html">Describing and Visualizing the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_probability_for_data_analysis.html">Probability for Data Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_association_between_variables.html">Association between variables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Data Distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_statistical_inference.html">Statistical Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="07_storytelling_with_data.html">Storytelling with Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_predictive_modeling.html">Introduction to Predictive Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_linear_regression.html">Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 11</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_beyond_linear_regression.html">Beyond Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 12</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_classification_knn.html">Classification Task, Evaluation Measures, and K-Nearest Neighbor</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 13</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_logistic_regression.html">Logistic Regression - Statistical View</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 14</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_multiclass_logistic_regression.html">Multiclass Logistic Regression and Predictive View</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 15</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="14_map_naive_bayes.html">Generative Classifiers and Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 16</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="15_data_as_nd_points.html">Data as N-Dimensional Points</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_clustering.html">Clustering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 17</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17_density_estimation.html">Density Estimation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 18</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="18_principal_component_analysis.html">Dimensionality Reduction: Principal Component Analysis (PCA)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes2526/blob/master/lecturenotes/lectures/05_data_distributions.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526/issues/new?title=Issue%20on%20page%20%2Flectures/05_data_distributions.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/05_data_distributions.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Data Distributions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-variables">Discrete Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-mass-functions-pmf">Probability Mass Functions (PMF)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-fair-coin">Example: Probability Mass Function for a Fair Coin</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-biased-coin">Example: Probability Mass Function for a Biased Coin</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-probability-mass-functions-from-data">Computing Probability Mass Functions from Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-probability-mass-function">Exercise: Probability Mass Function</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-distribution-functions-of-discrete-variables">Cumulative Distribution Functions of Discrete Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-pmf-and-cdf-of-a-fair-die">Example – PMF and CDF of a fair die</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-cdfs-for-discrete-variables-from-data">Computing CDFs for Discrete Variables from Data</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-variables">Continuous Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions-pdf">Probability Density Functions (PDF)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-uniform-pdf">Example: Uniform PDF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-a-pdf-with-density-histograms-and-density-estimates">Approximating a PDF with Density Histograms and Density Estimates</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-distribution-functions-cdf">Cumulative Distribution Functions (CDF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-probability-distributions">Common Probability Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-uniform-distribution">Discrete Uniform Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bernoulli-distribution">Bernoulli Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-distribution">Binomial Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-distribution">Categorical Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-distribution">Multinomial Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-distribution">Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#central-limit-theorem">Central Limit Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian">Multivariate Gaussian</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-sigma">Effect of <span class="math notranslate nohighlight">\(\Sigma\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-gaussian-distribution">Estimation of the Parameters of a Gaussian Distribution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#describing-data-distributions">Describing Data Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation">Expectation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-and-standard-deviation">Variance and Standard Deviation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self-information">Self-Information</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-of-a-distribution">Entropy of a Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-of-a-bernoulli-variable">Entropy of a Bernoulli variable</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization">Standardization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-distributions-to-data">Fitting Distributions to Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="data-distributions">
<h1>Data Distributions<a class="headerlink" href="#data-distributions" title="Permalink to this heading">#</a></h1>
<p>We have seen how it is possible to assign a probability value to a given
outcome of a random variable.</p>
<p>In practice, it is often useful to assign probability values to <strong>all the values</strong> that the random variable can assume.</p>
<p>To do so, we can define a <strong>function</strong>, which we will call <strong>probability
distribution</strong> which assigns a probability value to each of the possible
values of a random variable.</p>
<p>In the case of discrete variables, we will talk about “<strong>probability mass functions</strong>”, whereas in the case of continuous variables, we will refer to “<strong>probability density functions</strong>”.</p>
<p>A probability distribution characterizes the random variable and defines
which outcomes it is more likely to observe.</p>
<p>Once we find that a given random variable <span class="math notranslate nohighlight">\(X\)</span> is characterized by a
probability distirbution <span class="math notranslate nohighlight">\(P(X)\)</span>, we can say that <strong>“X follows P”</strong> and
write:</p>
<div class="math notranslate nohighlight">
\[X \sim P\]</div>
<section id="discrete-variables">
<h2>Discrete Variables<a class="headerlink" href="#discrete-variables" title="Permalink to this heading">#</a></h2>
<p>We will start by focusing on discrete variables.</p>
<section id="probability-mass-functions-pmf">
<h3>Probability Mass Functions (PMF)<a class="headerlink" href="#probability-mass-functions-pmf" title="Permalink to this heading">#</a></h3>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is discrete, <span class="math notranslate nohighlight">\(P(X)\)</span> is called a “probability mass function”
(PMF).</p>
<p><span class="math notranslate nohighlight">\(P\)</span> can be seen as a function mapping the values of <span class="math notranslate nohighlight">\(X\)</span> to real numbers indicating whether a
given value is more or less likely.</p>
<p>A PMF on a random variable <span class="math notranslate nohighlight">\(X\)</span> is a function:</p>
<div class="math notranslate nohighlight">
\[P:\Omega \rightarrow \lbrack 0,1\rbrack\]</div>
<p>which satisfies the following property:</p>
<div class="math notranslate nohighlight">
\[\sum_{\mathbf{x}\mathbf{\in}\mathbf{\Omega}}^{}{\mathbf{P}\mathbf{(}\mathbf{x}\mathbf{)}}\mathbf{=}\mathbf{1}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Omega\)</span> is the sample space <span class="math notranslate nohighlight">\(X\)</span>,</p>
<p>This condition implies that the probability distribution is normalized (and probability axioms are satisfied).</p>
<section id="example-probability-mass-function-for-a-fair-coin">
<h4>Example: Probability Mass Function for a Fair Coin<a class="headerlink" href="#example-probability-mass-function-for-a-fair-coin" title="Permalink to this heading">#</a></h4>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be the random variable indicating the outcome of a
coin toss.</p>
<ul class="simple">
<li><p>The space of all possible outcomes (the domain of <span class="math notranslate nohighlight">\(P(X)\)</span>) is
<span class="math notranslate nohighlight">\(\{ head,\ tail\}\)</span>.</p></li>
<li><p>The probabilities <span class="math notranslate nohighlight">\(P(head)\)</span> and <span class="math notranslate nohighlight">\(P(tail)\)</span> must be l<strong>arger than or
equal to zero and smaller than or equal to 1</strong>.</p></li>
<li><p>Also, <span class="math notranslate nohighlight">\(P(head) + P(tail) = 1\ \)</span>. This is obvious, as one of the two
outcomes will always happen. Indeed, if we had <span class="math notranslate nohighlight">\(P(tail) = 0.3\)</span>, this
would mean that, <span class="math notranslate nohighlight">\(30\)</span> times out of <span class="math notranslate nohighlight">\(100\)</span> times we toss a coin, the
outcome will be tail. What will happen in all other cases? The
outcome will be head, hence, <span class="math notranslate nohighlight">\(P(head)=0.7\)</span>, so <span class="math notranslate nohighlight">\(P(head) + P(tail) = 1\)</span>.</p></li>
<li><p>In the case of a fair coin, we can characterize <span class="math notranslate nohighlight">\(P(X)\)</span> as a
<strong>“discrete uniform distribution”</strong>, i.e., a distribution which maps any
value <span class="math notranslate nohighlight">\(x \in X\)</span> to a constant, such that the properties of the
probability mass functions are satisfied.</p></li>
<li><p>If we have <span class="math notranslate nohighlight">\(N\)</span> possible outcomes, the discrete uniform probability
will be <span class="math notranslate nohighlight">\(P(X = x) = \frac{1}{N}\)</span> , which means that all outcomes
have the same probability.</p></li>
<li><p>This definition satisfies the constraints. Indeed:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\frac{1}{N} \geq 0,\ \forall N\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{i}^{}{P\left( X = x_{i} \right)} = 1\)</span></p></li>
</ul>
</li>
</ul>
<p>A probability mass function can be visualized as a 2D diagram where the
values of the function (<span class="math notranslate nohighlight">\(P(x)\)</span>) is plotted against the values of the
independent variable <span class="math notranslate nohighlight">\(x\)</span>. This is the diagram associated to the PMF of
the previous example, where <span class="math notranslate nohighlight">\(P(head) = P(tail) = 0.5\)</span>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/51a3431126a7f8fa3b08074abd14d946736c99d548d81f770c5a34fcce8a1747.png" src="../_images/51a3431126a7f8fa3b08074abd14d946736c99d548d81f770c5a34fcce8a1747.png" />
</div>
</div>
</section>
<section id="example-probability-mass-function-for-a-biased-coin">
<h4>Example: Probability Mass Function for a Biased Coin<a class="headerlink" href="#example-probability-mass-function-for-a-biased-coin" title="Permalink to this heading">#</a></h4>
<p>Now suppose we tossed our coin for 10000 times and discovered that 6000
times the outcome was “head”, whereas 4000 times it was “tail”. We
deduce the coin is not fair.</p>
<p>Using a <strong>frequentist</strong> approach, we can manually assign values to our
PMF using the general formula:</p>
<div class="math notranslate nohighlight">
\[P(x) = \frac{\# trials\ in\ which\ X = x}{\#\ trials}\]</div>
<p>That is, in our case:</p>
<div class="math notranslate nohighlight">
\[P(head) = \frac{6000}{10000} = 0.6;P(tail) = \frac{4000}{10000} = 0.4\]</div>
<p>We shall note that the probability we just defined satisfies all
properties of probabilities, i.e.:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(0 \leq P(x) \leq 1\ \forall x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{x}^{}{P(x) = 1}.\)</span></p></li>
</ul>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/4e3dfbdda16792a7647058fd634fdeb49720d8e9ca3db4dc10ee1aee46a8b5d6.png" src="../_images/4e3dfbdda16792a7647058fd634fdeb49720d8e9ca3db4dc10ee1aee46a8b5d6.png" />
</div>
</div>
</section>
<section id="computing-probability-mass-functions-from-data">
<h4>Computing Probability Mass Functions from Data<a class="headerlink" href="#computing-probability-mass-functions-from-data" title="Permalink to this heading">#</a></h4>
<p>In practice, Probability Mass Functions are computed exactly as relative frequencies. Let’s see a quick example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">titanic</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv&#39;</span><span class="p">,</span>
                     <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;PassengerId&#39;</span><span class="p">)</span>
<span class="n">pclass</span> <span class="o">=</span> <span class="n">titanic</span><span class="p">[</span><span class="s1">&#39;Pclass&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">pclass</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="n">pclass</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>  <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pclass</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">pclass</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;PMF of a Passenger Class&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">pclass</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/79725b8e10e12e75d76674dc371470c4dc17fa0e481fd9abe3ce715e71abb9f8.png" src="../_images/79725b8e10e12e75d76674dc371470c4dc17fa0e481fd9abe3ce715e71abb9f8.png" />
</div>
</div>
</section>
<section id="exercise-probability-mass-function">
<h4>Exercise: Probability Mass Function<a class="headerlink" href="#exercise-probability-mass-function" title="Permalink to this heading">#</a></h4>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable representing the outcome of rolling a fair
dice with <span class="math notranslate nohighlight">\(6\)</span> faces:</p>
<ul class="simple">
<li><p>What is the space of possible values of <span class="math notranslate nohighlight">\(X\)</span>?</p></li>
<li><p>What is its cardinality?</p></li>
<li><p>What is the associated probability mass function <span class="math notranslate nohighlight">\(P(X)\)</span>?</p></li>
<li><p>Suppose the dice is not fair and <span class="math notranslate nohighlight">\(P(X = 1) = 0.2\)</span>, whereas all other
outcomes are equally probable. What is the probability mass function
of <span class="math notranslate nohighlight">\(P(X)\)</span>?</p></li>
<li><p>Draw the PMF obtained for the dice.</p></li>
</ul>
</section>
</section>
<section id="cumulative-distribution-functions-of-discrete-variables">
<h3>Cumulative Distribution Functions of Discrete Variables<a class="headerlink" href="#cumulative-distribution-functions-of-discrete-variables" title="Permalink to this heading">#</a></h3>
<p>If the discrete variable <span class="math notranslate nohighlight">\(X\)</span> can be ordered (i.e., it is at least ordinal), we can define the <strong>Cumulative Distribution Function (CDF)</strong> as:</p>
<div class="math notranslate nohighlight">
\[F(x) = P(X \leq x) = \sum_{y \leq x} P(y)\]</div>
<p>The CDF records the cumulative probability of all values less than or equal to a given value. For example, if <span class="math notranslate nohighlight">\(H\)</span> measures the height of people in centimeters, <span class="math notranslate nohighlight">\(F(180)\)</span> indicates the <strong>probability of selecting a person with height at most 180 cm</strong>.</p>
<section id="example-pmf-and-cdf-of-a-fair-die">
<h4>Example – PMF and CDF of a fair die<a class="headerlink" href="#example-pmf-and-cdf-of-a-fair-die" title="Permalink to this heading">#</a></h4>
<p>For a fair die, the PMF is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x) = \begin{cases}
\frac{1}{6} &amp; \text{if } x \in \{1, 2, 3, 4, 5, 6\} \\
0 &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>The CDF is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
F(x) = \begin{cases}
0 &amp; \text{if } x &lt; 1 \\
\frac{1}{6} &amp; \text{if } 1 \leq x &lt; 2 \\
\frac{2}{6} &amp; \text{if } 2 \leq x &lt; 3 \\
\frac{3}{6} &amp; \text{if } 3 \leq x &lt; 4 \\
\frac{4}{6} &amp; \text{if } 4 \leq x &lt; 5 \\
\frac{5}{6} &amp; \text{if } 5 \leq x &lt; 6 \\
1 &amp; \text{if } x \geq 6
\end{cases}
\end{split}\]</div>
<p>The diagram below shows an example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/fba505c294701293785eca7ab48cf10bfc572aa75684294994956908e4b48832.png" src="../_images/fba505c294701293785eca7ab48cf10bfc572aa75684294994956908e4b48832.png" />
</div>
</div>
</section>
<section id="computing-cdfs-for-discrete-variables-from-data">
<h4>Computing CDFs for Discrete Variables from Data<a class="headerlink" href="#computing-cdfs-for-discrete-variables-from-data" title="Permalink to this heading">#</a></h4>
<p>Empirical Cumulative Distribution Functions (ECDFs), introduced in the lecture on data description, allow us to estimate the CDF directly from observed data. For a discrete variable, the ECDF at a value <span class="math notranslate nohighlight">\(x\)</span> is simply the proportion of data points less than or equal to <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Here is a quick example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load the Titanic dataset</span>
<span class="n">titanic</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;PassengerId&#39;</span><span class="p">)</span>

<span class="c1"># Extract the age column, removing missing values</span>
<span class="n">ages</span> <span class="o">=</span> <span class="n">titanic</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Sort the ages</span>
<span class="n">ages_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">ages</span><span class="p">)</span>

<span class="c1"># Compute the ECDF</span>
<span class="n">ecdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ages_sorted</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">ages_sorted</span><span class="p">)</span>

<span class="c1"># Plot the ECDF</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ages_sorted</span><span class="p">,</span> <span class="n">ecdf</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;ECDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Empirical Cumulative Distribution Function (ECDF) of Ages - Titanic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/340412e5c563ceb30736aa4a41452e3bb020b2686d03ee428b88c83a38c4b1d7.png" src="../_images/340412e5c563ceb30736aa4a41452e3bb020b2686d03ee428b88c83a38c4b1d7.png" />
</div>
</div>
</section>
</section>
</section>
<section id="continuous-variables">
<h2>Continuous Variables<a class="headerlink" href="#continuous-variables" title="Permalink to this heading">#</a></h2>
<p>We will now focus on the case in which <span class="math notranslate nohighlight">\(X\)</span> is a continuous variable. In this case, we cannot use the same definitions as for discrete variables and we have to resort to new definition.</p>
<section id="probability-density-functions-pdf">
<h3>Probability Density Functions (PDF)<a class="headerlink" href="#probability-density-functions-pdf" title="Permalink to this heading">#</a></h3>
<p>In order to define a probability distribution over a continuous variable, we first have to introduce the concept of “probability density function”.</p>
<p>A probability density function over a variable <span class="math notranslate nohighlight">\(X\)</span> is defined as follows:</p>
<div class="math notranslate nohighlight">
\[ f:\Omega \rightarrow [0,1] \]</div>
<p>and must satisfy the following property:</p>
<div class="math notranslate nohighlight">
\[\int f(x)\,dx = 1\]</div>
<p>This condition is equivalent to <span class="math notranslate nohighlight">\(\sum P(x) = 1\)</span> in the case of a discrete variable. The sum becomes an integral for continuous variables.</p>
<p>We can then define the concept of probability based on the density as follows:</p>
<div class="math notranslate nohighlight">
\[ P(a \leq x \leq b) = \int_{a}^b f(x)\,dx \]</div>
<p><strong>Intuitively, the density function <span class="math notranslate nohighlight">\(f(x)\)</span> describes how likely it is to observe values near <span class="math notranslate nohighlight">\(x\)</span>. The higher the density at a point, the more likely it is to find the variable in a small interval around that point. In other words, the density can be seen as a “concentration” of probability: regions where <span class="math notranslate nohighlight">\(f(x)\)</span> is high correspond to intervals where the variable is more frequently observed. To obtain an actual probability, we must integrate the density over an interval.</strong></p>
<blockquote>
<div><p>For a continuous random variable, the probability of observing exactly <span class="math notranslate nohighlight">\(x\)</span> is zero: <span class="math notranslate nohighlight">\(P(X = x) = 0\)</span>. However, the density function <span class="math notranslate nohighlight">\(f(x)\)</span> at <span class="math notranslate nohighlight">\(x\)</span> is generally not zero; it represents the probability per unit interval around <span class="math notranslate nohighlight">\(x\)</span>. Probability is obtained by integrating <span class="math notranslate nohighlight">\(f(x)\)</span> over an interval, not at a single point.</p>
</div></blockquote>
<section id="example-uniform-pdf">
<h4>Example: Uniform PDF<a class="headerlink" href="#example-uniform-pdf" title="Permalink to this heading">#</a></h4>
<p>Let us consider a random number generator which outputs numbers comprised between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable assuming the values generated by the
random number generator.</p>
<p>The PDF of <span class="math notranslate nohighlight">\(X\)</span> will be a uniform distribution such that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(x) = 0\ \forall x &lt; a\ \ or\ x &gt; b\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(P(x) = \frac{1}{b - a}\ \forall a \leq x \leq b\)</span>;</p></li>
</ul>
<p>We can see that this PDF satisfies all constraints:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(x) \geq 0\ \forall x.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\int P(x)dx = 1\)</span> (prove that this is true as an exercise).</p></li>
</ul>
<p>The diagram below shows an illustration of a uniform PDF with bounds a and b:</p>
<div class="math notranslate nohighlight">
\[U(a,b)\]</div>
<p>Of course, continuous distributions can be (and
generally are) much more complicated than that.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/ae53cb33e911ffc32c49057972c62082d537f8167705cdf148c7cffc4b28810c.png" src="../_images/ae53cb33e911ffc32c49057972c62082d537f8167705cdf148c7cffc4b28810c.png" />
</div>
</div>
</section>
<section id="approximating-a-pdf-with-density-histograms-and-density-estimates">
<h4>Approximating a PDF with Density Histograms and Density Estimates<a class="headerlink" href="#approximating-a-pdf-with-density-histograms-and-density-estimates" title="Permalink to this heading">#</a></h4>
<p>When working with real data, we can try to approximate the PDF of a continuous variable with histograms.</p>
<p>Let the height and width of bin <span class="math notranslate nohighlight">\(b_j\)</span> be <span class="math notranslate nohighlight">\(h_j\)</span> and <span class="math notranslate nohighlight">\(w_j\)</span> respectively. In a <strong>normalized histogram</strong>, we define the height of bin <span class="math notranslate nohighlight">\(b_j\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[h_j=\frac{c_j}{n}\]</div>
<p>where <span class="math notranslate nohighlight">\(c_j\)</span> is the number of observations that fall in bin <span class="math notranslate nohighlight">\(b_j\)</span>.</p>
<p>Note that, while <strong>normalized histograms</strong> are such that each bar height is comprised between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, they do not satisfy the properties of PDFs.</p>
<p>Let the area of bin <span class="math notranslate nohighlight">\(b_j\)</span> be:</p>
<div class="math notranslate nohighlight">
\[A_j = w_j \cdot h_j\]</div>
<p>The integral over histogram <span class="math notranslate nohighlight">\(H\)</span> (i.e., the area under the histogram) will be:</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty} H(x) dx = \sum_j A_j = \sum_j w_j \frac{c_j}{n} = \frac{1}{n}\sum_j w_j \cdot c_j = \sum w_j \neq 1\]</div>
<p>To ensure that this property is satisfied, we define a new kind of histogram, the <strong>density histogram</strong>, where bin heights are defined as:</p>
<div class="math notranslate nohighlight">
\[h_j = \frac{c_j}{w_j \cdot n}\]</div>
<p>In this case:</p>
<div class="math notranslate nohighlight">
\[\sum_j A_j = \sum_j w_j \frac{c_j}{w_j \cdot n} = \frac{1}{n}\sum_j \frac{w_j}{w_j} \cdot c_j = 1\]</div>
<p>To create a density histogram in Python, simply pass <code class="docutils literal notranslate"><span class="pre">density=True</span></code> to the <code class="docutils literal notranslate"><span class="pre">hist</span></code> function in matplotlib or pandas.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Load the Titanic dataset and extract the &#39;Age&#39; column, dropping missing values</span>
<span class="n">titanic</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv&#39;</span><span class="p">)</span>
<span class="n">ages</span> <span class="o">=</span> <span class="n">titanic</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Create a figure with two subplots for side-by-side comparison</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># --- Plot 1: Normalized Histogram (Probability) ---</span>
<span class="c1"># The sum of bar heights will equal 1</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">ages</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">ages</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ages</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Normalized Histogram (Probability)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability (Sum of heights = 1)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="c1"># --- Plot 2: Density Histogram ---</span>
<span class="c1"># The total area of the bars will equal 1</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ages</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Density Histogram&#39;</span><span class="p">)</span>

<span class="c1"># Fit a normal distribution and overlay its PDF</span>
<span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ages</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ages</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">ages</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">pdf_fitted</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu_hat</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma_hat</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf_fitted</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fitted Normal PDF&#39;</span><span class="p">)</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Density Histogram&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density (Total area = 1)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Add an overall title and show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Normalized vs. Density Histograms for Passenger Ages&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>  <span class="c1"># Adjust layout to make space for suptitle</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/74d4ffc660790933d26fe49ecd2068ef721c7632574f344e2e191167132e9baf.png" src="../_images/74d4ffc660790933d26fe49ecd2068ef721c7632574f344e2e191167132e9baf.png" />
</div>
</div>
<p>The figure above contrasts two important ways to normalize a histogram using Titanic passenger ages, each with a distinct but related interpretation:</p>
<ul class="simple">
<li><p><strong>Normalized Histogram (Left)</strong>:<br />
The y-axis represents <strong>probability</strong>. Each bar’s height shows the proportion of passengers in that age bin, so the <strong>sum of all bar heights equals 1</strong>. This treats the binned data like a discrete probability distribution and allows direct reading of probabilities (e.g., the chance of being 20–24 years old is roughly 0.12 or 12%).</p></li>
<li><p><strong>Density Histogram (Right)</strong>:<br />
The y-axis represents <strong>density</strong>—probability per unit on the x-axis (e.g., per year of age). Here, it’s the <strong>area</strong> of each bar (height × width) that reflects probability. The <strong>total area of all bars equals 1</strong>, mirroring the behavior of a continuous Probability Density Function (PDF).</p></li>
</ul>
<p><strong>Key takeaway</strong>:<br />
The density histogram is the correct empirical approximation of a continuous PDF. That’s why the smooth red PDF curve aligns with the right plot’s y-axis but would be mis-scaled on the left. While both histograms are normalized, only the density version properly reflects continuous probability.</p>
</section>
</section>
<section id="cumulative-distribution-functions-cdf">
<h3>Cumulative Distribution Functions (CDF)<a class="headerlink" href="#cumulative-distribution-functions-cdf" title="Permalink to this heading">#</a></h3>
<p>Just like in the case of discrete variables, we can define Cumulative Distribution Functions (CDFs) for continuous random variables using their density functions. The CDF of a random variable is generally defined as:</p>
<div class="math notranslate nohighlight">
\[ F(x) = \int_{-\infty}^x f(x)\,dx \]</div>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<p>The CDF of the uniform distribution will be given by:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F(x) = 0\)</span> for <span class="math notranslate nohighlight">\(x &lt; a\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(F(x) = \frac{x - a}{b - a}\)</span> for <span class="math notranslate nohighlight">\(a \leq x \leq b\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(F(x) = 1\)</span> for <span class="math notranslate nohighlight">\(x &gt; b\)</span></p></li>
</ul>
<p>The plot below shows a diagram:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/fc9bf7ac2f687b9dfe7a358a76ac3b9c72071baeac2a0b4737c234d842737a42.png" src="../_images/fc9bf7ac2f687b9dfe7a358a76ac3b9c72071baeac2a0b4737c234d842737a42.png" />
</div>
</div>
</section>
</section>
<section id="common-probability-distributions">
<h2>Common Probability Distributions<a class="headerlink" href="#common-probability-distributions" title="Permalink to this heading">#</a></h2>
<p>There are several common probability distributions which can be used to
describe random events. <strong>These distributions have an analytical
formulation which depends generally on one or more parameters.</strong></p>
<p>When we have <strong>enough evidence that a given random variable is well
described by one of these distributions</strong>, we can simply “fit” the
distribution to the data (i.e., choose the correct parameters for the
distribution) and use the analytical formulation to deal with the random
variable.</p>
<p>It is hence useful to know the <strong>most common probability distributions</strong>
so that we can recognize the cases in which they can be used.</p>
<section id="discrete-uniform-distribution">
<h3>Discrete Uniform Distribution<a class="headerlink" href="#discrete-uniform-distribution" title="Permalink to this heading">#</a></h3>
<p>The discrete uniform distribution is controlled by a parameter <span class="math notranslate nohighlight">\(k \in \mathbb{N}\)</span> and assumes that all outcomes have the same probability of occurring:</p>
<div class="math notranslate nohighlight">
\[P(X=a_i) = \frac{1}{k}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\Omega = \{a_1,\ldots,a_k\}\)</span>.</p>
<section id="id1">
<h4>Example<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h4>
<p>The outcomes of rolling a fair die follow a uniform distribution with <span class="math notranslate nohighlight">\(k=6\)</span>, as shown in the diagram below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/873833561695cc3ce4719c10f8662362e6e0913d22054e4f7b3e61f9c6cb7cbb.png" src="../_images/873833561695cc3ce4719c10f8662362e6e0913d22054e4f7b3e61f9c6cb7cbb.png" />
</div>
</div>
</section>
</section>
<section id="bernoulli-distribution">
<h3>Bernoulli Distribution<a class="headerlink" href="#bernoulli-distribution" title="Permalink to this heading">#</a></h3>
<p>The Bernoulli distribution is a distribution over a single binary random
variable, i.e., the variable <span class="math notranslate nohighlight">\(X\)</span> can take only two values:
<span class="math notranslate nohighlight">\(\left\{ 0,1 \right\}\)</span>.</p>
<p>The distribution is controlled by a single parameter
<span class="math notranslate nohighlight">\(\phi \in \lbrack 0,1\rbrack\)</span>, which gives the probability of the
variable to be equal to 1.</p>
<p>The analytical formulation of the Bernoulli distribution is very simple:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(X = 1) = \phi\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(P(X = 0) = 1 - \phi\)</span></p></li>
</ul>
<section id="id2">
<h4>Example<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h4>
<p>A skewed coin lands on “head” <span class="math notranslate nohighlight">\(60\%\)</span> of the
times. If we define <span class="math notranslate nohighlight">\(X = 1\)</span> when the outcome is head and <span class="math notranslate nohighlight">\(X = 0\)</span> when
the outcome is tail, then the variable follows a Bernoulli distribution
with <span class="math notranslate nohighlight">\(\phi = 0.6\)</span>.</p>
<p>The diagram below gives an example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/6b7934277dea21380d79c796266a0cbb010b1fa2f86d6911ce35bc986fdc2d42.png" src="../_images/6b7934277dea21380d79c796266a0cbb010b1fa2f86d6911ce35bc986fdc2d42.png" />
</div>
</div>
</section>
</section>
<section id="binomial-distribution">
<h3>Binomial Distribution<a class="headerlink" href="#binomial-distribution" title="Permalink to this heading">#</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/38c0272acfc817db4ab2de0664d7b825d1793a028f14525fe686298cf2ca457e.png" src="../_images/38c0272acfc817db4ab2de0664d7b825d1793a028f14525fe686298cf2ca457e.png" />
</div>
</div>
<p>The binomial distribution is a discrete probability distribution (PMF)
over natural numbers with parameters <span class="math notranslate nohighlight">\(\mathbf{n}\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(\mathbf{p}\)</span></p>
<p>It models <strong>the probability of obtaining</strong> <span class="math notranslate nohighlight">\(\mathbf{k}\)</span> <strong>successes in a
sequence of</strong> <span class="math notranslate nohighlight">\(\mathbf{n}\)</span> <strong>independent experiments which follow a
Bernoulli distribution with parameter</strong>
<span class="math notranslate nohighlight">\(\mathbf{p}\mathbf{\ (}\mathbf{\phi}\mathbf{=}\mathbf{p}\mathbf{)}\)</span><strong>;</strong></p>
<p>The probability mass function of the distribution is given by:</p>
<div class="math notranslate nohighlight">
\[P(k) = \binom{n}{k}p^{k}(1 - p)^{n - k}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span> is the number of successes</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of independent trials</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> is the probability of a success in a single trial</p></li>
</ul>
<section id="id3">
<h4>Example<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h4>
<p>What is the probability of tossing a coin three times and obtaining
three heads? We have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k = 3\)</span>: number of successes (three times head)</p></li>
<li><p><span class="math notranslate nohighlight">\(n = 3\)</span>: number of trials</p></li>
<li><p><span class="math notranslate nohighlight">\(p = 0.5\)</span>: the probability of getting a head when tossing a coin</p></li>
</ul>
<p>The required probability will be given by:</p>
<div class="math notranslate nohighlight">
\[P(3) = \binom{3}{3}{0.5}^{3}(1 - 0.5)^{3 - 3} = {0.5}^{3} = 0.125\]</div>
<p><strong>Exercise</strong></p>
<p>What is the probability of tossing an unfair coin
(<span class="math notranslate nohighlight">\(P\left( 'head^{'} \right) = 0.6\)</span>) 7 times and obtaining <span class="math notranslate nohighlight">\(2\)</span> tails?</p>
</section>
</section>
<section id="categorical-distribution">
<h3>Categorical Distribution<a class="headerlink" href="#categorical-distribution" title="Permalink to this heading">#</a></h3>
<p>The <strong>multinoulli</strong> or <strong>categorical</strong> distribution is a distribution of
a <em>single discrete variable with</em> <span class="math notranslate nohighlight">\(k\)</span> <em>different states</em>, where <span class="math notranslate nohighlight">\(k\)</span> is
finite.</p>
<ul class="simple">
<li><p>The distribution is parametrized by a vector
<span class="math notranslate nohighlight">\(\mathbf{p} \in \lbrack 0,1\rbrack^k\)</span>, where <span class="math notranslate nohighlight">\(p_{i}\)</span> gives the
probability of the <span class="math notranslate nohighlight">\(i^{th}\)</span> state.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{p}\)</span> must be such that <span class="math notranslate nohighlight">\(\sum_{i = 1}^kp_{i} = 1\)</span> to
obtain a valid probability distribution.</p></li>
<li><p>The analytical form of the distribution is given by:
<span class="math notranslate nohighlight">\(p(x = i) = p_{i}\)</span>;</p></li>
</ul>
<p>This distribution is the <strong>generalization of the Bernoulli distribution
to the case of multiple states</strong>.</p>
<p><strong>Example: Rolling a Biased Die</strong></p>
<p>Consider a single roll of a biased six-sided die. The outcomes are discrete and finite: <span class="math notranslate nohighlight">\(X \in \{1, 2, 3, 4, 5, 6\}\)</span>.</p>
<p>If the die is biased such that the probabilities are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(X=1) = 0.10\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(X=2) = 0.15\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(X=3) = 0.20\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(X=4) = 0.20\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(X=5) = 0.10\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(X=6) = 0.25\)</span></p></li>
</ul>
<p>Then the outcome follows a <strong>Categorical distribution</strong> with <span class="math notranslate nohighlight">\(k = 6\)</span> and the specified probabilities <span class="math notranslate nohighlight">\(\{p_1, p_2, \dots, p_6\} = \{0.1,0.15,0.2,0.2,0.1,0.25\}\)</span>.</p>
<p>The figure below shows an example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/6e78a69bca3a1f5a0aa193f5f2ab997e900b23bdf09c830d4f3404a636566184.png" src="../_images/6e78a69bca3a1f5a0aa193f5f2ab997e900b23bdf09c830d4f3404a636566184.png" />
</div>
</div>
</section>
<section id="multinomial-distribution">
<h3>Multinomial Distribution<a class="headerlink" href="#multinomial-distribution" title="Permalink to this heading">#</a></h3>
<p>The multinomial distribution <strong>generalizes the binomial distribution to
the case in which the experiments are not binary</strong>, but they can have
multiple outcomes (e.g., <em>a dice vs a coin</em>).</p>
<p>In particular, the multinomial distribution models the probability of
obtaining exactly <span class="math notranslate nohighlight">\((n_{1},\ldots,n_{k})\)</span> occurrences (with
<span class="math notranslate nohighlight">\(n = \sum_{i}^{}n_{i}\)</span>) for each of the <span class="math notranslate nohighlight">\(k\)</span> possible outcomes in a
sequence of <span class="math notranslate nohighlight">\(n\)</span> independent experiments which follow a Categorial
distribution with probabilities <span class="math notranslate nohighlight">\(p_{1},\ldots,p_{k}\)</span>.</p>
<p>The parameters of the distribution are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span>: the number of trials</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span>: the number of possible outcomes</p></li>
<li><p><span class="math notranslate nohighlight">\(p_{1},\ldots,p_{k}\)</span> the probabilities of obtaining a given class in
each trial (with <span class="math notranslate nohighlight">\(\sum_{i = 1}^{k}p_{i} = 1\)</span>)</p></li>
</ul>
<p>The PMF of the distribution is:</p>
<div class="math notranslate nohighlight">
\[P\left( n_{1},\ldots,n_{k} \right) = \frac{n!}{n_{1}!\ldots n_{k}!}p_{1}^{n_{1}} \cdot \ldots \cdot p_{k}^{n_{k}}\]</div>
<section id="id4">
<h4>Example<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h4>
<p>Given a fair die with 6 possible outcomes, what is the probability of
getting 3 times 1, 2 times 2, 4 time 3, 5 times 4, 0 times 5, and 1 time
6, rolling the dice for 15 times?</p>
<p>We have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n = 15\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k = 6\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p_{1} = p_{2} = \ldots p_{6} = \frac{1}{6}\)</span></p></li>
</ul>
<p>The required probability is given by:</p>
<div class="math notranslate nohighlight">
\[P(3,2,4,5,0,1) = \frac{15!}{3!2!4!5!0!1!} \cdot \frac{1}{6^{3}} \cdot \frac{1}{6^{2}} \cdot \frac{1}{6^{4}} \cdot \frac{1}{6^{5}} \cdot \frac{1}{6^{0}} \cdot \frac{1}{6^{1}} = 8.04 \cdot 10^{- 5}\]</div>
<p>The bar chart below illustrate the example, showing the observed counts and the estimated probability.</p>
<p>This low probability reflects the combinatorial nature of the multinomial distribution, which accounts for both the number of ways the outcomes can be arranged and the likelihood of each individual outcome.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/8f875f3295e56bb70e9c15e698d3047b85fb129b9c5d63aace98b5b1ce30d2c6.png" src="../_images/8f875f3295e56bb70e9c15e698d3047b85fb129b9c5d63aace98b5b1ce30d2c6.png" />
</div>
</div>
</section>
</section>
<section id="gaussian-distribution">
<h3>Gaussian Distribution<a class="headerlink" href="#gaussian-distribution" title="Permalink to this heading">#</a></h3>
<p>The Bernoulli and Categorical distributions are PMF, i.e., distributions
over discrete random variables.</p>
<p>A common PDF when dealing with real values is the <strong>Gaussian
distribution,</strong> also known as <strong>Normal Distribution</strong>.</p>
<p>The distribution is characterized by two parameters:</p>
<ul class="simple">
<li><p>The mean <span class="math notranslate nohighlight">\(\mu\mathfrak{\in R}\)</span></p></li>
<li><p>The standard deviation <span class="math notranslate nohighlight">\(\sigma \in (0, + \infty)\)</span></p></li>
</ul>
<p>In practice, the distribution is often <em>seen in terms of</em> <span class="math notranslate nohighlight">\(\mu\)</span> <em>and</em>
<span class="math notranslate nohighlight">\(\sigma^{2}\)</span> rather than <span class="math notranslate nohighlight">\(\sigma\)</span>, where <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is called <strong>the
variance</strong>.</p>
<p>The analytical formulation of the Normal distribution is as follows:</p>
<div class="math notranslate nohighlight">
\[N\left( x;\mu,\sigma^{2} \right) = \sqrt{\frac{1}{2\pi\sigma^{2}}}e^{- \frac{1}{2\sigma^{2}}(x - \mu)^{2}}\]</div>
<p>The term under the square root is a normalization term which ensures
that the distribution integrates to 1.</p>
<p>The Gaussian distribution is very used when we do not have much prior
knowledge on the real distribution we wish to model. This in mainly due
to the <strong>central limit theorem</strong>, which states that the sum of many
independent random variables with the same distribution is approximately
normally distributed.</p>
<section id="interpretation">
<h4>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this heading">#</a></h4>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/7424e352fc629169358298bc0e6b45157a3afbde3ec09b06657cbe90e0df68e5.png" src="../_images/7424e352fc629169358298bc0e6b45157a3afbde3ec09b06657cbe90e0df68e5.png" />
</div>
</div>
<p>If we plot the PDF of a Normal distribution, we can find that it is easy
to interpret the meaning of its parameters:</p>
<ul class="simple">
<li><p>The resulting curve has a maximum (highest probability) when
<span class="math notranslate nohighlight">\(x = \mu\)</span></p></li>
<li><p>The curve is symmetric, with the inflection points at
<span class="math notranslate nohighlight">\(x = \mu \pm \sigma\)</span></p></li>
<li><p>The example shows a normal distribution for <span class="math notranslate nohighlight">\(\mu = 0\)</span> and
<span class="math notranslate nohighlight">\(\sigma = 1\)</span></p></li>
</ul>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/18f813befa45db3fcee3aadfa333efb0179687a92fd8a444d66c4d568df2bf9b.png" src="../_images/18f813befa45db3fcee3aadfa333efb0179687a92fd8a444d66c4d568df2bf9b.png" />
</div>
</div>
<p>Another notable property of the Normal distribution is that:</p>
<ul class="simple">
<li><p>About <span class="math notranslate nohighlight">\(68\%\)</span> of the density is comprised in the interval
<span class="math notranslate nohighlight">\(\lbrack - \sigma,\sigma\rbrack\)</span>;</p></li>
<li><p>About <span class="math notranslate nohighlight">\(95\%\)</span> of the density is comprised in the interval
<span class="math notranslate nohighlight">\(\lbrack - 2\sigma,2\sigma\rbrack\)</span>;</p></li>
<li><p>Almost 100% of the density is comprised in the interval
<span class="math notranslate nohighlight">\(\lbrack - 3\sigma,3\sigma\rbrack\)</span>.</p></li>
</ul>
</section>
</section>
<section id="central-limit-theorem">
<h3>Central Limit Theorem<a class="headerlink" href="#central-limit-theorem" title="Permalink to this heading">#</a></h3>
<p>The <strong>Central Limit Theorem (CLT)</strong> is a fundamental statistical principle stating that the distribution of the sum (or average) of a large number of independent and identically distributed (i.i.d.) random variables <span class="math notranslate nohighlight">\(\{X_i\}_{i=1}^n\)</span> tends toward a normal (Gaussian) distribution as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, regardless of the original distribution of each <span class="math notranslate nohighlight">\(X_i\)</span>.</p>
<p>This result is crucial because it explains why the Gaussian distribution is so commonly observed in data analysis and natural phenomena, even when the underlying data are not normally distributed.</p>
<p><strong>Key points:</strong></p>
<ul class="simple">
<li><p>The CLT applies to sums or averages of i.i.d. random variables.</p></li>
<li><p>The convergence to the normal distribution becomes more accurate as the number of variables <span class="math notranslate nohighlight">\(n\)</span> increases.</p></li>
<li><p>The original distribution of the variables can be arbitrary (e.g., uniform, binomial, etc.).</p></li>
</ul>
<p><strong>Illustration:</strong><br />
The plot below shows the distributions of the average outcome when rolling different numbers of dice. Each die represents an independent random variable. As the number of dice increases, the distribution of the average outcome becomes increasingly similar to a Gaussian curve.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>  <span class="c1"># Import seaborn for KDE</span>

<span class="c1"># Simulate rolling a fair six-sided die</span>
<span class="n">num_rolls</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">5000</span><span class="p">]</span>

<span class="c1"># List to store the means of each sample</span>
<span class="n">sample_means</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">sample_size</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
    <span class="n">means</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_rolls</span><span class="p">)]</span>
    <span class="n">sample_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>

<span class="c1"># Plot the sample means for different sample sizes with KDE</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">sample_means</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;KDE&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sample Size </span><span class="si">{</span><span class="n">sample_size</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sample Mean&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Distribution of Sample Means with KDE&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/165936268f08ece39a8abec1c8fbee36176c5547f20acc333db309f9afaf2c00.png" src="../_images/165936268f08ece39a8abec1c8fbee36176c5547f20acc333db309f9afaf2c00.png" />
</div>
</div>
<section id="multivariate-gaussian">
<h4>Multivariate Gaussian<a class="headerlink" href="#multivariate-gaussian" title="Permalink to this heading">#</a></h4>
<p>The formulation of the Gaussian distribution generalizes to the
multivariate case, i.e., the case in which <span class="math notranslate nohighlight">\(X\)</span> is d-dimensional.</p>
<p>In that case, the distribution is parametrized by a <strong>d-dimensional
vector</strong> <span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> and a
<span class="math notranslate nohighlight">\(\mathbf{d}\mathbf{\times}\mathbf{d}\mathbf{\ }\)</span><strong>positive definite
symmetric matrix</strong> <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>. The formulation of the
multi-variate Gaussian is:</p>
<div class="math notranslate nohighlight">
\[N\left( \mathbf{x;\mu,}\mathbf{\Sigma} \right) = \sqrt{\frac{1}{(2\pi)^{d}\det(\Sigma)}}e^{( - \frac{1}{2}\left( \mathbf{x} - \mathbf{\mu} \right)^{T}\Sigma^{- 1}\left( \mathbf{x} - \mathbf{\mu} \right))}\]</div>
<p>In the 2D case, <span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> is a 2D
point representing the center of the Gaussian (the position of the
mode), whereas the matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> influences the “shape” of the
Gaussian.</p>
<p>Examples of bivariate Gaussian distributions are shown below.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/0d0fb910bb61f8d84df156291d3780df429557249514af6f7796660c162a7e7b.png" src="../_images/0d0fb910bb61f8d84df156291d3780df429557249514af6f7796660c162a7e7b.png" />
</div>
</div>
<p>The two plots above are common representations for bivariate continuous distributions:</p>
<ul class="simple">
<li><p>The plot on the top shows a 3D representation of the PDF in which the X and Y axes are the values of the variables, while the third axis reports the probability density.</p></li>
<li><p>Since it’s often hard to draw 3D graphs, we often use a contour plot to represent the 3D curves. In the 3D plot, curves of the same color represent points which have the same density in the 3D plot.</p></li>
</ul>
</section>
<section id="effect-of-sigma">
<h4>Effect of <span class="math notranslate nohighlight">\(\Sigma\)</span><a class="headerlink" href="#effect-of-sigma" title="Permalink to this heading">#</a></h4>
<p>Similar to how variance affects the dispersion of a 1D Gaussian, the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> affects the dispersion in both axes. As a result, changing the values of the matrix will affect the shape of the distribution. Let’s consider the general covariance matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma = \begin{bmatrix}
  \sigma_x^2 &amp; \sigma_{xy} \\
  \sigma_{yx} &amp; \sigma_y^2
\end{bmatrix}
\end{split}\]</div>
<p>The covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> determines the shape, orientation, and spread of a bivariate Gaussian distribution:</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(\Sigma\)</span> is diagonal (<span class="math notranslate nohighlight">\(\sigma_{xy}=\sigma_{yx}=0\)</span>), the distribution is symmetric and spreads equally along both axes, resulting in circular contours.</p></li>
<li><p>If the variances along the axes differ, the distribution becomes elongated along the axis with higher variance.</p></li>
<li><p>Non-zero off-diagonal elements introduce correlation: a positive covariance causes the distribution to tilt so that higher values of one variable are associated with higher values of the other, while a negative covariance produces an opposite tilt.</p></li>
<li><p>Larger absolute values of covariance lead to more pronounced stretching along a diagonal direction. When both variances and covariances are large, the distribution is highly dispersed and strongly correlated. These effects are clearly visible when comparing different covariance matrices in contour plots.</p></li>
</ul>
<p>Some examples are shown below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/2456640d297d692ffc78bb9d4de6e85abb5ed451f19370054cbf9f7dee45ce03.png" src="../_images/2456640d297d692ffc78bb9d4de6e85abb5ed451f19370054cbf9f7dee45ce03.png" />
</div>
</div>
</section>
<section id="estimation-of-the-parameters-of-a-gaussian-distribution">
<h4>Estimation of the Parameters of a Gaussian Distribution<a class="headerlink" href="#estimation-of-the-parameters-of-a-gaussian-distribution" title="Permalink to this heading">#</a></h4>
<p>We have noted that in many cases we can assume a random variable follows
a Gaussian distribution. However, it is not yet clear how to choose the
parameters of the Gaussian distribution.</p>
<p>Given some data (remember, data is values assumed by random variables!),
we can obtain the parameters of the Gaussian distribution related to the
data with a <strong>maximum likelihood</strong> estimation.</p>
<p>This consists in computing the mean and variance parameters using the
following formula (in the univariate case):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu = \frac{1}{n}\sum_{j}^{}x_{j}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma^{2} = \frac{1}{n-1}\sum_{j}^{}\left( x_{j} - \mu \right)^{2}\)</span></p></li>
</ul>
<p>Where <span class="math notranslate nohighlight">\(x_{j}\)</span> represent the different data points.</p>
<p>In the multi-variate case, the computation of the multi-dimensional
<span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> vector is similar:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{\mu} = \frac{1}{n}\sum_{j}^{}\mathbf{x}_{j}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is instead computed as the covariance matrix related to
<span class="math notranslate nohighlight">\(X\)</span>: <span class="math notranslate nohighlight">\(\Sigma = Cov(\mathbf{X})\)</span>, i.e.,
<span class="math notranslate nohighlight">\(\Sigma_{ij} = Cov\left( \mathbf{X}_{i},\mathbf{X}_{j} \right)\)</span></p></li>
</ul>
<p>The diagram below shows an example in which we fit a Gaussian to a set of data and compare it with a 2D KDE of the data.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/b085af774b84420478c6ac526657ae4751411da62d6ee59626e984f551f771c8.png" src="../_images/b085af774b84420478c6ac526657ae4751411da62d6ee59626e984f551f771c8.png" />
</div>
</div>
</section>
</section>
</section>
<section id="describing-data-distributions">
<h2>Describing Data Distributions<a class="headerlink" href="#describing-data-distributions" title="Permalink to this heading">#</a></h2>
<p>To effectively summarize and interpret the properties of a probability distribution, it is useful to rely on a set of descriptive statistics. Measures such as <strong>expectation</strong> (mean), <strong>variance</strong>, and <strong>covariance</strong> allow us to capture the central tendency, variability, and relationships between variables, while <strong>standardization</strong> allows us to compare variables on a common scale. These concepts provide essential tools for understanding the behavior of random variables and for analyzing data distributions in practice.</p>
<section id="expectation">
<h3>Expectation<a class="headerlink" href="#expectation" title="Permalink to this heading">#</a></h3>
<p>When it is known that a random variable follows a probability
distribution, <strong>it is possible to characterize that variable</strong> (and
hence the related probability distribution) <strong>with some statistics</strong>.</p>
<p>The most straightforward of them is the expectation. <strong>The concept of
expectation is very related to the concept of mean.</strong> When we compute
the mean of a given set of numbers, we usually sum all the numbers
together and then divide by the total.</p>
<p>Since a probability distribution tell us which values will be more
frequent than others, we compute this mean with a <strong>weighted average</strong>,
where the weights are given by the probability distribution.</p>
<p>Specifically, we can define the expectation of a <strong>discrete random variable</strong> X as
follows:</p>
<div class="math notranslate nohighlight">
\[E_{X\sim P}\lbrack X\rbrack = \sum_{x \in \Omega}^{}{xP(x)}\]</div>
<p>In the case of <strong>continuous variables</strong>, the expectation takes the form of
an integral:</p>
<div class="math notranslate nohighlight">
\[E_{X \sim P}\lbrack X\rbrack = \int_{x \in \Omega} xf(x)dx\]</div>
<p>The diagram below shows an example of expectation:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/a3883f3aa39b0c5b645807de74c1997d2888ddc0da960a19655f993f84a6c3d5.png" src="../_images/a3883f3aa39b0c5b645807de74c1997d2888ddc0da960a19655f993f84a6c3d5.png" />
</div>
</div>
</section>
<section id="variance-and-standard-deviation">
<h3>Variance and Standard Deviation<a class="headerlink" href="#variance-and-standard-deviation" title="Permalink to this heading">#</a></h3>
<p>The variance gives a measure of how much variability there is in a
variable <span class="math notranslate nohighlight">\(X\)</span> around its mean <span class="math notranslate nohighlight">\(E\lbrack X\rbrack\)</span>.</p>
<p>Similarly to the concept of variance as a descriptive measure, we can define the variance in this context as follows:</p>
<div class="math notranslate nohighlight">
\[Var_{X \sim P}[X] = E_{X \sim P}[(X-E_{X \sim P}[X])^2]\]</div>
<p>As seen with data description, to make the measure of spread more interpretable, we often use the <strong>standard deviation</strong>, which is simply the square root of the variance:</p>
<div class="math notranslate nohighlight">
\[
Std_{X \sim P}[X]=\sqrt{Var_{X \sim P}[X]}
\]</div>
<p>Standard deviation tells us, on average, how far values tend to deviate from the mean, using the same units as the original variable.</p>
<p>The diagram below shows an example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/e19d314541d601ad55060773ffddd3c1d69473ae1abbb34e1521b6e311bd8a96.png" src="../_images/e19d314541d601ad55060773ffddd3c1d69473ae1abbb34e1521b6e311bd8a96.png" />
</div>
</div>
</section>
<section id="covariance">
<h3>Covariance<a class="headerlink" href="#covariance" title="Permalink to this heading">#</a></h3>
<p>The covariance gives a measure of how two variables are linearly related
to each other. It allows to <strong>measure to what extent the increase of one
of the variables corresponds to an increase of the value of the other
one</strong>.</p>
<p>Given two random variables <span class="math notranslate nohighlight">\(X \sim P_X\)</span> and <span class="math notranslate nohighlight">\(Y \sim P_Y\)</span>, the covariance is defined as
follows:</p>
<div class="math notranslate nohighlight">
\[Cov_{X \sim P_X, Y \sim P_Y}(X,Y) = E_{X \sim P_X, Y \sim P_Y}\lbrack\left( X - E_{X \sim P_X}\lbrack X\rbrack \right)\left( Y - E_{Y \sim P_Y}\lbrack Y\rbrack \right)\rbrack\]</div>
<p>We can distinguish the following terms:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\lbrack X\rbrack\)</span> and <span class="math notranslate nohighlight">\(E\lbrack Y\rbrack\)</span> are the expectations of
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((X - E\lbrack X\rbrack)\)</span> and <span class="math notranslate nohighlight">\((Y - E\lbrack Y\rbrack)\)</span> are the
differences between the samples and the expected values.</p></li>
<li><p><span class="math notranslate nohighlight">\(\left( X - E\lbrack X\rbrack \right)\left( Y - E\lbrack Y\rbrack \right)\)</span>
computes the product between the differences.</p></li>
</ul>
<p>We have:</p>
<ul class="simple">
<li><p>If the signs of the terms agree, the product is positive.</p></li>
<li><p>If the signs of the terms disagree, the product is negative.</p></li>
</ul>
<p>In practice, if when <span class="math notranslate nohighlight">\(X\)</span> is larger than the mean, then <span class="math notranslate nohighlight">\(Y\)</span>
is larger than the mean and vice versa, when <span class="math notranslate nohighlight">\(X\)</span> is lower
than the mean then <span class="math notranslate nohighlight">\(Y\)</span> is lower than the mean, then the two
variables are <em>correlated,</em> and the covariance is high.</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a multi-dimensional variable
<span class="math notranslate nohighlight">\(X = \lbrack X_{1},X_{2},\ldots,X_{n}\rbrack\)</span>, we can compute all the
possible covariances between variable pairs:
<span class="math notranslate nohighlight">\(Cov\lbrack X_{i},X_{j}\rbrack\)</span>. This allows to create a matrix, which
is generally referred to as <strong>the covariance matrix</strong>. The general term
of the covariance matrix <span class="math notranslate nohighlight">\(Cov(X)\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[Cov(X)_{i,j} = \Sigma_{ij} = Cov(X_{i},X_{j})\]</div>
</section>
<section id="entropy">
<h3>Entropy<a class="headerlink" href="#entropy" title="Permalink to this heading">#</a></h3>
<p>Another fundamental way to characterize a probability distribution is through <strong>entropy</strong>, which measures the average uncertainty or unpredictability associated with the possible values of a random variable.</p>
<section id="self-information">
<h4>Self-Information<a class="headerlink" href="#self-information" title="Permalink to this heading">#</a></h4>
<p>To understand entropy, we first introduce the concept of <strong>self-information</strong>. The self-information of an event <span class="math notranslate nohighlight">\(x\)</span> with probability <span class="math notranslate nohighlight">\(P(x)\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
I(x) = -\log_2 P(x)
\]</div>
<p>(we will use <span class="math notranslate nohighlight">\(\log\)</span> in the future in this context to denote <span class="math notranslate nohighlight">\(\log_2\)</span> for brevity)</p>
<p>This definition is motivated by several properties of the logarithm:</p>
<ul class="simple">
<li><p>The logarithm makes self-information <strong>additive</strong> for independent events: the information gained from observing two independent events is the sum of their individual self-informations.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
I(X = x,Y = y) = - \log\left\lbrack P(X = x,Y = y) \right\rbrack = \]</div>
<div class="math notranslate nohighlight">
\[
= \log\left\lbrack P(X = x)P(Y = y) \right\rbrack = - logP(X = x) - \log P(Y = y) = I(x) + I(y)
\]</div>
<ul class="simple">
<li><p>The negative logarithm reflects the intuition that <strong>rare events are more informative</strong>: as <span class="math notranslate nohighlight">\(P(x)\)</span> decreases, <span class="math notranslate nohighlight">\(I(x)\)</span> increases (see plot below)</p></li>
<li><p>The base 2 logarithm measures information in <strong>bits</strong>, which is standard in information theory</p></li>
</ul>
<p>For example, if an event is certain (<span class="math notranslate nohighlight">\(P(x)=1\)</span>), <span class="math notranslate nohighlight">\(I(x)=0\)</span> (no new information). If an event is very unlikely (<span class="math notranslate nohighlight">\(P(x)\to 0\)</span>), <span class="math notranslate nohighlight">\(I(x)\to \infty\)</span> (maximum surprise).</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c1ab2eba39d5c9398d7cf4d9ed0976a873edd6d01cf38d96283a8698bcce0a6b.png" src="../_images/c1ab2eba39d5c9398d7cf4d9ed0976a873edd6d01cf38d96283a8698bcce0a6b.png" />
</div>
</div>
</section>
<section id="entropy-of-a-distribution">
<h4>Entropy of a Distribution<a class="headerlink" href="#entropy-of-a-distribution" title="Permalink to this heading">#</a></h4>
<p>The <strong>entropy</strong> of a random variable <span class="math notranslate nohighlight">\(X\)</span> is the average self-information over all possible values of <span class="math notranslate nohighlight">\(X\)</span>, weighted by their probabilities (i.e., the expected self-information):</p>
<ul class="simple">
<li><p>For discrete variables:
$<span class="math notranslate nohighlight">\(
H(X) = -\sum_{x \in \Omega} P(x) \log_2 P(x)
\)</span>$</p></li>
<li><p>For continuous variables (differential entropy):
$<span class="math notranslate nohighlight">\(
h(X) = -\int_{x \in \Omega} f(x) \log_2 f(x) dx
\)</span>$</p></li>
</ul>
<p>Entropy is highest when the distribution is uniform (maximum uncertainty) and lowest when the distribution is concentrated on a few values (minimum uncertainty).</p>
<blockquote>
<div><p><strong>Note:</strong> Entropy is measured in bits if the logarithm base 2 is used and nats if we use a natural logarithm (base <span class="math notranslate nohighlight">\(e\)</span>).</p>
</div></blockquote>
</section>
<section id="id5">
<h4>Example<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>A fair coin (<span class="math notranslate nohighlight">\(P(head)=0.5\)</span>, <span class="math notranslate nohighlight">\(P(tail)=0.5\)</span>): <span class="math notranslate nohighlight">\(H(X) = 1\)</span> bit.</p></li>
<li><p>A highly biased coin (<span class="math notranslate nohighlight">\(P(head)=0.99\)</span>, <span class="math notranslate nohighlight">\(P(tail)=0.01\)</span>): <span class="math notranslate nohighlight">\(H(X) \approx 0.08\)</span> bits.</p></li>
</ul>
<p>Entropy thus provides a synthetic measure of the uncertainty of a distribution, useful for comparing different random variables and for applications in information theory and machine learning.</p>
</section>
<section id="entropy-of-a-bernoulli-variable">
<h4>Entropy of a Bernoulli variable<a class="headerlink" href="#entropy-of-a-bernoulli-variable" title="Permalink to this heading">#</a></h4>
<p>Let’s consider a variable <span class="math notranslate nohighlight">\(X\)</span> following a Bernoulli distribution with probability <span class="math notranslate nohighlight">\(p\)</span>, i.e.:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X_p = \begin{cases}
1 &amp; \text{with probability } p\\
0 &amp; \text{with probability } 1-p
\end{cases}
\end{split}\]</div>
<p>Then the entropy of this variable will be:</p>
<div class="math notranslate nohighlight">
\[H(X_p) = -p\log p -(-1-p)\log(1-p)\]</div>
<p>If we plot <span class="math notranslate nohighlight">\(H(X_p)\)</span> with respect to <span class="math notranslate nohighlight">\(p\)</span>, we obtain the following graph:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/d190a05653ee5e0bef3c1b7b7c4142a91e8cdf617f694b2c8a4917ac240b32f5.png" src="../_images/d190a05653ee5e0bef3c1b7b7c4142a91e8cdf617f694b2c8a4917ac240b32f5.png" />
</div>
</div>
</section>
</section>
</section>
<section id="standardization">
<h2>Standardization<a class="headerlink" href="#standardization" title="Permalink to this heading">#</a></h2>
<p>Standardization transforms a random variable <span class="math notranslate nohighlight">\(X\)</span> into a variable <span class="math notranslate nohighlight">\(Z\)</span> so that it has:</p>
<ul class="simple">
<li><p>Expectation equal to zero: <span class="math notranslate nohighlight">\(E(Z) = 0\)</span>.</p></li>
<li><p>Variance equal to one: <span class="math notranslate nohighlight">\(Var(Z) = 1\)</span>.</p></li>
</ul>
<p>The standardized variable will be:</p>
<div class="math notranslate nohighlight">
\[Z = \frac{X - \mu_X}{\sigma_X} = \frac{X-E[X]}{\sqrt{Var[X]}}\]</div>
<p>The plot below shows a data distribution before and after standardization.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/492db2aa558de91b648ff53f0b208372cd03aaf352c3bf2320e8a6c5d7ab30dd.png" src="../_images/492db2aa558de91b648ff53f0b208372cd03aaf352c3bf2320e8a6c5d7ab30dd.png" />
</div>
</div>
</section>
<section id="fitting-distributions-to-data">
<h2>Fitting Distributions to Data<a class="headerlink" href="#fitting-distributions-to-data" title="Permalink to this heading">#</a></h2>
<p>Often, we want to fit a theoretical probability distribution to observed data. This means estimating the parameters of the distribution so that it best describes the data. In Python, this can be done easily using <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code>.</p>
<p>Below is an example of fitting a normal (Gaussian) distribution to a dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Load the Titanic dataset</span>
<span class="n">titanic</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;PassengerId&#39;</span><span class="p">)</span>

<span class="c1"># Extract the Age column and drop missing values</span>
<span class="n">ages</span> <span class="o">=</span> <span class="n">titanic</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Fit a normal distribution to the Age data</span>
<span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ages</span><span class="p">)</span>

<span class="c1"># Plot the histogram of Age</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ages</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Age Data&#39;</span><span class="p">)</span>

<span class="c1"># Plot the fitted normal distribution</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ages</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">ages</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">pdf_fitted</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu_hat</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma_hat</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf_fitted</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Fitted Normal</span><span class="se">\n</span><span class="s1">$\mu$=</span><span class="si">{</span><span class="n">mu_hat</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, $\sigma$=</span><span class="si">{</span><span class="n">sigma_hat</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Fit of a Normal Distribution to Titanic Age Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/63ecd7afbb42b87cb5e9932fc8eb1621eed7bc024fdd0c0d62bffa7f03fdb167.png" src="../_images/63ecd7afbb42b87cb5e9932fc8eb1621eed7bc024fdd0c0d62bffa7f03fdb167.png" />
</div>
</div>
<p>We can do the same with other distribution. For instance, with the binomial distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="c1"># Load the Titanic dataset</span>
<span class="n">titanic</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;PassengerId&#39;</span><span class="p">)</span>

<span class="c1"># Encode &#39;Sex&#39;: female=1, male=0</span>
<span class="n">sex_numeric</span> <span class="o">=</span> <span class="p">(</span><span class="n">titanic</span><span class="p">[</span><span class="s1">&#39;Sex&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Estimate p (probability of being female)</span>
<span class="n">p_hat</span> <span class="o">=</span> <span class="n">sex_numeric</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sex_numeric</span><span class="p">)</span>

<span class="c1"># Binomial PMF for n trials (here, n=1 for each passenger)</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p_hat</span><span class="p">)</span>

<span class="c1"># Plot the PMF</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="s1">&#39;Male&#39;</span><span class="p">,</span> <span class="s1">&#39;Female&#39;</span><span class="p">],</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;pink&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Binomial Distribution Fit for Sex</span><span class="se">\n</span><span class="s1">Estimated p = </span><span class="si">{</span><span class="n">p_hat</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sex&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9012a7995f0d373a5a95d6ea2a095a121f81c39a24aa61a7296b112c79db70b5.png" src="../_images/9012a7995f0d373a5a95d6ea2a095a121f81c39a24aa61a7296b112c79db70b5.png" />
</div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Parts of chapter 1 of [1];</p></li>
<li><p>Most of chapter 3 of [2];</p></li>
<li><p>Parts of chapter 8 of [3].</p></li>
</ul>
<p>[1] Bishop, Christopher M. <em>Pattern recognition and machine learning</em>.
springer, 2006.
<a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>
<p>[2] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <em>Deep
learning</em>. MIT press, 2016. <a class="reference external" href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></p>
<p>[3] Heumann, Christian, and Michael Schomaker Shalabh. Introduction to statistics and data analysis. Springer International Publishing Switzerland, 2016.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="04_association_between_variables.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Association between variables</p>
      </div>
    </a>
    <a class="right-next"
       href="06_statistical_inference.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Statistical Inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-variables">Discrete Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-mass-functions-pmf">Probability Mass Functions (PMF)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-fair-coin">Example: Probability Mass Function for a Fair Coin</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-biased-coin">Example: Probability Mass Function for a Biased Coin</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-probability-mass-functions-from-data">Computing Probability Mass Functions from Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-probability-mass-function">Exercise: Probability Mass Function</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-distribution-functions-of-discrete-variables">Cumulative Distribution Functions of Discrete Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-pmf-and-cdf-of-a-fair-die">Example – PMF and CDF of a fair die</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-cdfs-for-discrete-variables-from-data">Computing CDFs for Discrete Variables from Data</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-variables">Continuous Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions-pdf">Probability Density Functions (PDF)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-uniform-pdf">Example: Uniform PDF</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-a-pdf-with-density-histograms-and-density-estimates">Approximating a PDF with Density Histograms and Density Estimates</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-distribution-functions-cdf">Cumulative Distribution Functions (CDF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-probability-distributions">Common Probability Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-uniform-distribution">Discrete Uniform Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bernoulli-distribution">Bernoulli Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-distribution">Binomial Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-distribution">Categorical Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-distribution">Multinomial Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-distribution">Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#central-limit-theorem">Central Limit Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian">Multivariate Gaussian</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-sigma">Effect of <span class="math notranslate nohighlight">\(\Sigma\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-gaussian-distribution">Estimation of the Parameters of a Gaussian Distribution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#describing-data-distributions">Describing Data Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation">Expectation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-and-standard-deviation">Variance and Standard Deviation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self-information">Self-Information</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-of-a-distribution">Entropy of a Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-of-a-bernoulli-variable">Entropy of a Bernoulli variable</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization">Standardization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-distributions-to-data">Fitting Distributions to Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>