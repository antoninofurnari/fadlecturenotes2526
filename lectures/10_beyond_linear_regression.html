

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Beyond Linear Regression &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/10_beyond_linear_regression';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Linear Regression" href="09_linear_regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamentals of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">Introduction to the Labs and Work Environment Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_python_crash_course.html">Python Crash Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_python_data_science_crash_course.html">Python for Data Science Crash Course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro_data_analysis.html">Data Analysis Key Concepts, Loading and Inspecting the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="02_describing_and_visualizing_the_data.html">Describing and Visualizing the Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_probability_for_data_analysis.html">Probability for Data Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_association_between_variables.html">Association between variables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05_data_distributions.html">Data Distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_statistical_inference.html">Statistical Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="07_storytelling_with_data.html">Storytelling with Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_predictive_modeling.html">Introduction to Predictive Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_linear_regression.html">Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 11</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Beyond Linear Regression</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes2526/blob/master/lecturenotes/lectures/10_beyond_linear_regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes2526/issues/new?title=Issue%20on%20page%20%2Flectures/10_beyond_linear_regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/10_beyond_linear_regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Beyond Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-statistical-fixes-extending-the-linear-model">The “Statistical” Fixes: Extending the Linear Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interaction-terms">Interaction Terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-interaction">Interpreting the Interaction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-and-polynomial-regression">Quadratic and Polynomial Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-regression">Polynomial Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-understanding-to-prediction-the-machine-learning-perspective">From Understanding to Prediction: The Machine Learning Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-prediction-test-set-error">Metrics for Prediction (Test Set Error)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">1. Mean Squared Error (MSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">2. Root Mean Squared Error (RMSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">3. Mean Absolute Error (MAE)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-with-power-overfitting">The Problem with Power: Overfitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-regularization">The Solution: Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#revisiting-collinearity">Revisiting Collinearity</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-l2-penalty">Ridge Regression (L2 Penalty)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-ridge-regression-coefficients">Interpretation of the ridge regression coefficients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression-l-1-penalty">Lasso Regression (<span class="math notranslate nohighlight">\(L_1\)</span> Penalty)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-and-the-bias-variance-tradeoff">Regularization and the Bias-Variance Tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lambda-is-the-tradeoff-knob"><span class="math notranslate nohighlight">\(\lambda\)</span> is the “Tradeoff Knob”</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-theory-to-practice-the-scikit-learn-workflow">From Theory to Practice: The <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> Workflow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#our-new-toolkit">Our New Toolkit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-splitting">Data Splitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-normalization">Data Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regressor">Linear Regressor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression">Non-Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regularization">Ridge Regularization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">Lasso Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">Grid Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-pipelines">Scikit-Learn Pipelines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search-with-cross-validation">Grid Search with Cross Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-regression-algorithms">Other Regression Algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-and-model-selection">Comparison and Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="beyond-linear-regression">
<h1>Beyond Linear Regression<a class="headerlink" href="#beyond-linear-regression" title="Permalink to this heading">#</a></h1>
<p>In our last lecture, we did a deep, “statistical” dive into Linear Regression using <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>. Our goal was <strong>understanding</strong> and <strong>inference</strong>.</p>
<ul class="simple">
<li><p>We learned to interpret coefficients (<span class="math notranslate nohighlight">\(\beta\)</span>), p-values, and <span class="math notranslate nohighlight">\(R^2\)</span>.</p></li>
<li><p>We diagnosed our models using residual plots.</p></li>
</ul>
<p>This process revealed two major problems with our simple models:</p>
<ol class="arabic simple">
<li><p><strong>A “U-Shape” in our residuals:</strong> When modeling <code class="docutils literal notranslate"><span class="pre">mpg</span> <span class="pre">~</span> <span class="pre">horsepower</span></code>, the residual plot showed a clear non-linear pattern. This means our model was <strong>underfitting</strong> (high bias) because the real-world relationship <em>isn’t a line</em>.</p></li>
<li><p><strong>Multicollinearity:</strong> When we used multiple predictors (like <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code>), we saw that their high correlation made their p-values unstable, confusing our interpretation.</p></li>
</ol>
<p>Today, we will fix these problems by extending the linear model. In doing so, we will see that we start to lose simple interpretability. This loss will be our pivot to a new perspective: the <strong>Machine Learning</strong> approach, where our primary goal shifts from <em>understanding</em> to <strong>predictive accuracy</strong>.</p>
<section id="the-statistical-fixes-extending-the-linear-model">
<h2>The “Statistical” Fixes: Extending the Linear Model<a class="headerlink" href="#the-statistical-fixes-extending-the-linear-model" title="Permalink to this heading">#</a></h2>
<p>Let’s first try to fix our model while still staying in the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> (statistical) world.</p>
<section id="interaction-terms">
<h3>Interaction Terms<a class="headerlink" href="#interaction-terms" title="Permalink to this heading">#</a></h3>
<p>A simple linear model assumes the effect of each predictor on <span class="math notranslate nohighlight">\(Y\)</span> is <strong>additive</strong>. The formula</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 \text{horsepower} + \beta_2 \text{weight}\]</div>
<p>assumes that the effect of 1 unit of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> (which is <span class="math notranslate nohighlight">\(\beta_1\)</span>) is <strong>constant</strong>, regardless of the car’s <code class="docutils literal notranslate"><span class="pre">weight</span></code>.</p>
<p>Is that true? We can test this by adding an <strong>interaction term</strong>:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 \text{horsepower} + \beta_2 \text{weight} + \beta_3 (\text{horsepower} \times \text{weight})\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="c1"># Load and clean the data (same as last lecture)</span>
<span class="n">mpg</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;mpg&#39;</span><span class="p">)</span>
<span class="n">mpg</span> <span class="o">=</span> <span class="n">mpg</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;horsepower&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">mpg</span><span class="p">[</span><span class="s1">&#39;horsepower&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;coerce&#39;</span><span class="p">)</span>
<span class="n">mpg</span> <span class="o">=</span> <span class="n">mpg</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Fit the simple additive model</span>
<span class="n">model_additive</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ horsepower + weight&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Fit the new interaction model</span>
<span class="n">model_interaction</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;mpg ~ horsepower + weight + horsepower * weight&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">mpg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- Additive Model (R-squared) ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adj. R-squared: </span><span class="si">{</span><span class="n">model_additive</span><span class="o">.</span><span class="n">rsquared_adj</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Interaction Model (R-squared) ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adj. R-squared: </span><span class="si">{</span><span class="n">model_interaction</span><span class="o">.</span><span class="n">rsquared_adj</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Interaction Model Coefficients ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_interaction</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Additive Model (R-squared) ---
Adj. R-squared: 0.7049

--- Interaction Model (R-squared) ---
Adj. R-squared: 0.7465

--- Interaction Model Coefficients ---
=====================================================================================
                        coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------------
Intercept            63.5579      2.343     27.127      0.000      58.951      68.164
horsepower           -0.2508      0.027     -9.195      0.000      -0.304      -0.197
weight               -0.0108      0.001    -13.921      0.000      -0.012      -0.009
horsepower:weight  5.355e-05   6.65e-06      8.054      0.000    4.05e-05    6.66e-05
=====================================================================================
</pre></div>
</div>
</div>
</div>
</section>
<section id="interpreting-the-interaction">
<h3>Interpreting the Interaction<a class="headerlink" href="#interpreting-the-interaction" title="Permalink to this heading">#</a></h3>
<p>Notice two things:</p>
<ol class="arabic simple">
<li><p>The <strong>Adj. R-squared</strong> increased! This indicates the interaction model is a better fit.</p></li>
<li><p>The p-value for the <code class="docutils literal notranslate"><span class="pre">horsepower:weight</span></code> term is tiny (<span class="math notranslate nohighlight">\(&lt; 0.001\)</span>). This confirms the interaction is statistically significant.</p></li>
</ol>
<p>But how do we interpret the coefficient for <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>?
We can rewrite the formula:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + (\beta_1 + \beta_3 \times \text{weight}) \times \text{horsepower} + \beta_2 \text{weight}\]</div>
<p>The “effect” of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is no longer a single number, <span class="math notranslate nohighlight">\(\beta_1\)</span>. It is now <span class="math notranslate nohighlight">\((\beta_1 + \beta_3 \times \text{weight})\)</span>. This means the effect of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> <em>depends on</em> the <code class="docutils literal notranslate"><span class="pre">weight</span></code> of the car.</p>
<p>We have gained accuracy, but at the cost of simple, at-a-glance interpretability. This is the first “crack” in our “glass box” model.</p>
<p>Let’s diagnose our residuals to see if we gained something:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/b2933ef41686ba71c80efa949b15ca33b4bbee83c450eb73fb12c50c1137a953.png" src="../_images/b2933ef41686ba71c80efa949b15ca33b4bbee83c450eb73fb12c50c1137a953.png" />
</div>
</div>
<p>The situation improved! We now have a better model, even if not perfect.</p>
</section>
</section>
<section id="quadratic-and-polynomial-regression">
<h2>Quadratic and Polynomial Regression<a class="headerlink" href="#quadratic-and-polynomial-regression" title="Permalink to this heading">#</a></h2>
<p>We will now take another route to address our nonlinear problem. Let’s have a look at the relationship between <code class="docutils literal notranslate"><span class="pre">mpg</span></code> and <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;horsepower&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;mpg&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/31a2530de4026f64414dbbf93308e914fb1ec22664ff0647904fdcfc823a9ceb.png" src="../_images/31a2530de4026f64414dbbf93308e914fb1ec22664ff0647904fdcfc823a9ceb.png" />
</div>
</div>
<p>This relationship does not look linear. It looks instead as a quadratic, which would be fit by the following model:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 horsepower^2\]</div>
<p>Again, the model above is nonlinear in <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>, but we can still fit it with a linear regressor if we add a new variable <span class="math notranslate nohighlight">\(z=horsepower^2\)</span>.</p>
<p>The fit model will obtain <span class="math notranslate nohighlight">\(R^2=0.688\)</span>, larger than <span class="math notranslate nohighlight">\(R^2=608\)</span> obtained by the base model (<span class="math notranslate nohighlight">\(mpg = \beta_0 + \beta_1 horsepower\)</span>). Both have a large F-statistic.</p>
<p>The estimated coefficients are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;mpg ~ horsepower + I(horsepower**2)&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>          <td>   56.9001</td> <td>    1.800</td> <td>   31.604</td> <td> 0.000</td> <td>   53.360</td> <td>   60.440</td>
</tr>
<tr>
  <th>horsepower</th>         <td>   -0.4662</td> <td>    0.031</td> <td>  -14.978</td> <td> 0.000</td> <td>   -0.527</td> <td>   -0.405</td>
</tr>
<tr>
  <th>I(horsepower ** 2)</th> <td>    0.0012</td> <td>    0.000</td> <td>   10.080</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>
</tr>
</table></div></div>
</div>
<p>The coefficients now describe the quadratic:</p>
<div class="math notranslate nohighlight">
\[y = 58.9001 - 0.4662 x + 0.0012 x^2\]</div>
<p>If we plot it on the data, we obtain the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">=</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;mpg ~ horsepower + I(horsepower**2)&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;horsepower&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;mpg&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span><span class="mi">240</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">b0</span><span class="o">+</span><span class="n">b1</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b2</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c9695e2bd2e60e7d12ed3c6aa209f8225515e35ba9ff11a5db9e5cda134f3b4f.png" src="../_images/c9695e2bd2e60e7d12ed3c6aa209f8225515e35ba9ff11a5db9e5cda134f3b4f.png" />
</div>
</div>
<section id="polynomial-regression">
<h3>Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Permalink to this heading">#</a></h3>
<p>In general, we can fit a polynomial model to the data, choosing a suitable degree <span class="math notranslate nohighlight">\(d\)</span>. For instance, for <span class="math notranslate nohighlight">\(d=4\)</span> we have:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 horsepower^2 + \beta_3 horsepower^3 + \beta_4 horsepower^4\]</div>
<p>which identifies the following fit:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">=</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;mpg ~ horsepower + I(horsepower**2) + I(horsepower**3) + I(horsepower**4)&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span><span class="p">,</span><span class="n">b4</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;horsepower&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;mpg&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span><span class="mi">240</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">b0</span><span class="o">+</span><span class="n">b1</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b2</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">b3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="n">b4</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">4</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/32394701a73ab876c001642d0fee61759947ad13e79076b10b48f404e102d68d.png" src="../_images/32394701a73ab876c001642d0fee61759947ad13e79076b10b48f404e102d68d.png" />
</div>
</div>
<p>This approach is known as <strong>polynomial regression</strong> and allows to turn the linear regression into a nonlinear model. Note that, when we have more variables, polynomial regression also includes interaction terms. For instance, the linear model:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1x + \beta_2y\]</div>
<p>becomes the following polynomial model of degree <span class="math notranslate nohighlight">\(2\)</span>:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1x + \beta_2y + \beta_3x^2 + \beta_4y^2 + \beta_5 xy\]</div>
<p>As usual, we only have to add new variables for the squared and interaction term and solve the problem as a linear regression one. This is easily handled by libraries. Note that, as the number of variables increases, the number of terms to add for a given degree also increases.</p>
<blockquote>
<div><p>The key observation here is that, while the polynomial regression function is <strong>nonlinear in the variable</strong>, it is still <strong>linear in its coefficients</strong>, which makes it still solvable with ordinary least squares.</p>
</div></blockquote>
</section>
</section>
<section id="from-understanding-to-prediction-the-machine-learning-perspective">
<h2>From Understanding to Prediction: The Machine Learning Perspective<a class="headerlink" href="#from-understanding-to-prediction-the-machine-learning-perspective" title="Permalink to this heading">#</a></h2>
<p>The examples above show how quadratic and polynomial regressions allow to obtain <strong>excellent fits</strong> to the data, with improvements in terms of <code class="docutils literal notranslate"><span class="pre">Adj.</span> <span class="pre">R-squared</span></code> and significant p-vlaues.</p>
<p>Let’s consider the quadratic fit in particular. The <code class="docutils literal notranslate"><span class="pre">Adj.</span> <span class="pre">R-squared</span></code> (0.686) is much higher than the simple linear model’s (0.605). And the p-values for both <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">I(horsepower**2)</span></code> are <span class="math notranslate nohighlight">\(&lt; 0.001\)</span>.</p>
<p>This is a clear win, but at what cost?</p>
<blockquote>
<div><p><strong>We have lost all simple interpretability.</strong></p>
</div></blockquote>
<p>What is the “effect” of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> on <code class="docutils literal notranslate"><span class="pre">mpg</span></code>? It’s not <span class="math notranslate nohighlight">\(\beta_1\)</span>. From calculus, the effect of a 1-unit change in <span class="math notranslate nohighlight">\(X\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> is the derivative:
<span class="math notranslate nohighlight">\(\frac{d(mpg)}{d(horsepower)} = \beta_1 + 2\beta_2 \times \text{horsepower}\)</span></p>
<p>The effect is not a single number. It <em>depends on the value of horsepower</em>. We can’t just look at the table and give a simple explanation.</p>
<p><strong>This is the pivot.</strong> If we are willing to sacrifice simple, at-a-glance interpretability in exchange for a model that <em>fits better</em>… why stop here?</p>
<p>If our main goal is no longer <em>understanding</em> (p-values) but <strong>predictive accuracy</strong>, we should embrace this. We have just crossed the line from statistical inference into the realm of <strong>Machine Learning</strong>.</p>
<section id="metrics-for-prediction-test-set-error">
<h3>Metrics for Prediction (Test Set Error)<a class="headerlink" href="#metrics-for-prediction-test-set-error" title="Permalink to this heading">#</a></h3>
<p>When our goal is <strong>prediction</strong> (the ML approach), we do not care about <span class="math notranslate nohighlight">\(R^2\)</span> or RSE on the training data. We <em>only</em> care about the model’s performance on <strong>new, unseen data</strong> (the <strong>Test Set</strong>).</p>
<p>The following metrics are used to evaluate the model on the test set. (We will use <span class="math notranslate nohighlight">\(m\)</span> to denote the number of samples in the test set).</p>
<section id="mean-squared-error-mse">
<h4>1. Mean Squared Error (MSE)<a class="headerlink" href="#mean-squared-error-mse" title="Permalink to this heading">#</a></h4>
<p>The MSE is the average of the squared errors on the test set. It is the “predictive” version of the <span class="math notranslate nohighlight">\(R_{emp}\)</span> (Empirical Risk) we saw earlier.</p>
<div class="math notranslate nohighlight">
\[MSE_{test} = \frac{1}{m}\sum_{j=1}^{m} (y_j - \hat{y}_j)^2\]</div>
<ul class="simple">
<li><p><strong>Interpretation:</strong> This is an <strong>error measure</strong>, so a good model has a <strong>small MSE</strong>.</p></li>
<li><p><strong>Problem:</strong> The units are <strong>squared</strong> (e.g., if <span class="math notranslate nohighlight">\(y\)</span> is in meters, MSE is in square-meters). This is not intuitive.</p></li>
</ul>
</section>
<section id="root-mean-squared-error-rmse">
<h4>2. Root Mean Squared Error (RMSE)<a class="headerlink" href="#root-mean-squared-error-rmse" title="Permalink to this heading">#</a></h4>
<p>The RMSE is the fix for the MSE’s unit problem and is the most common metric for predictive regression. It is simply the square root of the MSE.</p>
<div class="math notranslate nohighlight">
\[RMSE_{test} = \sqrt{MSE_{test}} = \sqrt{\frac{1}{m}\sum_{j=1}^{m} (y_j - \hat{y}_j)^2}\]</div>
<ul class="simple">
<li><p><strong>Interpretation:</strong> RMSE is in the <strong>same units as <span class="math notranslate nohighlight">\(Y\)</span></strong> (e.g., meters). It can be read as “On average, our model’s predictions on the test set are wrong by about [RMSE value].”</p></li>
<li><p><strong>Key Property:</strong> Because it squares errors before averaging, the RMSE <strong>penalizes large errors</strong> (outliers) more heavily than small errors.</p></li>
</ul>
</section>
<section id="mean-absolute-error-mae">
<h4>3. Mean Absolute Error (MAE)<a class="headerlink" href="#mean-absolute-error-mae" title="Permalink to this heading">#</a></h4>
<p>The MAE is an alternative to RMSE that is also highly interpretable.</p>
<div class="math notranslate nohighlight">
\[MAE_{test} = \frac{1}{m}\sum_{j=1}^{m} |y_j - \hat{y}_j|\]</div>
<ul class="simple">
<li><p><strong>Interpretation:</strong> MAE is also in the <strong>same units as <span class="math notranslate nohighlight">\(Y\)</span></strong>. It measures the “average absolute error” of the predictions.</p></li>
<li><p><strong>Key Property:</strong> Unlike RMSE, MAE does <em>not</em> disproportionately penalize large errors. It is <strong>more robust to outliers</strong>.</p></li>
</ul>
<p>Let’s revisit a plot we saw in our last lecture, adding prediction error measures to the picture:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/6f6c3e5e0e732354712b7b5c96d64a51e934ce283389c5430447a75cb4737518.png" src="../_images/6f6c3e5e0e732354712b7b5c96d64a51e934ce283389c5430447a75cb4737518.png" />
</div>
</div>
</section>
</section>
</section>
<section id="the-problem-with-power-overfitting">
<h2>The Problem with Power: Overfitting<a class="headerlink" href="#the-problem-with-power-overfitting" title="Permalink to this heading">#</a></h2>
<p>We’ve solved our underfitting problem! By adding a <span class="math notranslate nohighlight">\(horsepower^2\)</span> term in <strong>our quadratic regressione example</strong>, we created a model with a <strong>higher capacity</strong> (more flexibility) that fits the data better.</p>
<p>But this creates a new, dangerous question:</p>
<blockquote>
<div><p><strong>If <span class="math notranslate nohighlight">\(degree=2\)</span> is good, is <span class="math notranslate nohighlight">\(degree=5\)</span> better? What about <span class="math notranslate nohighlight">\(degree=20\)</span>?</strong></p>
</div></blockquote>
<p>A high-degree polynomial model has <em>enormous</em> capacity. It can “wiggle” as much as it needs to fit every single data point. This leads to the central problem of machine learning: <strong>overfitting</strong>.</p>
<p><strong>Overfitting</strong> (or <strong>high variance</strong>) is when a model learns the <em>noise</em> in the training data, not the underlying <em>signal</em>. It will have a fantastic <span class="math notranslate nohighlight">\(R^2\)</span> on the data it was trained on, but will fail (generalize) miserably on any new data.</p>
<p>Look at the example below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/30d09c21fb508be5bb95e5f52d3a438a387eae29a7645c125c1045551a8e0d71.png" src="../_images/30d09c21fb508be5bb95e5f52d3a438a387eae29a7645c125c1045551a8e0d71.png" />
</div>
</div>
<p>We can see that, while the linear model is clearly <strong>underfitting</strong>, the high-degree polynomial is <strong>overfitting</strong>! The quadratic model seems to be in the sweet spot, but how shall we find the correct degree in a systematic way?</p>
<section id="the-solution-regularization">
<h3>The Solution: Regularization<a class="headerlink" href="#the-solution-regularization" title="Permalink to this heading">#</a></h3>
<p>The plot above shows our dilemma: the <span class="math notranslate nohighlight">\(degree=15\)</span> model is “too powerful.” How can we use a high-capacity model <em>without</em> it overfitting?</p>
<p>Unfortunately, as the model becomes complex, it’s not straightforward to use plain statistics to answer this question.</p>
<p>Instead, following a pure machine learning perspective, we will use a technique called <strong>regularization</strong>.</p>
<p><strong>Regularization adds a “penalty”</strong> to the cost function to <em>discourage</em> the model from becoming too complex.</p>
<p>Instead of just minimizing <span class="math notranslate nohighlight">\(RSS\)</span>, we minimize</p>
<div class="math notranslate nohighlight">
\[RSS + \text{Penalty Term}\]</div>
<p>This penalty term is designed to <strong>force the <span class="math notranslate nohighlight">\(\beta\)</span> coefficients to stay small</strong>. A “wiggly” (overfit) curve <em>requires</em> massive <span class="math notranslate nohighlight">\(\beta\)</span> coefficients. By penalizing large coefficients, we force the model to find a <em>simpler, smoother</em> curve, even if it has many features.</p>
<section id="revisiting-collinearity">
<h4>Revisiting Collinearity<a class="headerlink" href="#revisiting-collinearity" title="Permalink to this heading">#</a></h4>
<p>Regularization also happens to be a fantastic solution to our <em>other</em> major problem: <strong>multicollinearity</strong>.</p>
<p>As we saw, when predictors like <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code> are highly correlated, the <span class="math notranslate nohighlight">\(X^T X\)</span> matrix is ill-conditioned, and the OLS coefficient estimates become <strong>unstable</strong> and their standard errors explode.</p>
<p>Regularization “stabilizes” this matrix, allowing the model to find sensible, stable coefficients even in the presence of multicollinearity.</p>
</section>
</section>
<section id="ridge-regression-l2-penalty">
<h3>Ridge Regression (L2 Penalty)<a class="headerlink" href="#ridge-regression-l2-penalty" title="Permalink to this heading">#</a></h3>
<p>Ridge Regression is the most common type of regularization. It adds a penalty proportional to the <strong>sum of the squares of the coefficients</strong>. This is also called the <strong><span class="math notranslate nohighlight">\(L_2\)</span> norm</strong>.</p>
<p>The cost function becomes:
$<span class="math notranslate nohighlight">\(RSS_{L2} = \sum_{i=1}^m (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^n \beta_j^2\)</span>$</p>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(RSS\)</span> term pushes the model to <strong>fit the data</strong> (reducing bias).</p></li>
<li><p>The <span class="math notranslate nohighlight">\(\lambda \sum \beta_j^2\)</span> term pushes the coefficients to be <strong>small</strong> (reducing variance).</p></li>
</ul>
<p>The <strong>hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span></strong> (called <code class="docutils literal notranslate"><span class="pre">alpha</span></code> in code) controls this tradeoff.</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\lambda = 0\)</span></strong>: This is just standard OLS.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(\lambda \to \infty\)</span></strong>: The penalty is everything. All coefficients are forced to zero (a flat line).</p></li>
</ul>
<p><strong>Key Property:</strong> Ridge Regression <em>shrinks</em> coefficients close to zero, but <strong>it will not set them <em>exactly</em> to zero</strong>.</p>
<p><strong>Crucial Step:</strong> Because the penalty <span class="math notranslate nohighlight">\(\sum \beta_j^2\)</span> is based on the <em>size</em> of the coefficients, it is critical that we <strong>standardize (z-score)</strong> all our features first. Otherwise, a feature measured in kilometers will be penalized differently than one measured in millimeters.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/694110f73c411c8963d5e8668dd58cb2608a346f98bfa6fcd02b940d84a64fef.png" src="../_images/694110f73c411c8963d5e8668dd58cb2608a346f98bfa6fcd02b940d84a64fef.png" />
</div>
</div>
<p>As you can see, as <span class="math notranslate nohighlight">\(\lambda\)</span> increases, all coefficients are smoothly “shrunk” toward zero. Interestingly, the model <strong>“decides”</strong> which coefficients to shrink for a given value of <span class="math notranslate nohighlight">\(\lambda\)</span>. For instance, for low values of <span class="math notranslate nohighlight">\(\lambda\)</span>, <code class="docutils literal notranslate"><span class="pre">weight</span></code> is shrunk, while <code class="docutils literal notranslate"><span class="pre">acceleration</span></code> is first shrunk, then allowed to be larger than zero. This is due to the fact that, the regularization term acts as a sort of “soft constraint” encouraging the model to find smaller weights, while still finding a good solution.</p>
<p>It can be shown (but we will not see it formally), that ridge regression <strong>reduces the variance of coefficient estimates</strong>. At the same time, <strong>the bias is increased</strong>. So, finding a good value of <span class="math notranslate nohighlight">\(\lambda\)</span> allows to <strong>control the bias-variance trade-off</strong>.</p>
<section id="interpretation-of-the-ridge-regression-coefficients">
<h4>Interpretation of the ridge regression coefficients<a class="headerlink" href="#interpretation-of-the-ridge-regression-coefficients" title="Permalink to this heading">#</a></h4>
<p>Let us compare the parameters obtained through a ridge regressor with those obtained with a linear regressor (OLS):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;variables&#39;</span><span class="p">:[</span><span class="s1">&#39;displacement&#39;</span><span class="p">,</span><span class="s1">&#39;cylinders&#39;</span> <span class="p">,</span> <span class="s1">&#39;horsepower&#39;</span> <span class="p">,</span> <span class="s1">&#39;weight&#39;</span> <span class="p">,</span> <span class="s1">&#39;acceleration&#39;</span> <span class="p">,</span> <span class="s1">&#39;model_year&#39;</span> <span class="p">,</span> <span class="s1">&#39;origin&#39;</span><span class="p">],</span> <span class="s1">&#39;ridge_params&#39;</span><span class="p">:</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;mpg ~ displacement + cylinders + horsepower + weight + acceleration + model_year + origin&quot;</span><span class="p">,</span> <span class="n">data2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_regularized</span><span class="p">(</span><span class="n">L1_wt</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span><span class="s1">&#39;ols_params&#39;</span><span class="p">:</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;mpg ~ displacement + cylinders + horsepower + weight + acceleration + model_year + origin&quot;</span><span class="p">,</span> <span class="n">data2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">:]})</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;variables&#39;</span><span class="p">)</span>
<span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ridge_params</th>
      <th>ols_params</th>
    </tr>
    <tr>
      <th>variables</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>displacement</th>
      <td>-0.998445</td>
      <td>2.079303</td>
    </tr>
    <tr>
      <th>cylinders</th>
      <td>-0.969852</td>
      <td>-0.840519</td>
    </tr>
    <tr>
      <th>horsepower</th>
      <td>-1.027231</td>
      <td>-0.651636</td>
    </tr>
    <tr>
      <th>weight</th>
      <td>-1.401530</td>
      <td>-5.492050</td>
    </tr>
    <tr>
      <th>acceleration</th>
      <td>0.199621</td>
      <td>0.222014</td>
    </tr>
    <tr>
      <th>model_year</th>
      <td>1.375600</td>
      <td>2.762119</td>
    </tr>
    <tr>
      <th>origin</th>
      <td>0.830411</td>
      <td>1.147316</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As we can see, the ridge parameters has a smaller scale. This is due to the regularization term. As a result, the parameters <strong>cannot be interpreted statistically as the ones of a linear regressor</strong>. Instead, we can interpret them as denoting the relative importance of the variable to the prediction.</p>
</section>
</section>
<section id="lasso-regression-l-1-penalty">
<h3>Lasso Regression (<span class="math notranslate nohighlight">\(L_1\)</span> Penalty)<a class="headerlink" href="#lasso-regression-l-1-penalty" title="Permalink to this heading">#</a></h3>
<p>Lasso is an alternative that has a different, very useful property. It adds a penalty proportional to the <strong>sum of the absolute values of the coefficients</strong>. This is also called the <strong><span class="math notranslate nohighlight">\(L_1\)</span> norm</strong>.</p>
<p>The cost function becomes:</p>
<div class="math notranslate nohighlight">
\[RSS_{L1} = \sum_{i=1}^m (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^n |\beta_j|\]</div>
<p>This subtle change has a profound effect. The “sharp corners” of the absolute value function allow the optimization to find solutions where “useless” coefficients are set to <strong>exactly zero</strong>.</p>
<p><strong>Its Superpower:</strong> Lasso performs <strong>automatic feature selection</strong>, which is great when you have hundreds of features.</p>
<p>The figure below shows how the coefficient estimates change for different values of <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">zscore</span>
<span class="kn">from</span> <span class="nn">ucimlrepo</span> <span class="kn">import</span> <span class="n">fetch_ucirepo</span> 
<span class="kn">from</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">import</span> <span class="n">ols</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
  
<span class="c1"># fetch dataset </span>
<span class="n">auto_mpg</span> <span class="o">=</span> <span class="n">fetch_ucirepo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span> 
  
<span class="c1"># data (as pandas dataframes) </span>
<span class="n">X</span> <span class="o">=</span> <span class="n">auto_mpg</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">features</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">auto_mpg</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span> 
  
<span class="n">data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">data2</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;mpg&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">zscore</span><span class="p">)</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()[</span><span class="s1">&#39;mpg&#39;</span><span class="p">])</span>
<span class="n">dd</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">d</span><span class="o">=</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;mpg ~ displacement + cylinders + horsepower + weight + acceleration + model_year + origin&quot;</span><span class="p">,</span> <span class="n">data2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_regularized</span><span class="p">(</span><span class="n">L1_wt</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">params</span>

    
    <span class="n">d</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="n">c</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="s2">&quot;Intercept&quot;</span><span class="p">,</span><span class="s2">&quot;displacement&quot;</span><span class="p">,</span> <span class="s2">&quot;cylinders&quot;</span><span class="p">,</span> <span class="s2">&quot;horsepower&quot;</span> <span class="p">,</span><span class="s2">&quot;weight&quot;</span> <span class="p">,</span><span class="s2">&quot;acceleration&quot;</span><span class="p">,</span> <span class="s2">&quot;model_year&quot;</span> <span class="p">,</span><span class="s2">&quot;origin&quot;</span><span class="p">],</span><span class="n">d</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="p">)</span>
    <span class="n">d</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">alpha</span>
    <span class="n">dd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">dd</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dd</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">dd</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Intercept&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\lambda$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Coefficients&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/94f9c08ca40d230e431b7445d2ce725fbd162e785851e732066ee70555ca216a.png" src="../_images/94f9c08ca40d230e431b7445d2ce725fbd162e785851e732066ee70555ca216a.png" />
</div>
</div>
<p>As can be seen, the model now makes “hard choices” on whether a value should be set to zero or not, thus performing variable selection. Once Lasso regression identified the variables to remove, we could create a reduced dataset keeping only columns with nonzero coefficients and fit a regular, interpretable, linear regressor with the original, non standardized, data.</p>
</section>
<section id="regularization-and-the-bias-variance-tradeoff">
<h3>Regularization and the Bias-Variance Tradeoff<a class="headerlink" href="#regularization-and-the-bias-variance-tradeoff" title="Permalink to this heading">#</a></h3>
<p>Regularization is our first, and most powerful, tool for <strong>explicitly managing the Bias-Variance Tradeoff</strong>.</p>
<p>Let’s think about the two extremes:</p>
<ol class="arabic simple">
<li><p><strong>A Standard OLS Model (or high-degree polynomial)</strong></p>
<ul class="simple">
<li><p>This model’s only goal is to <strong>minimize <span class="math notranslate nohighlight">\(RSS\)</span></strong>.</p></li>
<li><p>It will “succeed” by fitting the training data perfectly, including all its noise.</p></li>
<li><p>This results in a model with <strong><span class="math notranslate nohighlight">\(\text{Low Bias}\)</span></strong> (it’s a perfect fit for the data it saw) but <strong><span class="math notranslate nohighlight">\(\text{Extremely High Variance}\)</span></strong> (it’s “nervous” and will fail on new data). This is <strong>overfitting</strong>.</p></li>
</ul>
</li>
<li><p><strong>A Regularized Model (Ridge or Lasso)</strong></p>
<ul class="simple">
<li><p>This model has a <em>different</em> goal: minimize <span class="math notranslate nohighlight">\(RSS + \text{Penalty Term}\)</span>.</p></li>
<li><p>To get the lowest total score, it is <em>forced</em> to make a compromise. It will <em>intentionally</em> stop fitting the training data perfectly, in order to make its coefficients smaller and reduce the penalty.</p></li>
<li><p>This is the trade-off in action:</p>
<ul>
<li><p>We <strong>increase the bias</strong> slightly (because our model is now “imperfect” for the training data).</p></li>
<li><p>In exchange, we <strong>dramatically decrease the variance</strong> (because the model is smoother and no longer chasing noise).</p></li>
</ul>
</li>
</ul>
</li>
</ol>
</section>
<section id="lambda-is-the-tradeoff-knob">
<h3><span class="math notranslate nohighlight">\(\lambda\)</span> is the “Tradeoff Knob”<a class="headerlink" href="#lambda-is-the-tradeoff-knob" title="Permalink to this heading">#</a></h3>
<p>The <strong>hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span></strong> (called <code class="docutils literal notranslate"><span class="pre">alpha</span></code> in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>) is our <strong>explicit control knob</strong> for moving along the bias-variance curve.</p>
<ul class="simple">
<li><p><strong>When <span class="math notranslate nohighlight">\(\lambda = 0\)</span>:</strong> We are on the far right of the graph. We have a standard OLS model with <span class="math notranslate nohighlight">\(\text{Low Bias}\)</span> and <span class="math notranslate nohighlight">\(\text{High Variance}\)</span> (overfitting).</p></li>
<li><p><strong>As we increase <span class="math notranslate nohighlight">\(\lambda\)</span>:</strong> We “turn the knob,” increasing the penalty. This moves us left on the graph. <span class="math notranslate nohighlight">\(\text{Bias}\)</span> starts to go up, but <span class="math notranslate nohighlight">\(\text{Variance}\)</span> drops <em>faster</em>. The <strong>Total Error</strong> (the black line) goes down.</p></li>
<li><p><strong>At the “sweet spot”:</strong> We find the optimal <span class="math notranslate nohighlight">\(\lambda\)</span> that gives us the lowest possible <strong>Total Error</strong>.</p></li>
<li><p><strong>As <span class="math notranslate nohighlight">\(\lambda \to \infty\)</span>:</strong> We move to the far left of the graph. We are penalizing the coefficients so much that they all become zero (a flat line). The model has <span class="math notranslate nohighlight">\(\text{High Bias}\)</span> and <span class="math notranslate nohighlight">\(\text{Low Variance}\)</span> (underfitting).</p></li>
</ul>
<p>The <em>entire goal</em> of the machine learning workflow, which we are about to see, is to find the <strong>optimal value of <span class="math notranslate nohighlight">\(\lambda\)</span></strong>. We can’t use our <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> p-values for this. We must use a new technique, <strong>cross-validation</strong>, to find the <span class="math notranslate nohighlight">\(\lambda\)</span> that gives the best predictive score on unseen data.</p>
<p>The plot below shows how training and validation error change when fitting a polynomial of degree <span class="math notranslate nohighlight">\(15\)</span> (a high-capacity model) as we increase the regularization hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span>, which illustrates how <strong>regularization controls the bias-variance trade-off</strong>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation curve plot saved as &#39;validation_curve.png&#39;
</pre></div>
</div>
<img alt="../_images/7530bcb67f87198dfd9174263a873e38a5f2d76e0ef3d8d01e95b29ce916917a.png" src="../_images/7530bcb67f87198dfd9174263a873e38a5f2d76e0ef3d8d01e95b29ce916917a.png" />
</div>
</div>
<p>Let’s see how to interpret the plot:</p>
<ul class="simple">
<li><p><strong>Left Side (λ is tiny, e.g., 0.001):</strong></p>
<ul>
<li><p>The <strong>Training Error</strong> (blue) is very low. The model is <em>overfitting</em> the training data.</p></li>
<li><p>The <strong>Test Error</strong> (red) is high. The model is <em>not generalizing</em> well.</p></li>
<li><p>This is the <strong>High Variance</strong> region.</p></li>
</ul>
</li>
<li><p><strong>Right Side (λ is large, e.g., 1000):</strong></p>
<ul>
<li><p>The <strong>Training Error</strong> is high and almost equal to the test error. The model is <em>too simple</em> (too penalized).</p></li>
<li><p>The <strong>Test Error</strong> is also high.</p></li>
<li><p>This is the <strong>High Bias</strong> region (underfitting).</p></li>
</ul>
</li>
<li><p><strong>The “Sweet Spot” (Dashed Line):</strong></p>
<ul>
<li><p>This is the <strong>optimal <span class="math notranslate nohighlight">\(\lambda\)</span></strong> (our optimal <span class="math notranslate nohighlight">\(\lambda\)</span>).</p></li>
<li><p>It’s the “Goldilocks” point where the Test Error is at its minimum.</p></li>
<li><p>This is the perfect balance between bias and variance, giving us the best possible predictive model.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="from-theory-to-practice-the-scikit-learn-workflow">
<h2>From Theory to Practice: The <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> Workflow<a class="headerlink" href="#from-theory-to-practice-the-scikit-learn-workflow" title="Permalink to this heading">#</a></h2>
<p>We have now established all the theory.</p>
<ol class="arabic simple">
<li><p><strong>Our Problem:</strong> Simple linear models can <strong>underfit</strong> (high bias), like the “U-shape” in our <code class="docutils literal notranslate"><span class="pre">mpg</span> <span class="pre">~</span> <span class="pre">horsepower</span></code> plot.</p></li>
<li><p><strong>A Solution:</strong> We can use <strong>Polynomial Regression</strong> to add non-linear features (<span class="math notranslate nohighlight">\(X^2, X^3, \ldots\)</span>) which gives our model higher <strong>capacity</strong> to fit the curve.</p></li>
<li><p><strong>The New Problem:</strong> High-capacity models can easily <strong>overfit</strong> (high variance), “memorizing” the training data noise.</p></li>
<li><p><strong>A Better Solution:</strong> We use <strong>Regularization (Ridge <span class="math notranslate nohighlight">\(L_2\)</span> or Lasso <span class="math notranslate nohighlight">\(L_1\)</span>)</strong> to add a penalty (<span class="math notranslate nohighlight">\(\lambda\)</span>) that controls this high capacity, finding the “sweet spot” in the Bias-Variance tradeoff.</p></li>
</ol>
<p>We’ve seen the <em>theory</em> of <span class="math notranslate nohighlight">\(\lambda\)</span>. Now we need a practical, robust workflow to actually <em>find</em> the best <span class="math notranslate nohighlight">\(\lambda\)</span> and build the best predictive model.</p>
<p>It’s time to switch from <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> (built for <strong>inference</strong>) to <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> (built for <strong>prediction</strong>).</p>
<section id="our-new-toolkit">
<h3>Our New Toolkit<a class="headerlink" href="#our-new-toolkit" title="Permalink to this heading">#</a></h3>
<p>Our new workflow requires new rules and tools:</p>
<ol class="arabic simple">
<li><p><strong>Train/Test Split:</strong> This is the <strong>Golden Rule</strong>. We <em>must</em> split our data to simulate a “future” test set. Our only goal is to get the best score (e.g., <span class="math notranslate nohighlight">\(RMSE\)</span>) on this test data.</p></li>
<li><p><strong>Standardization:</strong> We <em>must</em> scale our features (e.g., <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>) so the regularization penalty <span class="math notranslate nohighlight">\(\lambda\)</span> affects them all fairly.</p></li>
<li><p><strong>Pipelines:</strong> We must bundle our steps (scaling, polynomial features, model) together. This is a best practice that prevents “data leakage” and makes our workflow reproducible.</p></li>
</ol>
</section>
<section id="dataset">
<h3>Dataset<a class="headerlink" href="#dataset" title="Permalink to this heading">#</a></h3>
<p>We’ll use the California Housing dataset provided by the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library. Let us load the data as a dataframe and have a look at the data description:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;DESCR&#39;</span><span class="p">])</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. _california_housing_dataset:

California Housing dataset
--------------------------

**Data Set Characteristics:**

:Number of Instances: 20640

:Number of Attributes: 8 numeric, predictive attributes and the target

:Attribute Information:
    - MedInc        median income in block group
    - HouseAge      median house age in block group
    - AveRooms      average number of rooms per household
    - AveBedrms     average number of bedrooms per household
    - Population    block group population
    - AveOccup      average number of household members
    - Latitude      block group latitude
    - Longitude     block group longitude

:Missing Attribute Values: None

This dataset was obtained from the StatLib repository.
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html

The target variable is the median house value for California districts,
expressed in hundreds of thousands of dollars ($100,000).

This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

A household is a group of people residing within a home. Since the average
number of rooms and bedrooms in this dataset are provided per household, these
columns may take surprisingly large values for block groups with few households
and many empty houses, such as vacation resorts.

It can be downloaded/loaded using the
:func:`sklearn.datasets.fetch_california_housing` function.

.. rubric:: References

- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
  Statistics and Probability Letters, 33 (1997) 291-297
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8.3252</td>
      <td>41.0</td>
      <td>6.984127</td>
      <td>1.023810</td>
      <td>322.0</td>
      <td>2.555556</td>
      <td>37.88</td>
      <td>-122.23</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8.3014</td>
      <td>21.0</td>
      <td>6.238137</td>
      <td>0.971880</td>
      <td>2401.0</td>
      <td>2.109842</td>
      <td>37.86</td>
      <td>-122.22</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.2574</td>
      <td>52.0</td>
      <td>8.288136</td>
      <td>1.073446</td>
      <td>496.0</td>
      <td>2.802260</td>
      <td>37.85</td>
      <td>-122.24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5.6431</td>
      <td>52.0</td>
      <td>5.817352</td>
      <td>1.073059</td>
      <td>558.0</td>
      <td>2.547945</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.8462</td>
      <td>52.0</td>
      <td>6.281853</td>
      <td>1.081081</td>
      <td>565.0</td>
      <td>2.181467</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>20635</th>
      <td>1.5603</td>
      <td>25.0</td>
      <td>5.045455</td>
      <td>1.133333</td>
      <td>845.0</td>
      <td>2.560606</td>
      <td>39.48</td>
      <td>-121.09</td>
    </tr>
    <tr>
      <th>20636</th>
      <td>2.5568</td>
      <td>18.0</td>
      <td>6.114035</td>
      <td>1.315789</td>
      <td>356.0</td>
      <td>3.122807</td>
      <td>39.49</td>
      <td>-121.21</td>
    </tr>
    <tr>
      <th>20637</th>
      <td>1.7000</td>
      <td>17.0</td>
      <td>5.205543</td>
      <td>1.120092</td>
      <td>1007.0</td>
      <td>2.325635</td>
      <td>39.43</td>
      <td>-121.22</td>
    </tr>
    <tr>
      <th>20638</th>
      <td>1.8672</td>
      <td>18.0</td>
      <td>5.329513</td>
      <td>1.171920</td>
      <td>741.0</td>
      <td>2.123209</td>
      <td>39.43</td>
      <td>-121.32</td>
    </tr>
    <tr>
      <th>20639</th>
      <td>2.3886</td>
      <td>16.0</td>
      <td>5.254717</td>
      <td>1.162264</td>
      <td>1387.0</td>
      <td>2.616981</td>
      <td>39.37</td>
      <td>-121.24</td>
    </tr>
  </tbody>
</table>
<p>20640 rows × 8 columns</p>
</div></div></div>
</div>
<p>The dataset contains <span class="math notranslate nohighlight">\(8\)</span> variables. The independent variable is <code class="docutils literal notranslate"><span class="pre">MedInc</span></code>, the average value of houses in a given suburb, while all other variables are independent. For our aims, we will treat the data as a matrix of numerical variables. We could easily convert the dataframe in this format, but scikit-learn allows to load the data directly in this format:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># let us load the data without passing as_frame=True</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span> <span class="c1"># the features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span> <span class="c1"># the targets</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(20640, 8) (20640,)
</pre></div>
</div>
</div>
</div>
</section>
<section id="data-splitting">
<h3>Data Splitting<a class="headerlink" href="#data-splitting" title="Permalink to this heading">#</a></h3>
<p>We will split the dataset into a training, a validation and a test set using the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1"># We&#39;ll do a 60:20:20 split</span>
<span class="n">val_prop</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">test_prop</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># We&#39;ll split the data in two steps - first let&#39;s create a test set and a combined trainval set</span>
<span class="n">X_trainval</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># We&#39;ll now split the combined trainval into train and val set with the chosen proportions</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">test_prop</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Let us check shapes and proportions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(12384, 8) (4128, 8) (4128, 8)
0.6 0.2 0.2
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function will split the data randomly. We are passing a fixed <code class="docutils literal notranslate"><span class="pre">random_state</span></code> to be able to replicate the results, but, in general, we should avoid that if we want the split to be truly random (though it is common to use random seeds for splitting in research). Note that, while the split is random, the function makes sure that the i-th element of the y variable corresponds to the i-th element of the X variable after the split.</p>
<p>We will now reason mainly on the validation set, comparing different models and parameter configurations. Once we are done with our explorations, we’ll check the final results on the test set.</p>
</section>
<section id="data-normalization">
<h3>Data Normalization<a class="headerlink" href="#data-normalization" title="Permalink to this heading">#</a></h3>
<p>We’ll start by normalizing the data with z-scoring. This will prove useful later when we use certain algorithms (e.g., regularization). Note that we have not normalized data before because we need to <strong>make sure that even mean and standard deviation parameters are not computed on the validation or test set</strong>. While this may seem a trivial detail, it is important to follow this rule as strictly as possible to avoid bias. We can normalize the data with the <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> object:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="c1"># tunes the internal parameters of the standard scaler</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="c1"># does not tune the parameters anymore</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Scikit-learn objects have a unified object-oriented interface. Each algorithm is an object (e.g., <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>) with standard methods, such as:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">fit</span></code> method to tune the internal parameters of the algorithm. In this case, it is a vector of means and a vector of standard deviations, but in the case of a linear regression it will be a vector of weights;</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">transform</span></code> method to transform the data. Note that in this stage no parameters are tuned, so we can safely apply this method to validation and test data. This method only applies to objects which transform the data, such as the standard scaler;</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">predict</span></code> method to obtain predictions. This applies only to predictive models, such as a linear regressor;</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">score</span></code> method to obtain a standard performance measure on the test or validation data. Also this only applies to predictive models.</p></li>
</ul>
<p>We will see examples of the last two methods later.</p>
</section>
<section id="linear-regressor">
<h3>Linear Regressor<a class="headerlink" href="#linear-regressor" title="Permalink to this heading">#</a></h3>
<p>We will start by training a linear regressor. We will use scikit-learn’s implementation which does not provide statistical details (e.g., p-values) but is optimized for predictive modeling. The train/test interface is the same as above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">linear_regressor</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linear_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># this tunes the internal parameters of the model</span>

<span class="c1"># Let us print the model&#39;s parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear_regressor</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">linear_regressor</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 0.86025287  0.1200073  -0.28039183  0.31208687 -0.00957447 -0.02615781
 -0.88821331 -0.86190739]
2.0680774192504314
</pre></div>
</div>
</div>
</div>
<p>We can obtain predictions on the validation set using the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4128,)
</pre></div>
</div>
</div>
</div>
<p>The function returns a vector of <span class="math notranslate nohighlight">\(4128\)</span> predictions, one for each example in the validation set. We can now evaluate the predictions using regression evaluation measures. We will use the standard implementation of the main evaluation measures as provided by scikit-learn:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5333346447415042 0.5297481095803488 0.727837969317587
</pre></div>
</div>
</div>
</div>
<p>All evaluation measures in scikit-learn follow the <code class="docutils literal notranslate"><span class="pre">evaluation_measure(y_true,</span> <span class="pre">y_pred)</span></code> convention. Note that the target variable <code class="docutils literal notranslate"><span class="pre">MedInc</span></code> is measured in tens of thousands of dollars, so an MAE of about <span class="math notranslate nohighlight">\(0.5\)</span> corresponds to an average error of about <span class="math notranslate nohighlight">\(5000\)</span> dollars. This is not that bad if we consider the mean and standard deviation of targets:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(np.float64(2.068077419250646), np.float64(1.1509151433486544))
</pre></div>
</div>
</div>
</div>
<p>Each predictor in scikit-learn also provides a <code class="docutils literal notranslate"><span class="pre">score</span></code> method which takes as input the validation (or test) inputs and outputs and computes some standard evaluation measures. By default the linear regressor in scikit-learn returns the <span class="math notranslate nohighlight">\(R^2\)</span> value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_regressor</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6142000785497264
</pre></div>
</div>
</div>
</div>
<p>While we are mainly interested in the performance of the model on the validation set (and ultimately on those on the test set), it is still useful to assess the performance on the training set for model diagnostics. For instance, if we see a big discrepancy between training and validation errors, then we can imagine that some overfitting is going on:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="n">mae_train</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
<span class="n">rmse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mae_train</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5266487515751342 0.5143795055231385 0.7172025554354491
</pre></div>
</div>
</div>
</div>
<p>We can see that, while there are some differences between training and test performance, those are minor, so we can deduce that there is no significant overfitting going on.</p>
<blockquote>
<div><p>We should note that we should <strong>always expect a certain degree of overfitting, depending on the task, the data and the model</strong>. When the difference between train and test error is large, and hence there is significant overfitting, we can try to reduce this effect with regularization techniques.</p>
</div></blockquote>
<p>To better compare models, we will now store the results of our analyses in a dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;Method&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Linear Regressor&#39;</span><span class="p">],</span>
    <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">],</span>
    <span class="s1">&#39;MAE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mae</span><span class="p">],</span>
    <span class="s1">&#39;MSE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mse</span><span class="p">],</span>
    <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">rmse</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>It is common to use the word “method” to refer to a predictive algorithm or pipeline.</p>
</section>
<section id="non-linear-regression">
<h3>Non-Linear Regression<a class="headerlink" href="#non-linear-regression" title="Permalink to this heading">#</a></h3>
<p>Let us now try to fit a non-linear regressor. We will use polynomial regression with different polynomial degrees. To do so, we will perform an explicit polynomial expansion of the features using the <code class="docutils literal notranslate"><span class="pre">PolynomialFeatures</span></code> object. For convenience, we will define a function performing training and validation and returning both training and validation performance:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="k">def</span> <span class="nf">trainval_polynomial</span><span class="p">(</span><span class="n">degree</span><span class="p">):</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
    <span class="c1"># While the model does not have any learnable parameters, the &quot;fit&quot; method here is used to compute the output number of features</span>
    <span class="n">pf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_train_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_val_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

    <span class="n">polyreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span> <span class="c1"># a Polynomial regressor is simply a linear regressor using polynomial features</span>
    <span class="n">polyreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">y_poly_train_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">)</span>
    <span class="n">y_poly_val_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_poly</span><span class="p">)</span>

    <span class="n">mae_train</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">rmse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">))</span>

    <span class="n">mae_val</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">mse_val</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">rmse_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">mae_train</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">mae_val</span><span class="p">,</span> <span class="n">mse_val</span><span class="p">,</span> <span class="n">rmse_val</span>
</pre></div>
</div>
</div>
</div>
<p>Let us now see what happens with different degrees:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;DEGREE: </span><span class="si">{}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">      </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="se">\n</span><span class="s2">TRAIN </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">VAL   </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="s2">&quot;MAE&quot;</span><span class="p">,</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSE&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">trainval_polynomial</span><span class="p">(</span><span class="n">d</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DEGREE: 1 
           MAE      MSE     RMSE
TRAIN     0.53     0.51     0.72 
VAL       0.53     0.53     0.73


DEGREE: 2 
           MAE      MSE     RMSE
TRAIN     0.46     0.42     0.65 
VAL       0.48     0.91     0.95


DEGREE: 3 
           MAE      MSE     RMSE
TRAIN     0.42     0.34     0.58 
VAL      23.48 2157650.15  1468.89
</pre></div>
</div>
</div>
</div>
</section>
<section id="ridge-regularization">
<h3>Ridge Regularization<a class="headerlink" href="#ridge-regularization" title="Permalink to this heading">#</a></h3>
<p>We can see that, as the polynomial gets larger, the effect of overfitting increases. We can try to reduce this effect with Ridge or Lasso regularization. We’ll focus on degree <span class="math notranslate nohighlight">\(2\)</span> and try to apply ridge regression to it. Since Ridge regression relies on a parameter, we will try some values of the regularization parameter <span class="math notranslate nohighlight">\(\alpha\)</span> (as it is called by sklearn). Let us define a function for convenience:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="k">def</span> <span class="nf">trainval_polynomial_ridge</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
    <span class="c1"># While the model does not have any learnable parameters, the &quot;fit&quot; method here is used to compute the output number of features</span>
    <span class="n">pf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_train_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_val_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

    <span class="n">polyreg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span> <span class="c1"># a Polynomial regressor is simply a linear regressor using polynomial features</span>
    <span class="n">polyreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">y_poly_train_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">)</span>
    <span class="n">y_poly_val_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_poly</span><span class="p">)</span>

    <span class="n">mae_train</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">rmse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">))</span>

    <span class="n">mae_val</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">mse_val</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">rmse_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">mae_train</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">mae_val</span><span class="p">,</span> <span class="n">mse_val</span><span class="p">,</span> <span class="n">rmse_val</span>
</pre></div>
</div>
</div>
</div>
<p>Let us now see the results for different values of <span class="math notranslate nohighlight">\(\alpha\)</span>. <span class="math notranslate nohighlight">\(\alpha=0\)</span> means no regularization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RIDGE, DEGREE: 2&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">400</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Alpha: </span><span class="si">{:0.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">      </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="se">\n</span><span class="s2">TRAIN </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">VAL   </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="s2">&quot;MAE&quot;</span><span class="p">,</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSE&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">alpha</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RIDGE, DEGREE: 2
Alpha: 0.00 
           MAE      MSE     RMSE
TRAIN     0.46     0.42     0.65 
VAL       0.48     0.91     0.96


Alpha: 100.00 
           MAE      MSE     RMSE
TRAIN     0.47     0.43     0.66 
VAL       0.48     0.54     0.74


Alpha: 200.00 
           MAE      MSE     RMSE
TRAIN     0.48     0.44     0.67 
VAL       0.49     0.51     0.72


Alpha: 300.00 
           MAE      MSE     RMSE
TRAIN     0.49     0.46     0.68 
VAL       0.50     0.50     0.71


Alpha: 400.00 
           MAE      MSE     RMSE
TRAIN     0.50     0.47     0.68 
VAL       0.51     0.51     0.71
</pre></div>
</div>
</div>
</div>
<p>We can see how, as alpha increases, the error on the training set increases, while the error on the test set decreases. For <span class="math notranslate nohighlight">\(\alpha=300\)</span> we obtained a slightly better result than our linear regressor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let us see if we can improve the results with a polynomial of degree 3:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RIDGE, DEGREE: 3&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Alpha: </span><span class="si">{:0.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">      </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="se">\n</span><span class="s2">TRAIN </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">VAL   </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="s2">&quot;MAE&quot;</span><span class="p">,</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSE&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">alpha</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RIDGE, DEGREE: 3
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Alpha: 0.00 
           MAE      MSE     RMSE
TRAIN     0.42     0.34     0.58 
VAL      23.50 2162209.37  1470.45


Alpha: 1.00 
           MAE      MSE     RMSE
TRAIN     0.42     0.34     0.58 
VAL      15.59 934580.09   966.74


Alpha: 10.00 
           MAE      MSE     RMSE
TRAIN     0.42     0.34     0.59 
VAL       1.57  4867.65    69.77


Alpha: 20.00 
           MAE      MSE     RMSE
TRAIN     0.42     0.35     0.59 
VAL       1.78  6690.78    81.80
</pre></div>
</div>
</div>
</div>
<p>Let us add the results of Polynomial regression of degree 2 with and without regularization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">poly2</span> <span class="o">=</span> <span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">poly2_ridge300</span> <span class="o">=</span> <span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">300</span><span class="p">)</span>
<span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">california_housing_val_results</span><span class="p">,</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Polynomial Regressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;degree=2&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">poly2</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">poly2</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">poly2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Polynomial Ridge Regressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;degree=2, alpha=300&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">poly2_ridge300</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">poly2_ridge300</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">poly2_ridge300</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="p">])</span>
<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480437</td>
      <td>0.912971</td>
      <td>0.955495</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="lasso-regression">
<h2>Lasso Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this heading">#</a></h2>
<p>Let us now try the same with Lasso regression:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="k">def</span> <span class="nf">trainval_polynomial_lasso</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
    <span class="c1"># While the model does not have any learnable parameters, the &quot;fit&quot; method here is used to compute the output number of features</span>
    <span class="n">pf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_train_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_val_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

    <span class="n">polyreg</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span> <span class="c1"># a Polynomial regressor is simply a linear regressor using polynomial features</span>
    <span class="n">polyreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">y_poly_train_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">)</span>
    <span class="n">y_poly_val_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_poly</span><span class="p">)</span>

    <span class="n">mae_train</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">rmse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">))</span>

    <span class="n">mae_val</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">mse_val</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">rmse_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">mae_train</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">mae_val</span><span class="p">,</span> <span class="n">mse_val</span><span class="p">,</span> <span class="n">rmse_val</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LSSO, DEGREE: 2&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.02</span><span class="p">,</span><span class="mf">0.03</span><span class="p">,</span><span class="mf">0.04</span><span class="p">,</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Alpha: </span><span class="si">{:0.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">      </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="se">\n</span><span class="s2">TRAIN </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">VAL   </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="s2">&quot;MAE&quot;</span><span class="p">,</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSE&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">trainval_polynomial_lasso</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">alpha</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LSSO, DEGREE: 2
Alpha: 0.02 
           MAE      MSE     RMSE
TRAIN     0.52     0.51     0.71 
VAL       0.54     1.19     1.09


Alpha: 0.03 
           MAE      MSE     RMSE
TRAIN     0.55     0.55     0.74 
VAL       0.55     0.59     0.77


Alpha: 0.04 
           MAE      MSE     RMSE
TRAIN     0.56     0.57     0.76 
VAL       0.57     0.59     0.77


Alpha: 0.05 
           MAE      MSE     RMSE
TRAIN     0.58     0.60     0.78 
VAL       0.58     0.61     0.78


Alpha: 0.06 
           MAE      MSE     RMSE
TRAIN     0.60     0.63     0.80 
VAL       0.60     0.64     0.80
</pre></div>
</div>
</div>
</div>
<p>Lasso regression does not seem to improve results. Let us put the results obtained for <span class="math notranslate nohighlight">\(\alpha=0.04\)</span> to the dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">poly2_lasso004</span> <span class="o">=</span> <span class="n">trainval_polynomial_lasso</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.04</span><span class="p">)</span>
<span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">california_housing_val_results</span><span class="p">,</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Polynomial Lasso Regressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;degree=2, alpha=0.04&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">poly2_lasso004</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">poly2_lasso004</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">poly2_lasso004</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="p">])</span>
<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480437</td>
      <td>0.912971</td>
      <td>0.955495</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Polynomial Lasso Regressor</td>
      <td>degree=2, alpha=0.04</td>
      <td>0.567318</td>
      <td>0.590100</td>
      <td>0.768180</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<section id="grid-search">
<h3>Grid Search<a class="headerlink" href="#grid-search" title="Permalink to this heading">#</a></h3>
<p>Polynomial regression and ridge regression have parameters to optimize. We have so far optimized them manually. However, in practice, it is common to perform a grid search. This consists in defining a grid of possible values to try and train/validate many models, to finally choose the one with best performance.</p>
<p>This can be done manually as shown in the following example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">grid_search</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">400</span><span class="p">,</span><span class="mi">25</span><span class="p">),</span> <span class="n">degrees</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)):</span>
    <span class="n">best_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating a=</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2"> d=</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2"> MSE=&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">a</span><span class="p">)</span>
            <span class="n">mse</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">mse</span><span class="o">&lt;</span><span class="n">best_mse</span><span class="p">:</span>
                <span class="n">best_mse</span> <span class="o">=</span> <span class="n">mse</span>
                <span class="n">best_alpha</span> <span class="o">=</span> <span class="n">a</span>
                <span class="n">best_degree</span> <span class="o">=</span> <span class="n">d</span>

    <span class="k">return</span> <span class="n">best_mse</span><span class="p">,</span> <span class="n">best_alpha</span><span class="p">,</span> <span class="n">best_degree</span>
<span class="n">grid_search</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluating a=200 d=0 MSE=1.37
Evaluating a=200 d=1 MSE=0.53
Evaluating a=200 d=2 MSE=0.51
Evaluating a=200 d=3 MSE=8161.95
Evaluating a=200 d=4 MSE=10884077.30
Evaluating a=225 d=0 MSE=1.37
Evaluating a=225 d=1 MSE=0.53
Evaluating a=225 d=2 MSE=0.51
Evaluating a=225 d=3 MSE=6710.18
Evaluating a=225 d=4 MSE=7244148.24
Evaluating a=250 d=0 MSE=1.37
Evaluating a=250 d=1 MSE=0.53
Evaluating a=250 d=2 MSE=0.51
Evaluating a=250 d=3 MSE=5542.92
Evaluating a=250 d=4 MSE=4880760.89
Evaluating a=275 d=0 MSE=1.37
Evaluating a=275 d=1 MSE=0.53
Evaluating a=275 d=2 MSE=0.50
Evaluating a=275 d=3 MSE=4598.99
Evaluating a=275 d=4 MSE=3313061.38
Evaluating a=300 d=0 MSE=1.37
Evaluating a=300 d=1 MSE=0.54
Evaluating a=300 d=2 MSE=0.50
Evaluating a=300 d=3 MSE=3831.00
Evaluating a=300 d=4 MSE=2255992.93
Evaluating a=325 d=0 MSE=1.37
Evaluating a=325 d=1 MSE=0.54
Evaluating a=325 d=2 MSE=0.50
Evaluating a=325 d=3 MSE=3202.40
Evaluating a=325 d=4 MSE=1534656.15
Evaluating a=350 d=0 MSE=1.37
Evaluating a=350 d=1 MSE=0.54
Evaluating a=350 d=2 MSE=0.51
Evaluating a=350 d=3 MSE=2684.99
Evaluating a=350 d=4 MSE=1038310.30
Evaluating a=375 d=0 MSE=1.37
Evaluating a=375 d=1 MSE=0.54
Evaluating a=375 d=2 MSE=0.51
Evaluating a=375 d=3 MSE=2256.89
Evaluating a=375 d=4 MSE=695260.83
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.5041553563051195, 300, 2)
</pre></div>
</div>
</div>
</div>
<p>Testing a range of values, we found that best results are obtained with degree equal to <span class="math notranslate nohighlight">\(2\)</span> and <span class="math notranslate nohighlight">\(\alpha=300\)</span>.</p>
</section>
<section id="scikit-learn-pipelines">
<h3>Scikit-Learn Pipelines<a class="headerlink" href="#scikit-learn-pipelines" title="Permalink to this heading">#</a></h3>
<p>Often, a predictive model is obtained by stacking different components. For instance, in our example, a Polynomial regressor is obtained by following this pipeline:</p>
<ul class="simple">
<li><p>Data standardization;</p></li>
<li><p>Polynomial feature expansion:</p></li>
<li><p>Ridge regression.</p></li>
</ul>
<p>While these steps can be carried out independently as seen before, scikit-learn offers the  powerful interface of <code class="docutils literal notranslate"><span class="pre">Pipelines</span></code> to automate this process. A pipeline stacks different components together and makes it convenient to change some of elements of the pipeline or to optimize its parameters. Let us define a pipeline as the one discussed above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="n">polynomial_regressor</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;polynomial_expansion&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;ridge_regression&#39;</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">())</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>To take full advantage of the pipeline, we will re-load the dataset and avoid applying the standard scaler manually:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># let us load the data without passing as_frame=True</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span> <span class="c1"># the features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span> <span class="c1"># the targets</span>

<span class="c1"># We&#39;ll do a 60:20:20 split</span>
<span class="n">val_prop</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">test_prop</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># We&#39;ll split the data in two steps - first let&#39;s create a test set and a combined trainval set</span>
<span class="n">X_trainval</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># We&#39;ll now split the combined trainval into train and val set with the chosen proportions</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">test_prop</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Let us check shapes and proportions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(12384, 8) (4128, 8) (4128, 8)
0.6 0.2 0.2
</pre></div>
</div>
</div>
</div>
<p>Our pipeline has two parameters that we need to set: the Ridge regressor’s <span class="math notranslate nohighlight">\(\alpha\)</span> and the degree of the polynomial. We can set these parameters as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We use the notation &quot;object__parameter&quot; to identify parameter names</span>
<span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">polynomial_expansion__degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ridge_regression__alpha</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                (&#x27;polynomial_expansion&#x27;, PolynomialFeatures()),
                (&#x27;ridge_regression&#x27;, Ridge(alpha=300))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label  sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" ><label for="sk-estimator-id-1" class="sk-toggleable__label  sk-toggleable__label-arrow"><div><div>Pipeline</div></div><div><a class="sk-estimator-doc-link " rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html">?<span>Documentation for Pipeline</span></a><span class="sk-estimator-doc-link ">i<span>Not fitted</span></span></div></label><div class="sk-toggleable__content "><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                (&#x27;polynomial_expansion&#x27;, PolynomialFeatures()),
                (&#x27;ridge_regression&#x27;, Ridge(alpha=300))])</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator  sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" ><label for="sk-estimator-id-2" class="sk-toggleable__label  sk-toggleable__label-arrow"><div><div>StandardScaler</div></div><div><a class="sk-estimator-doc-link " rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html">?<span>Documentation for StandardScaler</span></a></div></label><div class="sk-toggleable__content "><pre>StandardScaler()</pre></div> </div></div><div class="sk-item"><div class="sk-estimator  sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label  sk-toggleable__label-arrow"><div><div>PolynomialFeatures</div></div><div><a class="sk-estimator-doc-link " rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">?<span>Documentation for PolynomialFeatures</span></a></div></label><div class="sk-toggleable__content "><pre>PolynomialFeatures()</pre></div> </div></div><div class="sk-item"><div class="sk-estimator  sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" ><label for="sk-estimator-id-4" class="sk-toggleable__label  sk-toggleable__label-arrow"><div><div>Ridge</div></div><div><a class="sk-estimator-doc-link " rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.Ridge.html">?<span>Documentation for Ridge</span></a></div></label><div class="sk-toggleable__content "><pre>Ridge(alpha=300)</pre></div> </div></div></div></div></div></div></div></div>
</div>
<p>We can now fit and test the model as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.49922827945186954, 0.5041553563051195, np.float64(0.7100389822433129))
</pre></div>
</div>
</div>
</div>
</section>
<section id="grid-search-with-cross-validation">
<h3>Grid Search with Cross Validation<a class="headerlink" href="#grid-search-with-cross-validation" title="Permalink to this heading">#</a></h3>
<p>Scikit-learn offers a powerful interface to perform grid search with cross validation. In this case, rather than using a fixed training set, a K-Fold validation is performed for each parameter choice in order to find the best performing parameter combination. This is convenient when a validation set is not available. We will combine this approach with the pipelines to easily automate the search of optimal parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">make_scorer</span>

<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">polynomial_regressor</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;polynomial_expansion__degree&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="s1">&#39;ridge_regression__alpha&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">400</span><span class="p">,</span><span class="mi">25</span><span class="p">)},</span> <span class="n">scoring</span><span class="o">=</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">,</span><span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We will now fit the model on the union of training and validation set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-2 {
  color: var(--sklearn-color-text);
}

#sk-container-id-2 pre {
  padding: 0;
}

#sk-container-id-2 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-2 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-2 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-2 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-2 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-2 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-2 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-2 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-2 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-2 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-2 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-2 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-2 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-2 div.sk-label label.sk-toggleable__label,
#sk-container-id-2 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-2 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-2 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-2 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-2 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-2 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-2 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-2 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-2 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                                       (&#x27;polynomial_expansion&#x27;,
                                        PolynomialFeatures()),
                                       (&#x27;ridge_regression&#x27;, Ridge(alpha=300))]),
             param_grid={&#x27;polynomial_expansion__degree&#x27;: range(0, 5),
                         &#x27;ridge_regression__alpha&#x27;: range(200, 400, 25)},
             scoring=make_scorer(mean_squared_error, greater_is_better=False, response_method=&#x27;predict&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" ><label for="sk-estimator-id-5" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>GridSearchCV</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html">?<span>Documentation for GridSearchCV</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                                       (&#x27;polynomial_expansion&#x27;,
                                        PolynomialFeatures()),
                                       (&#x27;ridge_regression&#x27;, Ridge(alpha=300))]),
             param_grid={&#x27;polynomial_expansion__degree&#x27;: range(0, 5),
                         &#x27;ridge_regression__alpha&#x27;: range(200, 400, 25)},
             scoring=make_scorer(mean_squared_error, greater_is_better=False, response_method=&#x27;predict&#x27;))</pre></div> </div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" ><label for="sk-estimator-id-6" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>best_estimator_: Pipeline</div></div></label><div class="sk-toggleable__content fitted"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                (&#x27;polynomial_expansion&#x27;, PolynomialFeatures()),
                (&#x27;ridge_regression&#x27;, Ridge(alpha=250))])</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" ><label for="sk-estimator-id-7" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>StandardScaler</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html">?<span>Documentation for StandardScaler</span></a></div></label><div class="sk-toggleable__content fitted"><pre>StandardScaler()</pre></div> </div></div><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" ><label for="sk-estimator-id-8" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>PolynomialFeatures</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">?<span>Documentation for PolynomialFeatures</span></a></div></label><div class="sk-toggleable__content fitted"><pre>PolynomialFeatures()</pre></div> </div></div><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" ><label for="sk-estimator-id-9" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>Ridge</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.Ridge.html">?<span>Documentation for Ridge</span></a></div></label><div class="sk-toggleable__content fitted"><pre>Ridge(alpha=250)</pre></div> </div></div></div></div></div></div></div></div></div></div></div></div></div>
</div>
<p>Let us check the best parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;polynomial_expansion__degree&#39;: 2, &#39;ridge_regression__alpha&#39;: 250}
</pre></div>
</div>
</div>
</div>
<p>These are similar to the ones found with our previous grid search. We can now fit the final model on the training set and evaluate on the validation set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.49556977845052785 0.506286626421417 0.7115382114977501
</pre></div>
</div>
</div>
</div>
<p>Let us add this result to our dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">california_housing_val_results</span><span class="p">,</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Cross-Validated Polynomial Ridge Regressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;degree=2, alpha=250&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">mae</span><span class="p">,</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">mse</span><span class="p">,</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">rmse</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
<span class="p">])</span>
<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480437</td>
      <td>0.912971</td>
      <td>0.955495</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Polynomial Lasso Regressor</td>
      <td>degree=2, alpha=0.04</td>
      <td>0.567318</td>
      <td>0.590100</td>
      <td>0.768180</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Cross-Validated Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=250</td>
      <td>0.495570</td>
      <td>0.506287</td>
      <td>0.711538</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="other-regression-algorithms">
<h3>Other Regression Algorithms<a class="headerlink" href="#other-regression-algorithms" title="Permalink to this heading">#</a></h3>
<p>Thanks to the unified interface of scikit-learn objects, we can easily train other algorithms even if we do not know how they work inside. Of course, to be able to optimize them in the most complex situations we will need to know how they work internally. The following code shows how to train a neural network (we will not see the algorithm formally):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPRegressor</span>

<span class="n">mlp_regressor</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;mlp_regression&#39;</span><span class="p">,</span> <span class="n">MLPRegressor</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">mlp_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">mlp_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">))</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.373433510555124, 0.2997911694723271, np.float64(0.5475318890003824))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">california_housing_val_results</span><span class="p">,</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Neural Network&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">mae</span><span class="p">,</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">mse</span><span class="p">,</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">rmse</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
<span class="p">])</span>
<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480437</td>
      <td>0.912971</td>
      <td>0.955495</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Polynomial Lasso Regressor</td>
      <td>degree=2, alpha=0.04</td>
      <td>0.567318</td>
      <td>0.590100</td>
      <td>0.768180</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Cross-Validated Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=250</td>
      <td>0.495570</td>
      <td>0.506287</td>
      <td>0.711538</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Neural Network</td>
      <td></td>
      <td>0.373434</td>
      <td>0.299791</td>
      <td>0.547532</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="comparison-and-model-selection">
<h3>Comparison and Model Selection<a class="headerlink" href="#comparison-and-model-selection" title="Permalink to this heading">#</a></h3>
<p>We can now compare the performance of the different models using the table:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480437</td>
      <td>0.912971</td>
      <td>0.955495</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Polynomial Lasso Regressor</td>
      <td>degree=2, alpha=0.04</td>
      <td>0.567318</td>
      <td>0.590100</td>
      <td>0.768180</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Cross-Validated Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=250</td>
      <td>0.495570</td>
      <td>0.506287</td>
      <td>0.711538</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Neural Network</td>
      <td></td>
      <td>0.373434</td>
      <td>0.299791</td>
      <td>0.547532</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Alternatively, we can visualize the results graphically:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">california_housing_val_results</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Method&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0fd91ea819717b9bd04071ba67dfc770d798a655c74c2b10ece12e5776aba327.png" src="../_images/0fd91ea819717b9bd04071ba67dfc770d798a655c74c2b10ece12e5776aba327.png" />
</div>
</div>
<p>From the analysis of the validation performance, it is clear that the neural network performs better. We can now compute the final performance on the test set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">mlp_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">))</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.36642891489228313, 0.29069957110102324, np.float64(0.5391656249252388))
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Chapter 3 of [1]</p></li>
<li><p>Parts of chapter 11 of [2]</p></li>
</ul>
<p>[1] Heumann, Christian, and Michael Schomaker Shalabh. Introduction to statistics and data analysis. Springer International Publishing Switzerland, 2016.</p>
<p>[2] James, Gareth Gareth Michael. An introduction to statistical learning: with applications in Python, 2023.<a class="reference external" href="https://www.statlearning.com">https://www.statlearning.com</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="09_linear_regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Linear Regression</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-statistical-fixes-extending-the-linear-model">The “Statistical” Fixes: Extending the Linear Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interaction-terms">Interaction Terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-interaction">Interpreting the Interaction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-and-polynomial-regression">Quadratic and Polynomial Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-regression">Polynomial Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-understanding-to-prediction-the-machine-learning-perspective">From Understanding to Prediction: The Machine Learning Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-prediction-test-set-error">Metrics for Prediction (Test Set Error)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">1. Mean Squared Error (MSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">2. Root Mean Squared Error (RMSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">3. Mean Absolute Error (MAE)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-with-power-overfitting">The Problem with Power: Overfitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-regularization">The Solution: Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#revisiting-collinearity">Revisiting Collinearity</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-l2-penalty">Ridge Regression (L2 Penalty)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-ridge-regression-coefficients">Interpretation of the ridge regression coefficients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression-l-1-penalty">Lasso Regression (<span class="math notranslate nohighlight">\(L_1\)</span> Penalty)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-and-the-bias-variance-tradeoff">Regularization and the Bias-Variance Tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lambda-is-the-tradeoff-knob"><span class="math notranslate nohighlight">\(\lambda\)</span> is the “Tradeoff Knob”</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-theory-to-practice-the-scikit-learn-workflow">From Theory to Practice: The <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> Workflow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#our-new-toolkit">Our New Toolkit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-splitting">Data Splitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-normalization">Data Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regressor">Linear Regressor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression">Non-Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regularization">Ridge Regularization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">Lasso Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">Grid Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-pipelines">Scikit-Learn Pipelines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search-with-cross-validation">Grid Search with Cross Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-regression-algorithms">Other Regression Algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-and-model-selection">Comparison and Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>